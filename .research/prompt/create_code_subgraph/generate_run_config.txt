
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Hypothesis
open_problems='1.  Current LR schedulers for PEFT treat every training step in isolation.  However, GSM8K fine-tuning lasts only ‚âà2 000 optimiser steps, so step‚Äìto‚Äìstep LR correlations dominate final accuracy.  No method explicitly *models* these correlations or re-uses experience collected on earlier budgets.\n2.  Token-level difficulty is visible before the backward pass through inexpensive forward-only probes (e.g. logit entropy after the **first** adapter layer).  Existing token-aware tuners (BATS) wait until the full backward pass finishes, missing a chance to scale the *next* gradient.\n3.  Compute budgets in practice are often set *per experiment* (e.g. ‚Äúone hour on one A100‚Äù), not per step.  Scheduling should optimise the *entire training trajectory* under a user-supplied wall-clock budget B without the need for k-step roll-outs or quadratic programming.\n4.  Even the best RL-based controller (REACTOR) needs policy-gradient updates with high-variance rewards and hand-picked baselines; this slows convergence and couples the tuner‚Äôs learning rate to the very problem it tries to solve.' method='We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) ‚Äì a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.\n\nA.  Budget-conditioned trajectory encoder\n   ‚Ä¢ Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature ŒîŒª·µ¢ and entropy H·µ¢ after the *first* adapter layer.  This runs in the same forward pass as the loss ‚Äì no HVPs.\n   ‚Ä¢ Remaining budget b_t = B ‚Äì time_so_far is appended to every node feature, letting the network reason about *global* constraints.\n\nB.  Closed-form LR decoder\n   ‚Ä¢ Decoder is a single linear map W applied to h_t that outputs log Œ∑_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.\n   ‚Ä¢ To prevent instability we project W after every update onto the half-space defined by Œ∑_t¬∑ŒªÃÇ_max‚â§1.9 (edge-of-stability guard).\n\nC.  Self-supervised training objective\n   ‚Ä¢ We derive the *compute-normalised regret* R_T = (‚Ñí_T ‚Äì ‚Ñí_best)/time_T where ‚Ñí_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through Œ∑_t.\n   ‚Ä¢ We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.\n\nD.  Zero-shot transfer & warm start\n   ‚Ä¢ Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep ‚Äúouter‚Äù optimiser active ‚Äì  no policy search from scratch.\n\nImplementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (‚âà2 % run-time).' experimental_setup='Models: Qwen3-0.6B-Chat (primary) and Mistral-7B-Instruct (transfer) with LoRA r=64.\nData: GSM8K train/dev; GSM1k for contamination test.\nBudgets: 30 min and 60 min on single A100-80 GB (wall-clock, include eval).\nBaselines: Fixed LR, SALeLR, BATS, REACTOR (all tuned), plus human-tuned cosine.\nMetrics: 1) dev EM at budget B; 2) AUC(EM-vs-time)/B; 3) energy (Joules) from nvidia-smi.\nAblations: (a) remove graph edges ‚Üí MLP; (b) remove budget feature; (c) freeze W (no online).' primary_metric='Exact-match accuracy at 30 min wall-clock.' experimental_code='# GRAFF ‚Äì core\nclass GNN(nn.Module):\n    def __init__(self,d):\n        super().__init__(); self.msg=nn.Linear(d*2,64); self.up=nn.GRUCell(64,d)\n    def forward(self,x,adj):\n        m=self.msg(torch.cat([x[adj[0]],x[adj[1]]],-1))\n        agg=torch.zeros_like(x); agg.index_add_(0,adj[1],m)\n        return self.up(agg.reshape(-1,x.size(-1)),x.reshape(-1,x.size(-1))).view_as(x)\nclass Graff(nn.Module):\n    def __init__(self,blocks):\n        super().__init__(); self.h=nn.Parameter(torch.zeros(len(blocks),16));\n        self.gnn=GNN(16); self.W=nn.Linear(16,1)\n    def forward(self,curv,entr,budget,adj):\n        x=torch.cat([curv,entr,budget.expand_as(curv)],-1)\n        self.h=self.gnn(self.h+x,adj)\n        eta=torch.exp(self.W(self.h.mean(0)))\n        m=torch.sigmoid(self.W(self.h)).squeeze()\n        return eta,m\n' expected_result='Dev EM @30 min: Fixed-LR 74 %, SALeLR 84 %, BATS 88.5 %, REACTOR 90.8 %, GRAFF 92.4 ¬± 0.2 %.\nEnergy usage to reach 90 % EM: REACTOR 1.00 √ó, GRAFF 0.83 √ó.\nAUC improves 22 % over REACTOR.\nZero-shot on Mistral-7B: +1.8 pp EM vs REACTOR with same W.' expected_conclusion='By replacing high-variance RL with a differentiable graph-recurrent encoder/decoder conditioned on remaining wall-clock budget, GRAFF learns an end-to-end compute-aware LR trajectory in closed form.  It exploits cheap early-layer entropy to pre-scale gradients, respects global time constraints, and transfers across model sizes, achieving superior EM per Joule without external solvers or policy-gradient tuning.'

## Research Method
We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) ‚Äì a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.

A.  Budget-conditioned trajectory encoder
   ‚Ä¢ Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature ŒîŒª·µ¢ and entropy H·µ¢ after the *first* adapter layer.  This runs in the same forward pass as the loss ‚Äì no HVPs.
   ‚Ä¢ Remaining budget b_t = B ‚Äì time_so_far is appended to every node feature, letting the network reason about *global* constraints.

B.  Closed-form LR decoder
   ‚Ä¢ Decoder is a single linear map W applied to h_t that outputs log Œ∑_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.
   ‚Ä¢ To prevent instability we project W after every update onto the half-space defined by Œ∑_t¬∑ŒªÃÇ_max‚â§1.9 (edge-of-stability guard).

C.  Self-supervised training objective
   ‚Ä¢ We derive the *compute-normalised regret* R_T = (‚Ñí_T ‚Äì ‚Ñí_best)/time_T where ‚Ñí_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through Œ∑_t.
   ‚Ä¢ We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.

D.  Zero-shot transfer & warm start
   ‚Ä¢ Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep ‚Äúouter‚Äù optimiser active ‚Äì  no policy search from scratch.

Implementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (‚âà2 % run-time).

## Experimental Design
experiment_summary='This experiment evaluates GRAFF‚Äîour new, differentiable, budget-aware learning-rate (LR) planner‚Äîagainst a strong token-aware RL baseline (REACTOR) when LoRA-fine-tuning a 0.6-billion-parameter Qwen3 model on the GSM8K mathematics dataset.  \n\nWorkflow\n1. Data loading: GSM8K train/dev is streamed with length-bucketing (batch_size = 64) and mixed-precision (bfloat16).  \n2. Model: Qwen3-0.6B is equipped with LoRA adapters (rank = 64, Œ± = 16) on all self-attention and MLP layers; only adapter weights are trainable.  \n3. GRAFF module: each mini-batch‚Äôs forward pass collects two cheap probes per adapter block‚Äîlogit entropy after the first adapter layer and Hessian-free curvature estimate ŒîŒª·µ¢.  Node features = [ŒîŒª·µ¢, H·µ¢, remaining_budget].  A 2-layer message-passing GNN updates the latent state h_t, after which a single linear decoder W outputs (a) global LR Œ∑_t for the optimiser and (b) per-token multipliers m_{t,i}.  W is projected after every update to satisfy Œ∑_t¬∑ŒªÃÇ_max ‚â§ 1.9.  \n4. Joint optimisation: the normal model loss plus the compute-normalised regret R_T is back-propagated once; Adam (lr = 2e-4) updates adapter weights while a second Adam (lr = 1e-3) updates W.  \n5. Budget tracking: wall-clock time is monitored with CUDA events; once 30-min or 60-min budgets are exhausted, training halts and dev exact-match (EM) is recorded.  \n6. Baseline: REACTOR replaces GRAFF but keeps all other settings identical.  \n7. Hyper-parameter search over three key knobs (see below) is done with 50 trials of Optuna using the 30-min EM as the objective.  \n8. Metrics collected: dev EM @30 min (primary), dev EM @60 min, AUC(EM-vs-time)/B, and energy (Joules) read every 5 s from nvidia-smi.  \nThe entire pipeline fits comfortably into an 80 GB A100 (peak ‚âà34 GB) and <10 GB system RAM per worker; RAM-heavy dataloading is disabled by streaming.' evaluation_metrics=['Exact-match accuracy at 30 min wall-clock', 'Exact-match accuracy at 60 min wall-clock', 'AUC(EM-vs-time)/B', 'Energy (Joules)', 'Exact-match accuracy at 30 min wall-clock.'] proposed_method='GRAFF (Graph-Recurrent Allocation of Fine-tuning Funds) is a learning-rate scheduler that plans an entire training trajectory in closed form while respecting a user-supplied wall-clock budget B.\n\nKey components\n1. Budget-conditioned encoder: Each adapter block is a node in a lightweight graph neural network.  Node features include instantaneous curvature (ŒîŒª·µ¢) estimated via a single Hessian-vector product, token-level entropy (H·µ¢) after the first adapter layer, and the remaining budget b_t = B ‚Äì time_so_far.  One message-passing step updates latent states h_t.\n2. Linear decoder: A single weight matrix W maps the mean of h_t to log Œ∑_t (global LR) and each h_t to token multipliers m_{t,i}.  Because the decoder is linear, gradients wrt W flow through the normal loss; no REINFORCE or separate controller optimisation is needed.\n3. Stability guard: After every optimiser step, W is projected so that Œ∑_t¬∑ŒªÃÇ_max ‚â§ 1.9, keeping updates near the edge of stability.\n4. Objective: Compute-normalised regret R_T = (‚Ñí_T ‚Äì ‚Ñí_best)/time_T is differentiable in W through Œ∑_t, allowing simultaneous optimisation of adapter weights and scheduler parameters with standard back-prop.\n5. Warm-start: W is pre-trained for 300 mini-batches on a 70 M-parameter Qwen-tiny model; the same W is then continuously fine-tuned online when larger models are trained, enabling zero-shot transfer across sizes.\n\nAlgorithmic procedure per mini-batch\n(1) Forward pass through model and probes ‚Üí collect ŒîŒª·µ¢, H·µ¢.\n(2) GNN update ‚Üí h_t.\n(3) Decode (Œ∑_t, m_{t,i}); scale optimiser LR and token gradients.\n(4) Backward pass; accumulate gradients of both adapter weights and W.\n(5) Adam steps for both parameter sets; project W; log time & energy.' comparative_methods=['REACTOR (RL-based LR scheduler)'] models_to_use=['Qwen3-0.6B'] datasets_to_use=['gsm8k'] hyperparameters_to_search={'adapter_lr_init': '1e-4-5e-4', 'graff_outer_lr': '5e-4-2e-3', 'LoRA_rank': '32,64,96'} external_resources=ExternalResources(hugging_face=HuggingFace(models=[HuggingFaceResource(id='Qwen/Qwen3-0.6B', author='Qwen', sha='c1899de289a04d12100db370d81485cdf75e47ca', created_at=datetime.datetime(2025, 4, 27, 3, 40, 8, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2025, 7, 26, 3, 46, 27, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=7034007, likes=765, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='LICENSE', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='generation_config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='merges.txt', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='model.safetensors', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tokenizer.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='tokenizer_config.json', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='vocab.json', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license='apache-2.0', language=[], library_name='transformers', pipeline_tag='text-generation', tags=[], datasets=[], model_type=None, base_model=['Qwen/Qwen3-0.6B-Base'], task_categories=[], size_categories=[], metrics=[], widget=[]), tags=['transformers', 'safetensors', 'qwen3', 'text-generation', 'conversational', 'arxiv:2505.09388', 'base_model:Qwen/Qwen3-0.6B-Base', 'base_model:finetune:Qwen/Qwen3-0.6B-Base', 'license:apache-2.0', 'autotrain_compatible', 'text-generation-inference', 'endpoints_compatible', 'region:us'], pipeline_tag='text-generation', library_name='transformers', readme='---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-0.6B-Base\n---\n\n# Qwen3-0.6B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-0.6B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 0.6B\n- Number of Paramaters (Non-Embedding): 0.44B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 16 for Q and 8 for KV\n- Context Length: 32,768 \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n> [!TIP]\n> If you encounter significant endless repetitions, please refer to the [Best Practices](#best-practices) section for optimal sampling parameters, and set the ``presence_penalty`` to 1.5.\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: \'qwen3\'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-0.6B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model\'s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model\'s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model\'s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-0.6B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r\'s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r\'s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    \'model\': \'Qwen3-0.6B\',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # \'model_type\': \'qwen_dashscope\',\n    # \'api_key\': os.getenv(\'DASHSCOPE_API_KEY\'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    \'model_server\': \'http://localhost:8000/v1\',  # api_base\n    \'api_key\': \'EMPTY\',\n\n    # Other parameters:\n    # \'generate_cfg\': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         \'thought_in_content\': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {\'mcpServers\': {  # You can specify the MCP configuration file\n            \'time\': {\n                \'command\': \'uvx\',\n                \'args\': [\'mcp-server-time\', \'--local-timezone=Asia/Shanghai\']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  \'code_interpreter\',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{\'role\': \'user\', \'content\': \'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen\'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \\boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code='from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-0.6B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-0.6B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r\'s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r\'s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    \'model\': \'Qwen3-0.6B\',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # \'model_type\': \'qwen_dashscope\',\n    # \'api_key\': os.getenv(\'DASHSCOPE_API_KEY\'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    \'model_server\': \'http://localhost:8000/v1\',  # api_base\n    \'api_key\': \'EMPTY\',\n\n    # Other parameters:\n    # \'generate_cfg\': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         \'thought_in_content\': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {\'mcpServers\': {  # You can specify the MCP configuration file\n            \'time\': {\n                \'command\': \'uvx\',\n                \'args\': [\'mcp-server-time\', \'--local-timezone=Asia/Shanghai\']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  \'code_interpreter\',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{\'role\': \'user\', \'content\': \'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen\'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)')], datasets=[HuggingFaceResource(id='openai/gsm8k', author='openai', sha='e53f048856ff4f594e959d75785d2c2d37b678ee', created_at=datetime.datetime(2022, 4, 12, 10, 22, 10, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2024, 1, 4, 12, 5, 15, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=463748, likes=939, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='main/test-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='main/train-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='socratic/test-00000-of-00001.parquet', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='socratic/train-00000-of-00001.parquet', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license=['mit'], language=['en'], library_name=None, pipeline_tag=None, tags=['math-word-problems'], datasets=[], model_type=None, base_model=None, task_categories=['text2text-generation'], size_categories=['1K<n<10K'], metrics=[], widget=[]), tags=['annotations_creators:crowdsourced', 'language_creators:crowdsourced', 'multilinguality:monolingual', 'source_datasets:original', 'language:en', 'license:mit', 'size_categories:10K<n<100K', 'format:parquet', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'arxiv:2110.14168', 'region:us', 'math-word-problems'], pipeline_tag=None, library_name=None, readme='---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- crowdsourced\nlanguage:\n- en\nlicense:\n- mit\nmultilinguality:\n- monolingual\nsize_categories:\n- 1K<n<10K\nsource_datasets:\n- original\ntask_categories:\n- text2text-generation\ntask_ids: []\npaperswithcode_id: gsm8k\npretty_name: Grade School Math 8K\ntags:\n- math-word-problems\ndataset_info:\n- config_name: main\n  features:\n  - name: question\n    dtype: string\n  - name: answer\n    dtype: string\n  splits:\n  - name: train\n    num_bytes: 3963202\n    num_examples: 7473\n  - name: test\n    num_bytes: 713732\n    num_examples: 1319\n  download_size: 2725633\n  dataset_size: 4676934\n- config_name: socratic\n  features:\n  - name: question\n    dtype: string\n  - name: answer\n    dtype: string\n  splits:\n  - name: train\n    num_bytes: 5198108\n    num_examples: 7473\n  - name: test\n    num_bytes: 936859\n    num_examples: 1319\n  download_size: 3164254\n  dataset_size: 6134967\nconfigs:\n- config_name: main\n  data_files:\n  - split: train\n    path: main/train-*\n  - split: test\n    path: main/test-*\n- config_name: socratic\n  data_files:\n  - split: train\n    path: socratic/train-*\n  - split: test\n    path: socratic/test-*\n---\n\n# Dataset Card for GSM8K\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-instances)\n  - [Data Splits](#data-instances)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n\n## Dataset Description\n\n- **Homepage:** https://openai.com/blog/grade-school-math/\n- **Repository:** https://github.com/openai/grade-school-math\n- **Paper:** https://arxiv.org/abs/2110.14168\n- **Leaderboard:** [Needs More Information]\n- **Point of Contact:** [Needs More Information]\n\n### Dataset Summary\n\nGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.\n- These problems take between 2 and 8 steps to solve.\n- Solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ ‚àí √ó√∑) to reach the final answer.\n- A bright middle school student should be able to solve every problem: from the paper, "Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly defining a variable."\n- Solutions are provided in natural language, as opposed to pure math expressions. From the paper: "We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models‚Äô internal monologues""\n\n### Supported Tasks and Leaderboards\n\nThis dataset is generally used to test logic and math in language modelling.\nIt has been used for many benchmarks, including the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n\n### Languages\n\nThe text in the dataset is in English. The associated BCP-47 code is `en`.\n\n## Dataset Structure\n\n### Data Instances\n\nFor the `main` configuration, each instance contains a string for the grade-school level math question and a string for the corresponding answer with multiple steps of reasoning and calculator annotations (explained [here](https://github.com/openai/grade-school-math#calculation-annotations)).\n\n\n```python\n{\n    \'question\': \'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\',\n    \'answer\': \'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72\',\n}\n```\n\nFor the `socratic` configuration, each instance contains a string for a grade-school level math question, a string for the corresponding answer with multiple steps of reasoning, calculator annotations (explained [here](https://github.com/openai/grade-school-math#calculation-annotations)), and *Socratic sub-questions*.\n\n```python\n{\n    \'question\': \'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\',\n    \'answer\': \'How many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nHow many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72\',\n}\n```\n\n### Data Fields\n\nThe data fields are the same among `main` and `socratic` configurations and their individual splits.\n\n- question: The question string to a grade school math problem.\n\n- answer: The full solution string to the `question`. It contains multiple steps of reasoning with calculator annotations and the final numeric solution.\n\n### Data Splits\n\n| name   |train|validation|\n|--------|----:|---------:|\n|main    | 7473|      1319|\n|socratic| 7473|      1319|\n\n## Dataset Creation\n\n### Curation Rationale\n\n[Needs More Information]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\nFrom the paper, appendix A:\n\n> We initially collected a starting set of a thousand problems and natural language solutions by hiring freelance contractors on Upwork (upwork.com). We then worked with Surge AI (surgehq.ai), an NLP data labeling platform, to scale up our data collection. After collecting the full dataset, we asked workers to re-solve all problems, with no workers re-solving problems they originally wrote. We checked whether their final answers agreed with the original solutions, and any problems that produced disagreements were either repaired or discarded. We then performed another round of agreement checks on a smaller subset of problems, finding that 1.7% of problems still produce disagreements among contractors. We estimate this to be the fraction of problems that contain breaking errors or ambiguities. It is possible that a larger percentage of problems contain subtle errors.\n\n#### Who are the source language producers?\n\n[Needs More Information]\n\n### Annotations\n\n#### Annotation process\n\n[Needs More Information]\n\n#### Who are the annotators?\n\nSurge AI (surgehq.ai)\n\n### Personal and Sensitive Information\n\n[Needs More Information]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[Needs More Information]\n\n### Discussion of Biases\n\n[Needs More Information]\n\n### Other Known Limitations\n\n[Needs More Information]\n\n## Additional Information\n\n### Dataset Curators\n\n[Needs More Information]\n\n### Licensing Information\n\nThe GSM8K dataset is licensed under the [MIT License](https://opensource.org/licenses/MIT).\n\n### Citation Information\n\n```bibtex\n@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}\n```\n\n### Contributions\n\nThanks to [@jon-tow](https://github.com/jon-tow) for adding this dataset.', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code=None), HuggingFaceResource(id='madrylab/gsm8k-platinum', author='madrylab', sha='e762492455a1cf7967de89f05b6bef72fc713b66', created_at=datetime.datetime(2025, 3, 6, 18, 41, 25, tzinfo=TzInfo(UTC)), last_modified=datetime.datetime(2025, 3, 11, 14, 48, 29, tzinfo=TzInfo(UTC)), private=False, gated=False, disabled=False, downloads=3274, likes=43, siblings=[HuggingFaceSibling(rfilename='.gitattributes', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='README.md', size=None, blob_id=None, lfs=None), HuggingFaceSibling(rfilename='main/test-00000-of-00001.parquet', size=None, blob_id=None, lfs=None)], card_data=HuggingFaceCardData(license='mit', language=['en'], library_name=None, pipeline_tag=None, tags=['math-word-problems'], datasets=[], model_type=None, base_model=None, task_categories=['text2text-generation'], size_categories=['1K<n<10K'], metrics=[], widget=[]), tags=['language:en', 'license:mit', 'size_categories:1K<n<10K', 'format:parquet', 'modality:text', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'arxiv:2502.03461', 'arxiv:2110.14168', 'region:us', 'math-word-problems'], pipeline_tag=None, library_name=None, readme='---\nlanguage:\n- en\nlicense: mit\ndataset_info:\n  config_name: main\n  features:\n  - name: question\n    dtype: string\n  - name: answer\n    dtype: string\n  - name: cleaning_status\n    dtype: string\n  splits:\n  - name: test\n    num_bytes: 663954\n    num_examples: 1209\n  download_size: 380973\n  dataset_size: 663954\nconfigs:\n- config_name: main\n  data_files:\n  - split: test\n    path: main/test-*\ntask_categories:\n- text2text-generation\ntags:\n- math-word-problems\nsize_categories:\n- 1K<n<10K\n---\n\n# Dataset Card for GSM8K-Platinum\n\n[**üèÜ Homepage**](http://platinum-bench.csail.mit.edu/) &nbsp;|&nbsp; [**üì£ Blog**](https://gradientscience.org/gsm8k-platinum/) &nbsp;|&nbsp; [**üñ•Ô∏è Code**](https://github.com/MadryLab/platinum-benchmarks/) &nbsp;|&nbsp; [**üìñ Paper**](https://arxiv.org/abs/2502.03461) &nbsp;|&nbsp; [**üîç Error Viewer**](http://platinum-bench.csail.mit.edu/inspect?model=o1-2024-12-17-high&dataset=gsm8k_full)\n\n## Dataset Description\n\n- **Homepage:** http://platinum-bench.csail.mit.edu/\n- **Repository:** https://github.com/MadryLab/platinum-benchmarks/\n- **Paper:** https://arxiv.org/abs/2502.03461\n- **Leaderboard:** http://platinum-bench.csail.mit.edu/\n- **Point of Contact:** [Edward Vendrow](mailto:evendrow@mit.edu), [Joshua Vendrow](mailto:jvendrow@mit.edu)\n\n### Dataset Summary\n\n_**GSM8K-Platinum**_ is a revised version of the full test set of GSM8K (Grade School Math 8K), a dataset of grade school math word problems, providing a more accurate assessment of mathematical reasoning capabilities\n\nTo revise this dataset, we ran a variety of frontier models each individual example and manually examined any example for which at least one model made an error. We revise the labels of mislabeled examples, and remove any question that we determine to be poorly written (most often due to ambiguity in the problem statement). See our [paper](https://arxiv.org/abs/2502.03461) for further details on the revision process and our criteria for "bad" questions.\n\nPlease refer to the original GSM8K dataset at: [https://huggingface.co/datasets/openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k).\n\n<p align="center">\n<img src="https://cdn-uploads.huggingface.co/production/uploads/630b1e44cd26ad7f60d490e2/cAt7JFohPNFRYom5OMXTD.png" alt="Comparing GSM8K to GSM8K-Platinum" width=700 />\n</p>\n\n### Load the Dataset\n\nWe keep the original data columns from `openai/gsm8k`, so `madrylab/gsm8k-platinum` can be used directly as a drop-in to replace the original gsm8k dataset.\n\nTo load the dataset using HuggingFace `datasets`, you first need to `pip install datasets`, then run the following code:\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset("madrylab/gsm8k-platinum", "main", split="test")\n```\n\n## Dataset structure\n\n### Dataset Subsets & Cleaning Statistics\n\n\n| GSM8K (Test) | # Flagged by Models | # Rejected | # Re-labeled | # Verified | GSM8K-Platinum\n| ----- | ----- | ----- | ----- | ----- | ----- | \n1319 | 219 | 110 | 10 | 99 | 1209\n\n### Data Instances\n\nAn example from the **GSM8K-Platinum** looks as follows:\n```\n{\n    \'question\': \'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\',\n    \'answer\': \'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3\',\n    \'cleaning_status\': \'consensus\'\n}\n```\n\n### Data Fields\n- **question** (`str`): The question to a grade school math problem.\n- **answer** (`str`): The full solution to the question. It contains multiple steps of reasoning with calculator annotations and the final numeric solution.\n- **cleaning_status** (`str`): One of:\n\t1. *consensus*: all LLMs agreed with the label, so the example was not manually reviewed.\n\t2. *verified*: the original target was manually verified to be correct.\n\t3. *revised*: the answer is updated from the original answer.\n\n### Prompt Example\n\nDuring our revision process, we used the following zero-shot prompt to query models with questions from GSM8K:\n\n```\nSolve the following math word problem.\n\nA robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n\nThink step-by-step. Then, provide the final answer as a single integer in the format "Answer: XXX" with no extra formatting.\n```\nThe instruction to "think step-by-step" was excluded for reasoning models.\n\n## Dataset Creation\n\n### Curation Rationale\n\nGSM8K is one of a number of LLM benchmarks that contain significant label noise such as mislabeled or ambiguous questions. Due to this label noise, progress in these benchmarks often stalls before models actually achieve reliable performance on them. As a result, the comminuty often considers these benchmarks to be "saturated" and discards them too early, discouraging machine learning practictioners from ever striving to achieve proper reliability.\n\nIn our [previous work](https://arxiv.org/abs/2502.03461), we revised a number of such benchmarks, including a 300-example subset of the GSM8K test set (these revised benchmarks are publically avaiable at: [https://huggingface.co/datasets/madrylab/platinum-bench](https://huggingface.co/datasets/madrylab/platinum-bench)). To further aid all who currently utilize GSM8K for evaluation (e.g., during the model development process), we have decided to revise the full GSM8K test set. By doing so, **GSM8K-Platinum** now serves as a natural and easy drop-in for the original GSM8K test set. \n\n### Source Data and Attribution\n\nWe sourced GSM8K from OpenAI\'s official huggingface repository: [https://huggingface.co/datasets/openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k). This dataset is protected by the [MIT](https://github.com/openai/grade-school-math/blob/master/LICENSE) license.\n\nPlease defer to the GSM8K dataset card for further details on their collection and annotation process.\n\n## Additional Information\n\n### Licensing Information\n\nThe further annotations we provide are licensed under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/legalcode) license.\n\n### Citation Information\nCite this dataset as well as the citation for the original GSM8K dataset.\n\n```\n@misc{vendrow2025largelanguagemodelbenchmarks,\n      title={Do Large Language Model Benchmarks Test Reliability?}, \n      author={Joshua Vendrow and Edward Vendrow and Sara Beery and Aleksander Madry},\n      year={2025},\n      eprint={2502.03461},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2502.03461}, \n}\n```\n\n```\n@article{cobbe2021gsm8k,\n  title={Training Verifiers to Solve Math Word Problems},\n  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\n  journal={arXiv preprint arXiv:2110.14168},\n  year={2021}\n}\n```', model_index=None, widget_data=None, config=None, transformers_info=None, spaces=None, safetensors=None, used_storage=None, extracted_code=None)])) experiment_code=None

## Experiment Runs

- Run ID: proposed-iter1-Qwen3-0.6B-gsm8k
  Method: proposed
  Model: Qwen3-0.6B
  Dataset: gsm8k

- Run ID: comparative-1-iter1-Qwen3-0.6B-gsm8k
  Method: comparative-1
  Model: Qwen3-0.6B
  Dataset: gsm8k


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-iter1-Qwen3-0.6B-gsm8k",
      "run_config_yaml": "run_id: proposed-iter1-Qwen3-0.6B-gsm8k\nmethod: proposed  # GRAFF scheduler\nmodel:\n  name: Qwen/Qwen3-0.6B\n  dtype: bfloat16\n  lora:\n    r: 64            # LoRA rank\n    alpha: 16\n    dropout: 0.05\n  parameter_count: 0.6B\n\ndataset:\n  name: openai/gsm8k\n  train_split: train\n  eval_split: test\n  max_tokens: 2048\n  streaming: true\n  shuffle_buffer: 10000\n\ntraining:\n  budget_minutes: 30            # wall-clock budget B\n  global_batch_size: 64         # after gradient accumulation\n  micro_batch_size: 8\n  gradient_accumulation_steps: 8\n  total_steps: 2000             # safety cap; early-stop by budget\n  mixed_precision: bf16\n  gradient_checkpointing: true\n  clip_grad_norm: 1.0\n\n  optimizer:\n    type: adamw\n    lr_init: 2e-4               # will be rescaled by GRAFF\n    betas: [0.9, 0.98]\n    weight_decay: 0.01\n\n  scheduler:\n    type: graff\n    outer_lr: 1e-3              # Adam lr for W\n    stability_guard: 1.9        # Œ∑¬∑ŒªÃÇ_max ‚â§ 1.9\n    warm_start_ckpt: graff_pretrain_qwen_tiny.pt\n\nevaluation:\n  interval_steps: 100\n  metrics:\n    - exact_match\n\noptuna:\n  n_trials: 50\n  direction: maximize\n  search_space:\n    adapter_lr_init:\n      type: loguniform\n      low: 1.0e-4\n      high: 5.0e-4\n    graff_outer_lr:\n      type: loguniform\n      low: 5.0e-4\n      high: 2.0e-3\n    lora_r:\n      type: categorical\n      choices: [32, 64, 96]\n"
    },
    {
      "run_id": "comparative-1-iter1-Qwen3-0.6B-gsm8k",
      "run_config_yaml": "run_id: comparative-1-iter1-Qwen3-0.6B-gsm8k\nmethod: comparative-1  # REACTOR RL scheduler\nmodel:\n  name: Qwen/Qwen3-0.6B\n  dtype: bfloat16\n  lora:\n    r: 64\n    alpha: 16\n    dropout: 0.05\n  parameter_count: 0.6B\n\ndataset:\n  name: openai/gsm8k\n  train_split: train\n  eval_split: test\n  max_tokens: 2048\n  streaming: true\n  shuffle_buffer: 10000\n\ntraining:\n  budget_minutes: 30\n  global_batch_size: 64\n  micro_batch_size: 8\n  gradient_accumulation_steps: 8\n  total_steps: 2000\n  mixed_precision: bf16\n  gradient_checkpointing: true\n  clip_grad_norm: 1.0\n\n  optimizer:\n    type: adamw\n    lr_init: 2e-4            # initial LR before REACTOR scaling\n    betas: [0.9, 0.98]\n    weight_decay: 0.01\n\n  scheduler:\n    type: reactor\n    controller_lr: 1e-4      # policy-gradient step size\n    baseline_decay: 0.99     # EMA baseline for REINFORCE\n    entropy_bonus: 0.01\n\nevaluation:\n  interval_steps: 100\n  metrics:\n    - exact_match\n\noptuna:\n  n_trials: 50\n  direction: maximize\n  search_space:\n    adapter_lr_init:\n      type: loguniform\n      low: 1.0e-4\n      high: 5.0e-4\n    controller_lr:\n      type: loguniform\n      low: 5.0e-5\n      high: 5.0e-4\n    lora_r:\n      type: categorical\n      choices: [32, 64, 96]\n"
    }
  ]
}
