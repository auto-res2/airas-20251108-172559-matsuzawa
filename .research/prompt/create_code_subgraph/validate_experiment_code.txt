
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/runs/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/runs/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
       ```json
       {
         "primary_metric": "Exact-match accuracy at 30 min wall-clock.",
         "metrics": {
           "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
           "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
         },
         "best_proposed": {
           "run_id": "proposed-iter2-model-dataset",
           "value": 0.92
         },
         "best_baseline": {
           "run_id": "comparative-1-model-dataset",
           "value": 0.88
         },
         "gap": 4.55
       }
       ```
       The structure must include:
       - "primary_metric": The primary evaluation metric name from the hypothesis
       - "metrics": All collected metrics organized by metric name, then by run_id
       - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
       - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
       - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
         * Must use the expected results from the hypothesis to determine metric direction (higher vs lower is better)
         * If the metric should be minimized, reverse the sign of the gap
         * The gap represents the percentage improvement of the proposed method over the best baseline
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Hypothesis
open_problems='1.  Current LR schedulers for PEFT treat every training step in isolation.  However, GSM8K fine-tuning lasts only ≈2 000 optimiser steps, so step–to–step LR correlations dominate final accuracy.  No method explicitly *models* these correlations or re-uses experience collected on earlier budgets.\n2.  Token-level difficulty is visible before the backward pass through inexpensive forward-only probes (e.g. logit entropy after the **first** adapter layer).  Existing token-aware tuners (BATS) wait until the full backward pass finishes, missing a chance to scale the *next* gradient.\n3.  Compute budgets in practice are often set *per experiment* (e.g. “one hour on one A100”), not per step.  Scheduling should optimise the *entire training trajectory* under a user-supplied wall-clock budget B without the need for k-step roll-outs or quadratic programming.\n4.  Even the best RL-based controller (REACTOR) needs policy-gradient updates with high-variance rewards and hand-picked baselines; this slows convergence and couples the tuner’s learning rate to the very problem it tries to solve.' method='We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.\n\nA.  Budget-conditioned trajectory encoder\n   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.\n   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.\n\nB.  Closed-form LR decoder\n   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.\n   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).\n\nC.  Self-supervised training objective\n   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.\n   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.\n\nD.  Zero-shot transfer & warm start\n   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.\n\nImplementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).' experimental_setup='Models: Qwen3-0.6B-Chat (primary) and Mistral-7B-Instruct (transfer) with LoRA r=64.\nData: GSM8K train/dev; GSM1k for contamination test.\nBudgets: 30 min and 60 min on single A100-80 GB (wall-clock, include eval).\nBaselines: Fixed LR, SALeLR, BATS, REACTOR (all tuned), plus human-tuned cosine.\nMetrics: 1) dev EM at budget B; 2) AUC(EM-vs-time)/B; 3) energy (Joules) from nvidia-smi.\nAblations: (a) remove graph edges → MLP; (b) remove budget feature; (c) freeze W (no online).' primary_metric='Exact-match accuracy at 30 min wall-clock.' experimental_code='# GRAFF – core\nclass GNN(nn.Module):\n    def __init__(self,d):\n        super().__init__(); self.msg=nn.Linear(d*2,64); self.up=nn.GRUCell(64,d)\n    def forward(self,x,adj):\n        m=self.msg(torch.cat([x[adj[0]],x[adj[1]]],-1))\n        agg=torch.zeros_like(x); agg.index_add_(0,adj[1],m)\n        return self.up(agg.reshape(-1,x.size(-1)),x.reshape(-1,x.size(-1))).view_as(x)\nclass Graff(nn.Module):\n    def __init__(self,blocks):\n        super().__init__(); self.h=nn.Parameter(torch.zeros(len(blocks),16));\n        self.gnn=GNN(16); self.W=nn.Linear(16,1)\n    def forward(self,curv,entr,budget,adj):\n        x=torch.cat([curv,entr,budget.expand_as(curv)],-1)\n        self.h=self.gnn(self.h+x,adj)\n        eta=torch.exp(self.W(self.h.mean(0)))\n        m=torch.sigmoid(self.W(self.h)).squeeze()\n        return eta,m\n' expected_result='Dev EM @30 min: Fixed-LR 74 %, SALeLR 84 %, BATS 88.5 %, REACTOR 90.8 %, GRAFF 92.4 ± 0.2 %.\nEnergy usage to reach 90 % EM: REACTOR 1.00 ×, GRAFF 0.83 ×.\nAUC improves 22 % over REACTOR.\nZero-shot on Mistral-7B: +1.8 pp EM vs REACTOR with same W.' expected_conclusion='By replacing high-variance RL with a differentiable graph-recurrent encoder/decoder conditioned on remaining wall-clock budget, GRAFF learns an end-to-end compute-aware LR trajectory in closed form.  It exploits cheap early-layer entropy to pre-scale gradients, respects global time constraints, and transfers across model sizes, achieving superior EM per Joule without external solvers or policy-gradient tuning.'

# Current Research Method
We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.

A.  Budget-conditioned trajectory encoder
   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.
   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.

B.  Closed-form LR decoder
   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.
   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).

C.  Self-supervised training objective
   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.
   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.

D.  Zero-shot transfer & warm start
   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.

Implementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).

# Experimental Design
- Strategy: This experiment evaluates GRAFF—our new, differentiable, budget-aware learning-rate (LR) planner—against a strong token-aware RL baseline (REACTOR) when LoRA-fine-tuning a 0.6-billion-parameter Qwen3 model on the GSM8K mathematics dataset.  

Workflow
1. Data loading: GSM8K train/dev is streamed with length-bucketing (batch_size = 64) and mixed-precision (bfloat16).  
2. Model: Qwen3-0.6B is equipped with LoRA adapters (rank = 64, α = 16) on all self-attention and MLP layers; only adapter weights are trainable.  
3. GRAFF module: each mini-batch’s forward pass collects two cheap probes per adapter block—logit entropy after the first adapter layer and Hessian-free curvature estimate Δλᵢ.  Node features = [Δλᵢ, Hᵢ, remaining_budget].  A 2-layer message-passing GNN updates the latent state h_t, after which a single linear decoder W outputs (a) global LR η_t for the optimiser and (b) per-token multipliers m_{t,i}.  W is projected after every update to satisfy η_t·λ̂_max ≤ 1.9.  
4. Joint optimisation: the normal model loss plus the compute-normalised regret R_T is back-propagated once; Adam (lr = 2e-4) updates adapter weights while a second Adam (lr = 1e-3) updates W.  
5. Budget tracking: wall-clock time is monitored with CUDA events; once 30-min or 60-min budgets are exhausted, training halts and dev exact-match (EM) is recorded.  
6. Baseline: REACTOR replaces GRAFF but keeps all other settings identical.  
7. Hyper-parameter search over three key knobs (see below) is done with 50 trials of Optuna using the 30-min EM as the objective.  
8. Metrics collected: dev EM @30 min (primary), dev EM @60 min, AUC(EM-vs-time)/B, and energy (Joules) read every 5 s from nvidia-smi.  
The entire pipeline fits comfortably into an 80 GB A100 (peak ≈34 GB) and <10 GB system RAM per worker; RAM-heavy dataloading is disabled by streaming.
- Proposed Method: GRAFF (Graph-Recurrent Allocation of Fine-tuning Funds) is a learning-rate scheduler that plans an entire training trajectory in closed form while respecting a user-supplied wall-clock budget B.

Key components
1. Budget-conditioned encoder: Each adapter block is a node in a lightweight graph neural network.  Node features include instantaneous curvature (Δλᵢ) estimated via a single Hessian-vector product, token-level entropy (Hᵢ) after the first adapter layer, and the remaining budget b_t = B – time_so_far.  One message-passing step updates latent states h_t.
2. Linear decoder: A single weight matrix W maps the mean of h_t to log η_t (global LR) and each h_t to token multipliers m_{t,i}.  Because the decoder is linear, gradients wrt W flow through the normal loss; no REINFORCE or separate controller optimisation is needed.
3. Stability guard: After every optimiser step, W is projected so that η_t·λ̂_max ≤ 1.9, keeping updates near the edge of stability.
4. Objective: Compute-normalised regret R_T = (ℒ_T – ℒ_best)/time_T is differentiable in W through η_t, allowing simultaneous optimisation of adapter weights and scheduler parameters with standard back-prop.
5. Warm-start: W is pre-trained for 300 mini-batches on a 70 M-parameter Qwen-tiny model; the same W is then continuously fine-tuned online when larger models are trained, enabling zero-shot transfer across sizes.

Algorithmic procedure per mini-batch
(1) Forward pass through model and probes → collect Δλᵢ, Hᵢ.
(2) GNN update → h_t.
(3) Decode (η_t, m_{t,i}); scale optimiser LR and token gradients.
(4) Backward pass; accumulate gradients of both adapter weights and W.
(5) Adam steps for both parameter sets; project W; log time & energy.
- Evaluation Metrics: ['Exact-match accuracy at 30 min wall-clock', 'Exact-match accuracy at 60 min wall-clock', 'AUC(EM-vs-time)/B', 'Energy (Joules)', 'Exact-match accuracy at 30 min wall-clock.']

# Experiment Runs

- Run ID: proposed-iter1-Qwen3-0.6B-gsm8k
  Method: proposed
  Model: Qwen3-0.6B
  Dataset: gsm8k
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Qwen3-0.6B-gsm8k
method: proposed  # GRAFF scheduler
model:
  name: Qwen/Qwen3-0.6B
  dtype: bfloat16
  lora:
    r: 64            # LoRA rank
    alpha: 16
    dropout: 0.05
  parameter_count: 0.6B

dataset:
  name: openai/gsm8k
  train_split: train
  eval_split: test
  max_tokens: 2048
  streaming: true
  shuffle_buffer: 10000

training:
  budget_minutes: 30            # wall-clock budget B
  global_batch_size: 64         # after gradient accumulation
  micro_batch_size: 8
  gradient_accumulation_steps: 8
  total_steps: 2000             # safety cap; early-stop by budget
  mixed_precision: bf16
  gradient_checkpointing: true
  clip_grad_norm: 1.0

  optimizer:
    type: adamw
    lr_init: 2e-4               # will be rescaled by GRAFF
    betas: [0.9, 0.98]
    weight_decay: 0.01

  scheduler:
    type: graff
    outer_lr: 1e-3              # Adam lr for W
    stability_guard: 1.9        # η·λ̂_max ≤ 1.9
    warm_start_ckpt: graff_pretrain_qwen_tiny.pt

evaluation:
  interval_steps: 100
  metrics:
    - exact_match

optuna:
  n_trials: 50
  direction: maximize
  search_space:
    adapter_lr_init:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    graff_outer_lr:
      type: loguniform
      low: 5.0e-4
      high: 2.0e-3
    lora_r:
      type: categorical
      choices: [32, 64, 96]

  ```
  

- Run ID: comparative-1-iter1-Qwen3-0.6B-gsm8k
  Method: comparative-1
  Model: Qwen3-0.6B
  Dataset: gsm8k
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Qwen3-0.6B-gsm8k
method: comparative-1  # REACTOR RL scheduler
model:
  name: Qwen/Qwen3-0.6B
  dtype: bfloat16
  lora:
    r: 64
    alpha: 16
    dropout: 0.05
  parameter_count: 0.6B

dataset:
  name: openai/gsm8k
  train_split: train
  eval_split: test
  max_tokens: 2048
  streaming: true
  shuffle_buffer: 10000

training:
  budget_minutes: 30
  global_batch_size: 64
  micro_batch_size: 8
  gradient_accumulation_steps: 8
  total_steps: 2000
  mixed_precision: bf16
  gradient_checkpointing: true
  clip_grad_norm: 1.0

  optimizer:
    type: adamw
    lr_init: 2e-4            # initial LR before REACTOR scaling
    betas: [0.9, 0.98]
    weight_decay: 0.01

  scheduler:
    type: reactor
    controller_lr: 1e-4      # policy-gradient step size
    baseline_decay: 0.99     # EMA baseline for REINFORCE
    entropy_bonus: 0.01

evaluation:
  interval_steps: 100
  metrics:
    - exact_match

optuna:
  n_trials: 50
  direction: maximize
  search_space:
    adapter_lr_init:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    controller_lr:
      type: loguniform
      low: 5.0e-5
      high: 5.0e-4
    lora_r:
      type: categorical
      choices: [32, 64, 96]

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n\n# -------- CLI overrides ---------- #\nrun: ???   # required\nmode: full # full | trial\nresults_dir: ./outputs\n\n# -------- WandB ------------------ #\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online   # altered automatically by mode=trial\n\n# -------- Optuna ----------------- #\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# -------- Dataset ---------------- #\ndataset:\n  name: null\n  train_split: train\n  eval_split: test\n  max_tokens: 2048\n  streaming: false\n  shuffle_buffer: 10000\n\n# -------- Training -------------- #\ntraining:\n  budget_minutes: 30\n  global_batch_size: 64\n  micro_batch_size: 8\n  gradient_accumulation_steps: 8\n  total_steps: 2000\n  max_epochs: 100000   # effectively limitless unless overridden\n  mixed_precision: bf16\n  clip_grad_norm: 1.0\n\n  optimizer:\n    lr_init: 2e-4\n    betas: [0.9, 0.95]\n    weight_decay: 0.01\n\n  scheduler:\n    type: graff   # graff | reactor (can be overridden by run-specific YAML)\n    outer_lr: 1e-3\n    controller_lr: 1e-4\n    stability_guard: 1.9\n    baseline_decay: 0.99\n    entropy_bonus: 0.01\n\n# -------- Evaluation ------------- #\nevaluation:\n  interval_steps: 100\n  metrics: [exact_match]\n\n# -------- Model ------------------ #\nmodel:\n  name: null\n  dtype: bfloat16\n  lora:\n    r: 64\n    alpha: 16\n    dropout: 0.05", "evaluate_py": "import argparse\nimport json\nfrom itertools import combinations\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom scipy.stats import ttest_ind\nfrom sklearn.metrics import confusion_matrix\n\nPRIMARY_METRIC = \"Exact-match accuracy at 30 min wall-clock.\"\n\n################################################################################\n#                                UTILITIES                                    #\n################################################################################\n\ndef _save_json(obj: Dict, path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef _plot_learning_curve(run_id: str, history: pd.DataFrame, out_dir: Path) -\u003e Path:\n    plt.figure(figsize=(6, 4))\n    if \"dev_exact_match\" in history.columns:\n        sns.lineplot(data=history, x=\"_step\", y=\"dev_exact_match\")\n        plt.ylabel(\"Dev EM (%)\")\n    else:\n        sns.lineplot(data=history, x=\"_step\", y=\"train_loss\")\n        plt.ylabel(\"Loss\")\n    plt.title(f\"Learning curve \u2013 {run_id}\")\n    plt.xlabel(\"Optimiser step\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n\ndef _plot_confusion(run_id: str, preds: List[str], answers: List[str], out_dir: Path) -\u003e Path:\n    correct = [int(p.strip() == a.strip()) for p, a in zip(preds, answers)]\n    y_true = correct  # actual: 1 if label correct, 0 if wrong\n    y_pred = correct  # model predicts the same (binary EM) \u2013 diagonal matrix\n    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n    plt.figure(figsize=(3, 3))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False, cmap=\"Blues\", xticklabels=[\"Correct\", \"Incorrect\"], yticklabels=[\"Correct\", \"Incorrect\"])\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.title(f\"Confusion \u2013 {run_id}\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n\ndef _plot_comparison_bar(metric_values: Dict[str, float], out_dir: Path) -\u003e Path:\n    df = pd.DataFrame({\"run_id\": list(metric_values.keys()), \"value\": list(metric_values.values())})\n    plt.figure(figsize=(max(6, len(metric_values) * 1.2), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"value\", palette=\"viridis\")\n    for idx, v in enumerate(df.value):\n        plt.text(idx, v + 0.5, f\"{v:.1f}\", ha=\"center\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(PRIMARY_METRIC)\n    plt.tight_layout()\n    fname = out_dir / \"comparison_primary_metric_bar.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n\ndef _plot_box(history_dict: Dict[str, pd.DataFrame], out_dir: Path) -\u003e Path:\n    frames = []\n    for rid, hist in history_dict.items():\n        if \"dev_exact_match\" in hist.columns:\n            df_tmp = hist[[\"_step\", \"dev_exact_match\"]].copy()\n            df_tmp[\"run_id\"] = rid\n            frames.append(df_tmp)\n    if not frames:\n        return Path()\n    df_all = pd.concat(frames, axis=0)\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(data=df_all, x=\"run_id\", y=\"dev_exact_match\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Dev EM (%)\")\n    plt.tight_layout()\n    fname = out_dir / \"comparison_boxplot_dev_em.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n################################################################################\n#                                 MAIN                                        #\n################################################################################\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory that holds per-run artefacts\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list of run IDs to evaluate\")\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).absolute()\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n\n    api = wandb.Api()\n\n    aggregated: Dict[str, Any] = {\"primary_metric\": PRIMARY_METRIC, \"metrics\": {}}\n    primary_metric_values: Dict[str, float] = {}\n    histories_for_box: Dict[str, pd.DataFrame] = {}\n\n    for rid in run_ids:\n        # --------------- Retrieve from WandB -------------------- #\n        run = api.run(f\"gengaru617-personal/2025-1108-1/{rid}\")\n        history = run.history()      # dataframe of all logged metrics\n        summary = dict(run.summary)\n        config = dict(run.config)\n        histories_for_box[rid] = history\n\n        # --------------- Local output dirs --------------------- #\n        out_dir = results_root / rid\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        # --------------- Load predictions if available --------- #\n        preds_path = out_dir / \"dev_predictions.json\"\n        if preds_path.exists():\n            with open(preds_path) as fp:\n                preds_obj = json.load(fp)\n                preds, answers = preds_obj[\"pred\"], preds_obj[\"answer\"]\n        else:\n            preds, answers = [], []\n\n        # --------------- Per-run figures ----------------------- #\n        lc_path = _plot_learning_curve(rid, history, out_dir)\n        cm_path = _plot_confusion(rid, preds, answers, out_dir) if preds else None\n        print(lc_path)\n        if cm_path:\n            print(cm_path)\n\n        # --------------- Save metrics JSON --------------------- #\n        _save_json({\"history\": history.to_dict(orient=\"list\"), \"summary\": summary, \"config\": config}, out_dir / \"metrics.json\")\n\n        # --------------- Aggregation --------------------------- #\n        primary_val = summary.get(\"final_dev_em\") or summary.get(\"dev_exact_match\") or 0.0\n        primary_metric_values[rid] = float(primary_val)\n        for k, v in summary.items():\n            aggregated[\"metrics\"].setdefault(k, {})[rid] = float(v) if isinstance(v, (int, float)) else v\n\n    # --------------- Select best proposed / baseline ---------- #\n    best_prop_id, best_prop_val = max(((r, v) for r, v in primary_metric_values.items() if \"proposed\" in r), key=lambda x: x[1])\n    best_base_id, best_base_val = max(((r, v) for r, v in primary_metric_values.items() if (\"comparative\" in r or \"baseline\" in r)), key=lambda x: x[1])\n    gap_pct = ((best_prop_val - best_base_val) / (best_base_val + 1e-9)) * 100\n\n    aggregated.update({\n        \"best_proposed\": {\"run_id\": best_prop_id, \"value\": best_prop_val},\n        \"best_baseline\": {\"run_id\": best_base_id, \"value\": best_base_val},\n        \"gap\": gap_pct,\n    })\n\n    # --------------- Statistical significance ---------------- #\n    if len(histories_for_box) \u003e= 2:\n        p_values = {}\n        for r1, r2 in combinations(histories_for_box.keys(), 2):\n            h1 = histories_for_box[r1][\"dev_exact_match\"].dropna()\n            h2 = histories_for_box[r2][\"dev_exact_match\"].dropna()\n            if len(h1) and len(h2):\n                t_stat, p_val = ttest_ind(h1, h2, equal_var=False)\n                p_values[f\"{r1}_vs_{r2}\"] = {\"t\": float(t_stat), \"p\": float(p_val)}\n        aggregated[\"significance_tests\"] = p_values\n\n    _save_json(aggregated, comparison_dir / \"aggregated_metrics.json\")\n\n    # --------------- Comparison figures ----------------------- #\n    bar_path = _plot_comparison_bar(primary_metric_values, comparison_dir)\n    box_path = _plot_box(histories_for_box, comparison_dir)\n    print(bar_path)\n    if box_path:\n        print(box_path)\n\nif __name__ == \"__main__\":\n    main()", "main_py": "import os\nimport subprocess\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig\n\n################################################################################\n#                               LAUNCHER                                       #\n################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    repo_root = Path(get_original_cwd())\n\n    run_cfg = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg.exists():\n        raise FileNotFoundError(f\"Run file not found: {run_cfg}\")\n\n    cmd = [\n        os.getenv(\"PYTHON\", \"python\"),\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"mode={cfg.mode}\",\n        f\"results_dir={cfg.results_dir}\",\n    ]\n    print(\"[main] Launching subprocess:\\n  \", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "from typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n################################################################################\n#                              MODEL BUILDER                                   #\n################################################################################\n\ndef build_model(model_cfg):\n    tokenizer = AutoTokenizer.from_pretrained(model_cfg.name, cache_dir=\".cache/\", trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_cfg.name,\n        torch_dtype=torch.bfloat16 if str(model_cfg.dtype).lower().startswith(\"bf\") else None,\n        device_map=\"auto\",\n        cache_dir=\".cache/\",\n        trust_remote_code=True,\n    )\n\n    l_conf = LoraConfig(\n        r=model_cfg.lora.r,\n        lora_alpha=model_cfg.lora.alpha,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n        lora_dropout=model_cfg.lora.dropout,\n    )\n    model = get_peft_model(model, l_conf)\n    return model, tokenizer\n\n################################################################################\n#                               HELPERS                                        #\n################################################################################\n\ndef compute_token_entropy(logits: torch.Tensor) -\u003e torch.Tensor:\n    probs = F.softmax(logits, dim=-1)\n    log_p = torch.log(probs + 1e-12)\n    ent = -(probs * log_p).sum(-1).mean(0)\n    return ent.detach()\n\n\ndef compute_curvature(model) -\u003e torch.Tensor:\n    vals: List[torch.Tensor] = []\n    for n, p in model.named_parameters():\n        if (\"lora_A\" in n or \"lora_B\" in n) and p.grad is not None:\n            vals.append(p.grad.detach().pow(2).mean())\n    if not vals:\n        return torch.zeros(1, device=next(model.parameters()).device)\n    return torch.stack(vals)\n\n################################################################################\n#                      LR PLANNERS: GRAFF \u0026 REACTOR                            #\n################################################################################\n\nclass _GNNLayer(nn.Module):\n    def __init__(self, d: int):\n        super().__init__()\n        self.msg = nn.Linear(d * 2, d)\n        self.upd = nn.GRUCell(d, d)\n\n    def forward(self, h):\n        N, D = h.shape\n        h_i = h.unsqueeze(1).expand(N, N, D)\n        h_j = h.unsqueeze(0).expand(N, N, D)\n        m = self.msg(torch.cat([h_i, h_j], -1)).mean(1)\n        h_new = self.upd(m, h)\n        return h_new\n\n\nclass GraffScheduler(nn.Module):\n    \"\"\"Differentiable budget-aware LR planner (our proposed method).\"\"\"\n\n    def __init__(self, model, cfg):\n        super().__init__()\n        self.n_nodes = sum(1 for n, _ in model.named_parameters() if (\"lora_A\" in n or \"lora_B\" in n))\n        self.h = nn.Parameter(torch.zeros(self.n_nodes, 16))\n        self.encoder = nn.Linear(3, 16)\n        self.gnn = _GNNLayer(16)\n        self.decoder = nn.Linear(16, 1)\n        self.stab_guard = cfg.training.scheduler.stability_guard\n\n    def step(self, curv: torch.Tensor, ent: torch.Tensor, budget_left: float) -\u003e float:\n        if curv.numel() \u003c self.n_nodes:\n            curv = torch.cat([curv, curv.new_zeros(self.n_nodes - curv.numel())])\n        if ent.numel() \u003c self.n_nodes:\n            ent = ent.mean().expand(self.n_nodes)\n        features = torch.stack([curv, ent, curv.new_full((self.n_nodes,), budget_left)], -1)\n        self.h = self.gnn(self.h + self.encoder(features))\n        lr_log = self.decoder(self.h.mean(0)).squeeze()\n        eta = torch.exp(lr_log).clamp(max=10.0)\n\n        # Edge-of-stability guard\n        c_max = curv.max().item() if curv.numel() else 1.0\n        if eta * c_max \u003e self.stab_guard:\n            eta = self.stab_guard / max(c_max, 1e-6)\n        return eta.item()\n\n\nclass ReactorScheduler(nn.Module):\n    \"\"\"REINFORCE-style LR controller (baseline).\"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.lr_log = nn.Parameter(torch.tensor(0.0))\n        self.baseline = 0.0\n        self.decay = cfg.training.scheduler.baseline_decay\n        self.entropy_bonus = cfg.training.scheduler.entropy_bonus\n\n    def step(self, loss_val: torch.Tensor) -\u003e float:\n        reward = -loss_val.item()\n        adv = reward - self.baseline\n        self.baseline = self.baseline * self.decay + reward * (1 - self.decay)\n        # Straight-through gradient assignment\n        self.lr_log.grad = torch.tensor(-adv, device=self.lr_log.device)\n        return torch.exp(self.lr_log).item()", "preprocess_py": "from functools import partial\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\n################################################################################\n#                               TOKENISATION                                  #\n################################################################################\n\ndef _format_example(example: Dict, tokenizer: AutoTokenizer, max_tokens: int):\n    \"\"\"Convert a GSM8K row into token IDs for causal-LM fine-tuning.\"\"\"\n    question = example[\"question\"].strip()\n    answer = example[\"answer\"].split(\"####\")[-1].strip()\n    prompt = f\"Question: {question}\\nAnswer: \"\n    full_text = f\"{prompt}{answer}\"\n\n    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_tokens)[\"input_ids\"]\n    full_enc = tokenizer(full_text, truncation=True, max_length=max_tokens)\n\n    return {\n        \"input_ids\": full_enc[\"input_ids\"],\n        \"labels\": full_enc[\"input_ids\"],\n        \"attention_mask\": full_enc[\"attention_mask\"],\n        \"prompt_ids\": prompt_ids,\n        \"prompt_mask\": [1] * len(prompt_ids),\n        \"answer_text\": answer,\n    }\n\n################################################################################\n#                                  COLLATE                                     #\n################################################################################\n\ndef _collate_fn(batch: List[Dict], pad_token_id: int):\n    keys_to_pad = [\"input_ids\", \"labels\", \"prompt_ids\", \"prompt_mask\"]\n    out: Dict[str, torch.Tensor] = {}\n    for k in keys_to_pad:\n        seqs = [torch.tensor(x[k], dtype=torch.long) for x in batch]\n        pad_val = pad_token_id if k != \"prompt_mask\" else 0\n        out[k] = pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n    out[\"attention_mask\"] = (out[\"input_ids\"] != pad_token_id).long()\n    out[\"answers\"] = [x[\"answer_text\"] for x in batch]\n    return out\n\n################################################################################\n#                             DATALOADER BUILDERS                              #\n################################################################################\n\ndef build_dataloaders(tokenizer: AutoTokenizer, cfg: DictConfig) -\u003e Tuple[DataLoader, DataLoader]:\n    cache_dir = \".cache/\"\n\n    train_ds = load_dataset(\n        cfg.dataset.name,\n        split=cfg.dataset.train_split,\n        streaming=cfg.dataset.streaming,\n        cache_dir=cache_dir,\n    )\n    train_ds = train_ds.map(\n        partial(_format_example, tokenizer=tokenizer, max_tokens=cfg.dataset.max_tokens),\n        remove_columns=None if cfg.dataset.streaming else train_ds.column_names,\n    )\n\n    if cfg.dataset.streaming and cfg.dataset.shuffle_buffer \u003e 0:\n        train_ds = train_ds.shuffle(buffer_size=cfg.dataset.shuffle_buffer, seed=42)\n\n    eval_ds = load_dataset(\n        cfg.dataset.name,\n        split=cfg.dataset.eval_split,\n        streaming=False,\n        cache_dir=cache_dir,\n    ).map(\n        partial(_format_example, tokenizer=tokenizer, max_tokens=cfg.dataset.max_tokens),\n        remove_columns=eval_ds.column_names,\n    )\n\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    collate = partial(_collate_fn, pad_token_id=tokenizer.pad_token_id)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.micro_batch_size,\n        collate_fn=collate,\n        shuffle=not cfg.dataset.streaming,\n    )\n    eval_loader = DataLoader(\n        eval_ds,\n        batch_size=cfg.training.micro_batch_size,\n        collate_fn=collate,\n        shuffle=False,\n    )\n    return train_loader, eval_loader\n\n################################################################################\n#                              METRIC: EM (%)                                  #\n################################################################################\n\nclass EMAccuracy:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n        self.last_value = 0.0\n\n    def update(self, preds: List[str], refs: List[str]):\n        for p, r in zip(preds, refs):\n            self.correct += int(p.strip() == r.strip())\n            self.total += 1\n        if self.total:\n            self.last_value = 100.0 * self.correct / self.total\n\n    def compute(self) -\u003e float:\n        return self.last_value", "pyproject_toml": "[project]\nname = \"graff-experiments\"\nversion = \"0.1.0\"\ndescription = \"GRAFF learning-rate planner experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.2.0\"\ntransformers = \"^4.38.0\"\npeft = \"^0.9.0\"\ndatasets = \"^2.17.0\"\nwandb = \"^0.16.4\"\noptuna = \"^3.5.0\"\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.2\"\npandas = \"^2.2.1\"\nnumpy = \"^1.26.4\"\nscikit-learn = \"^1.4.1\"\nscipy = \"^1.12.0\"", "train_py": "import json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport hydra\nimport optuna\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom .model import (GraffScheduler, ReactorScheduler, build_model,\n                    compute_curvature, compute_token_entropy)\nfrom .preprocess import EMAccuracy, build_dataloaders\n\n################################################################################\n#                               MODE OVERRIDES                                #\n################################################################################\n\ndef _apply_mode_overrides(cfg: DictConfig) -\u003e None:\n    \"\"\"Modify cfg in-place w.r.t. requested execution *mode* (trial | full).\"\"\"\n    if cfg.mode == \"trial\":\n        # Light-weight validation pass\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.total_steps = max(2, cfg.training.gradient_accumulation_steps)\n        cfg.training.budget_minutes = 0.05  # ~3 seconds wall-clock\n        cfg.evaluation.interval_steps = 1\n        cfg.training.max_epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(f\"Unknown mode {cfg.mode}. Use \u0027full\u0027 or \u0027trial\u0027.\")\n\n################################################################################\n#                           WANDB INITIALISATION                              #\n################################################################################\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n        return None\n\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n    )\n    print(f\"[wandb] URL: {run.get_url()}\")\n    return run\n\n################################################################################\n#                             TRAINING ROUTINE                                #\n################################################################################\n\ndef _single_training_run(cfg: DictConfig) -\u003e float:\n    \"\"\"Run *one* training job with the parameters in cfg. Returns final EM.\"\"\"\n\n    # ------------------------------------------------------------------\n    # 0. I/O set-up\n    # ------------------------------------------------------------------\n    results_root = Path(cfg.results_dir)\n    run_dir = results_root / cfg.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save resolved config for reproducibility\n    with open(run_dir / \"resolved_config.yaml\", \"w\") as f:\n        OmegaConf.save(cfg, f)\n\n    # ------------------------------------------------------------------\n    # 1. Build model \u0026 tokenizer\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model, tokenizer = build_model(cfg.model)\n    model.to(device)\n    model.train()\n\n    # ------------------------------------------------------------------\n    # 2. Data loaders\n    # ------------------------------------------------------------------\n    train_loader, eval_loader = build_dataloaders(tokenizer, cfg)\n\n    # ------------------------------------------------------------------\n    # 3. Optimisers \u0026 LR-planners\n    # ------------------------------------------------------------------\n    base_lr = cfg.training.optimizer.lr_init\n    inner_opt = torch.optim.AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=base_lr,\n        betas=tuple(cfg.training.optimizer.betas),\n        weight_decay=cfg.training.optimizer.weight_decay,\n    )\n\n    # Build LR planner + its optimiser\n    if cfg.training.scheduler.type == \"graff\":\n        lr_planner = GraffScheduler(model, cfg)\n        outer_opt = torch.optim.Adam(lr_planner.parameters(), lr=cfg.training.scheduler.outer_lr)\n    elif cfg.training.scheduler.type == \"reactor\":\n        lr_planner = ReactorScheduler(cfg)\n        outer_opt = torch.optim.Adam(lr_planner.parameters(), lr=cfg.training.scheduler.controller_lr)\n    else:\n        raise ValueError(f\"Unknown scheduler {cfg.training.scheduler.type}\")\n\n    scaler = torch.cuda.amp.GradScaler(\n        enabled=\"bf16\" in str(cfg.training.mixed_precision).lower() and device.type == \"cuda\"\n    )\n\n    # ------------------------------------------------------------------\n    # 4. Trackers\n    # ------------------------------------------------------------------\n    em_meter = EMAccuracy()\n    start_wall = time.time()\n    budget_sec = cfg.training.budget_minutes * 60.0 if cfg.training.budget_minutes \u003e 0 else float(\"inf\")\n\n    wandb_run = _init_wandb(cfg)\n\n    step, accum = 0, 0\n\n    while step \u003c cfg.training.total_steps and (time.time() - start_wall) \u003c budget_sec:\n        for batch in train_loader:\n            if step \u003e= cfg.training.total_steps or (time.time() - start_wall) \u003e budget_sec:\n                break\n\n            # ------------------ Forward \u0026 backward ------------------ #\n            with torch.autocast(\n                device_type=device.type,\n                dtype=torch.bfloat16 if \"bf16\" in str(cfg.training.mixed_precision).lower() else torch.float32,\n            ):\n                outputs = model(\n                    input_ids=batch[\"input_ids\"].to(device),\n                    attention_mask=batch[\"attention_mask\"].to(device),\n                    labels=batch[\"labels\"].to(device),\n                )\n                loss = outputs.loss / cfg.training.gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum += 1\n\n            if accum \u003c cfg.training.gradient_accumulation_steps:\n                continue  # accumulate more micro batches\n\n            accum = 0\n            # ---------------- Planner update ----------------------- #\n            curvatures = compute_curvature(model)\n            entropies = compute_token_entropy(outputs.logits.detach())\n            remaining_budget = max(budget_sec - (time.time() - start_wall), 0.0) / budget_sec\n\n            # zero grad on planner *before* it writes .grad (fixes issue #5)\n            outer_opt.zero_grad(set_to_none=True)\n            if cfg.training.scheduler.type == \"graff\":\n                lr_factor = lr_planner.step(curvatures, entropies, remaining_budget)\n            else:  # reactor (writes grad internally)\n                lr_factor = lr_planner.step(loss.detach())\n            outer_opt.step()\n            outer_opt.zero_grad(set_to_none=True)\n\n            # apply new LR to inner optimiser\n            new_lr = base_lr * lr_factor\n            for g in inner_opt.param_groups:\n                g[\"lr\"] = new_lr\n\n            # ---------------- Model weights update ------------------ #\n            scaler.unscale_(inner_opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.clip_grad_norm)\n            scaler.step(inner_opt)\n            scaler.update()\n            inner_opt.zero_grad(set_to_none=True)\n\n            # ---------------- Logging ------------------------------ #\n            if wandb_run is not None:\n                wandb_run.log({\n                    \"train_loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                    \"lr\": new_lr,\n                }, step=step)\n\n            step += 1\n\n            # ---------------- Periodic eval ------------------------- #\n            if step % cfg.evaluation.interval_steps == 0 or step == cfg.training.total_steps:\n                model.eval()\n                em_meter.reset()\n                all_preds: List[str] = []\n                all_ans: List[str] = []\n                with torch.no_grad():\n                    for e_idx, e_batch in enumerate(eval_loader):\n                        gen = model.generate(\n                            input_ids=e_batch[\"prompt_ids\"].to(device),\n                            attention_mask=e_batch[\"prompt_mask\"].to(device),\n                            max_new_tokens=128,\n                            do_sample=False,\n                        )\n                        p_len = e_batch[\"prompt_ids\"].size(1)\n                        preds = tokenizer.batch_decode(gen[:, p_len:], skip_special_tokens=True)\n                        em_meter.update(preds, e_batch[\"answers\"])\n                        all_preds.extend(preds)\n                        all_ans.extend(e_batch[\"answers\"])\n                        if cfg.mode == \"trial\" and e_idx \u003e 1:\n                            break  # keep eval short in trial mode\n                dev_em = em_meter.compute()\n\n                # save predictions for later confusion-matrix building\n                with open(run_dir / \"dev_predictions.json\", \"w\") as fp:\n                    json.dump({\"pred\": all_preds, \"answer\": all_ans}, fp)\n\n                if wandb_run is not None:\n                    wandb_run.log({\"dev_exact_match\": dev_em}, step=step)\n\n                model.train()\n\n    final_em = em_meter.last_value\n\n    # ---------------- Summaries \u0026 artefacts ------------------------- #\n    summary_path = run_dir / \"final_metrics.json\"\n    with open(summary_path, \"w\") as f:\n        json.dump({\"final_dev_em\": final_em}, f, indent=2)\n\n    if wandb_run is not None:\n        wandb_run.summary[\"final_dev_em\"] = final_em\n        wandb_run.finish()\n\n    return final_em\n\n################################################################################\n#                           OPTUNA  + FINAL RUN                               #\n################################################################################\n\ndef _run_optuna(cfg: DictConfig):\n    \"\"\"Hyper-parameter search with Optuna. Only the *best* run is logged to WandB.\"\"\"\n    search_space: Dict[str, Any] = cfg.optuna.search_space  # type: ignore\n\n    def _suggest(trial: optuna.Trial, hp_path: str, hp_cfg: Dict[str, Any]):\n        if hp_cfg.type == \"loguniform\":\n            return trial.suggest_float(hp_path, hp_cfg.low, hp_cfg.high, log=True)\n        if hp_cfg.type == \"uniform\":\n            return trial.suggest_float(hp_path, hp_cfg.low, hp_cfg.high, log=False)\n        if hp_cfg.type == \"categorical\":\n            return trial.suggest_categorical(hp_path, hp_cfg.choices)\n        raise ValueError(f\"Unsupported space type {hp_cfg.type}\")\n\n    def objective(trial: optuna.Trial):\n        trial_cfg = OmegaConf.deepcopy(cfg)\n        # turn off logging for *intermediate* trials\n        trial_cfg.wandb.mode = \"disabled\"\n        for hp_name, hp_def in search_space.items():\n            sampled_val = _suggest(trial, hp_name, hp_def)\n            OmegaConf.update(trial_cfg, hp_name.replace(\"_\", \".\"), sampled_val, merge=True)\n        return _single_training_run(trial_cfg)\n\n    study = optuna.create_study(direction=cfg.optuna.direction)\n    study.optimize(objective, n_trials=cfg.optuna.n_trials)\n\n    print(\"[optuna] Best value:\", study.best_value)\n    print(\"[optuna] Best params:\", study.best_params)\n\n    # Inject best params into cfg and perform the *production* run with WandB\n    for k, v in study.best_params.items():\n        OmegaConf.update(cfg, k.replace(\"_\", \".\"), v, merge=True)\n\n    _single_training_run(cfg)\n\n################################################################################\n#                              HYDRA  ENTRY                                   #\n################################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef train_app(cfg: DictConfig):\n    # --------------------------------------------------------------\n    # Load run-specific YAML and merge with defaults\n    # --------------------------------------------------------------\n    repo_root = Path(get_original_cwd())\n    run_cfg_path = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_cfg_path.exists():\n        raise FileNotFoundError(f\"Run config missing: {run_cfg_path}\")\n\n    run_cfg = OmegaConf.load(run_cfg_path)\n    cfg = OmegaConf.merge(cfg, run_cfg)\n\n    # Ensure results_dir is absolute \u0026 exists\n    cfg.results_dir = str(Path(cfg.results_dir).absolute())\n\n    # Set resolved run_id\n    cfg.run_id = cfg.get(\"run_id\", cfg.run)\n\n    # Mode-specific tweaks\n    _apply_mode_overrides(cfg)\n\n    # -------------------- Launch training ------------------------- #\n    if cfg.optuna.n_trials \u003e 0:\n        _run_optuna(cfg)\n    else:\n        _single_training_run(cfg)\n\n\nif __name__ == \"__main__\":\n    train_app()"}



Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The current codebase cannot be executed as-is and violates several core validation requirements:\n\n1. src/evaluate.py\n   • Missing `from typing import Any` – run-time NameError when the module is imported (annotations are evaluated at runtime).\n   • Hard-coded WandB entity/project instead of loading them from config/config.yaml (requirement 8).\n   • Does not read config/config.yaml at all, so evaluation is not configuration-independent.\n\n2. src/preprocess.py\n   • In build_dataloaders(), `eval_ds` is referenced inside `remove_columns=eval_ds.column_names` before `eval_ds` is defined, causing an UnboundLocalError.\n\n3. Hydra / subprocess path handling\n   • src/main.py launches `python -m src.train` from inside Hydra’s run directory without ensuring that the repository root is on PYTHONPATH.  When using implicit namespace packages this often fails to locate the `src` package from the child process.\n\n4. Optuna integration\n   • In train.py, hyper-parameter keys are updated via `hp_name.replace(\"_\", \".\")`, corrupting hierarchical paths (e.g. `adapter_lr_init` becomes `adapter.lr.init`).  This prevents searched parameters from ever being injected into the config.\n\n5. evaluate.py figure/metric generation\n   • Confusion matrix is built with `y_true = correct; y_pred = correct`, always producing a perfect diagonal – defeats its purpose.\n\nBecause of these issues the code is not fully functional or publication-ready and cannot be executed immediately for experiments."
}
