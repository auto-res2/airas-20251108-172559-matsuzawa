
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/runs/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/runs/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
       ```json
       {
         "primary_metric": "Exact-match accuracy at 30 min wall-clock.",
         "metrics": {
           "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
           "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
         },
         "best_proposed": {
           "run_id": "proposed-iter2-model-dataset",
           "value": 0.92
         },
         "best_baseline": {
           "run_id": "comparative-1-model-dataset",
           "value": 0.88
         },
         "gap": 4.55
       }
       ```
       The structure must include:
       - "primary_metric": The primary evaluation metric name from the hypothesis
       - "metrics": All collected metrics organized by metric name, then by run_id
       - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
       - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
       - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
         * Must use the expected results from the hypothesis to determine metric direction (higher vs lower is better)
         * If the metric should be minimized, reverse the sign of the gap
         * The gap represents the percentage improvement of the proposed method over the best baseline
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Hypothesis
open_problems='1.  Current LR schedulers for PEFT treat every training step in isolation.  However, GSM8K fine-tuning lasts only ≈2 000 optimiser steps, so step–to–step LR correlations dominate final accuracy.  No method explicitly *models* these correlations or re-uses experience collected on earlier budgets.\n2.  Token-level difficulty is visible before the backward pass through inexpensive forward-only probes (e.g. logit entropy after the **first** adapter layer).  Existing token-aware tuners (BATS) wait until the full backward pass finishes, missing a chance to scale the *next* gradient.\n3.  Compute budgets in practice are often set *per experiment* (e.g. “one hour on one A100”), not per step.  Scheduling should optimise the *entire training trajectory* under a user-supplied wall-clock budget B without the need for k-step roll-outs or quadratic programming.\n4.  Even the best RL-based controller (REACTOR) needs policy-gradient updates with high-variance rewards and hand-picked baselines; this slows convergence and couples the tuner’s learning rate to the very problem it tries to solve.' method='We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.\n\nA.  Budget-conditioned trajectory encoder\n   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.\n   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.\n\nB.  Closed-form LR decoder\n   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.\n   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).\n\nC.  Self-supervised training objective\n   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.\n   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.\n\nD.  Zero-shot transfer & warm start\n   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.\n\nImplementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).' experimental_setup='Models: Qwen3-0.6B-Chat (primary) and Mistral-7B-Instruct (transfer) with LoRA r=64.\nData: GSM8K train/dev; GSM1k for contamination test.\nBudgets: 30 min and 60 min on single A100-80 GB (wall-clock, include eval).\nBaselines: Fixed LR, SALeLR, BATS, REACTOR (all tuned), plus human-tuned cosine.\nMetrics: 1) dev EM at budget B; 2) AUC(EM-vs-time)/B; 3) energy (Joules) from nvidia-smi.\nAblations: (a) remove graph edges → MLP; (b) remove budget feature; (c) freeze W (no online).' primary_metric='Exact-match accuracy at 30 min wall-clock.' experimental_code='# GRAFF – core\nclass GNN(nn.Module):\n    def __init__(self,d):\n        super().__init__(); self.msg=nn.Linear(d*2,64); self.up=nn.GRUCell(64,d)\n    def forward(self,x,adj):\n        m=self.msg(torch.cat([x[adj[0]],x[adj[1]]],-1))\n        agg=torch.zeros_like(x); agg.index_add_(0,adj[1],m)\n        return self.up(agg.reshape(-1,x.size(-1)),x.reshape(-1,x.size(-1))).view_as(x)\nclass Graff(nn.Module):\n    def __init__(self,blocks):\n        super().__init__(); self.h=nn.Parameter(torch.zeros(len(blocks),16));\n        self.gnn=GNN(16); self.W=nn.Linear(16,1)\n    def forward(self,curv,entr,budget,adj):\n        x=torch.cat([curv,entr,budget.expand_as(curv)],-1)\n        self.h=self.gnn(self.h+x,adj)\n        eta=torch.exp(self.W(self.h.mean(0)))\n        m=torch.sigmoid(self.W(self.h)).squeeze()\n        return eta,m\n' expected_result='Dev EM @30 min: Fixed-LR 74 %, SALeLR 84 %, BATS 88.5 %, REACTOR 90.8 %, GRAFF 92.4 ± 0.2 %.\nEnergy usage to reach 90 % EM: REACTOR 1.00 ×, GRAFF 0.83 ×.\nAUC improves 22 % over REACTOR.\nZero-shot on Mistral-7B: +1.8 pp EM vs REACTOR with same W.' expected_conclusion='By replacing high-variance RL with a differentiable graph-recurrent encoder/decoder conditioned on remaining wall-clock budget, GRAFF learns an end-to-end compute-aware LR trajectory in closed form.  It exploits cheap early-layer entropy to pre-scale gradients, respects global time constraints, and transfers across model sizes, achieving superior EM per Joule without external solvers or policy-gradient tuning.'

# Current Research Method
We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.

A.  Budget-conditioned trajectory encoder
   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.
   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.

B.  Closed-form LR decoder
   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.
   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).

C.  Self-supervised training objective
   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.
   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.

D.  Zero-shot transfer & warm start
   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.

Implementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).

# Experimental Design
- Strategy: This experiment evaluates GRAFF—our new, differentiable, budget-aware learning-rate (LR) planner—against a strong token-aware RL baseline (REACTOR) when LoRA-fine-tuning a 0.6-billion-parameter Qwen3 model on the GSM8K mathematics dataset.  

Workflow
1. Data loading: GSM8K train/dev is streamed with length-bucketing (batch_size = 64) and mixed-precision (bfloat16).  
2. Model: Qwen3-0.6B is equipped with LoRA adapters (rank = 64, α = 16) on all self-attention and MLP layers; only adapter weights are trainable.  
3. GRAFF module: each mini-batch’s forward pass collects two cheap probes per adapter block—logit entropy after the first adapter layer and Hessian-free curvature estimate Δλᵢ.  Node features = [Δλᵢ, Hᵢ, remaining_budget].  A 2-layer message-passing GNN updates the latent state h_t, after which a single linear decoder W outputs (a) global LR η_t for the optimiser and (b) per-token multipliers m_{t,i}.  W is projected after every update to satisfy η_t·λ̂_max ≤ 1.9.  
4. Joint optimisation: the normal model loss plus the compute-normalised regret R_T is back-propagated once; Adam (lr = 2e-4) updates adapter weights while a second Adam (lr = 1e-3) updates W.  
5. Budget tracking: wall-clock time is monitored with CUDA events; once 30-min or 60-min budgets are exhausted, training halts and dev exact-match (EM) is recorded.  
6. Baseline: REACTOR replaces GRAFF but keeps all other settings identical.  
7. Hyper-parameter search over three key knobs (see below) is done with 50 trials of Optuna using the 30-min EM as the objective.  
8. Metrics collected: dev EM @30 min (primary), dev EM @60 min, AUC(EM-vs-time)/B, and energy (Joules) read every 5 s from nvidia-smi.  
The entire pipeline fits comfortably into an 80 GB A100 (peak ≈34 GB) and <10 GB system RAM per worker; RAM-heavy dataloading is disabled by streaming.
- Proposed Method: GRAFF (Graph-Recurrent Allocation of Fine-tuning Funds) is a learning-rate scheduler that plans an entire training trajectory in closed form while respecting a user-supplied wall-clock budget B.

Key components
1. Budget-conditioned encoder: Each adapter block is a node in a lightweight graph neural network.  Node features include instantaneous curvature (Δλᵢ) estimated via a single Hessian-vector product, token-level entropy (Hᵢ) after the first adapter layer, and the remaining budget b_t = B – time_so_far.  One message-passing step updates latent states h_t.
2. Linear decoder: A single weight matrix W maps the mean of h_t to log η_t (global LR) and each h_t to token multipliers m_{t,i}.  Because the decoder is linear, gradients wrt W flow through the normal loss; no REINFORCE or separate controller optimisation is needed.
3. Stability guard: After every optimiser step, W is projected so that η_t·λ̂_max ≤ 1.9, keeping updates near the edge of stability.
4. Objective: Compute-normalised regret R_T = (ℒ_T – ℒ_best)/time_T is differentiable in W through η_t, allowing simultaneous optimisation of adapter weights and scheduler parameters with standard back-prop.
5. Warm-start: W is pre-trained for 300 mini-batches on a 70 M-parameter Qwen-tiny model; the same W is then continuously fine-tuned online when larger models are trained, enabling zero-shot transfer across sizes.

Algorithmic procedure per mini-batch
(1) Forward pass through model and probes → collect Δλᵢ, Hᵢ.
(2) GNN update → h_t.
(3) Decode (η_t, m_{t,i}); scale optimiser LR and token gradients.
(4) Backward pass; accumulate gradients of both adapter weights and W.
(5) Adam steps for both parameter sets; project W; log time & energy.
- Evaluation Metrics: ['Exact-match accuracy at 30 min wall-clock', 'Exact-match accuracy at 60 min wall-clock', 'AUC(EM-vs-time)/B', 'Energy (Joules)', 'Exact-match accuracy at 30 min wall-clock.']

# Experiment Runs

- Run ID: proposed-iter1-Qwen3-0.6B-gsm8k
  Method: proposed
  Model: Qwen3-0.6B
  Dataset: gsm8k
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Qwen3-0.6B-gsm8k
method: proposed  # GRAFF scheduler
model:
  name: Qwen/Qwen3-0.6B
  dtype: bfloat16
  lora:
    r: 64            # LoRA rank
    alpha: 16
    dropout: 0.05
  parameter_count: 0.6B

dataset:
  name: openai/gsm8k
  train_split: train
  eval_split: test
  max_tokens: 2048
  streaming: true
  shuffle_buffer: 10000

training:
  budget_minutes: 30            # wall-clock budget B
  global_batch_size: 64         # after gradient accumulation
  micro_batch_size: 8
  gradient_accumulation_steps: 8
  total_steps: 2000             # safety cap; early-stop by budget
  mixed_precision: bf16
  gradient_checkpointing: true
  clip_grad_norm: 1.0

  optimizer:
    type: adamw
    lr_init: 2e-4               # will be rescaled by GRAFF
    betas: [0.9, 0.98]
    weight_decay: 0.01

  scheduler:
    type: graff
    outer_lr: 1e-3              # Adam lr for W
    stability_guard: 1.9        # η·λ̂_max ≤ 1.9
    warm_start_ckpt: graff_pretrain_qwen_tiny.pt

evaluation:
  interval_steps: 100
  metrics:
    - exact_match

optuna:
  n_trials: 50
  direction: maximize
  search_space:
    adapter_lr_init:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    graff_outer_lr:
      type: loguniform
      low: 5.0e-4
      high: 2.0e-3
    lora_r:
      type: categorical
      choices: [32, 64, 96]

  ```
  

- Run ID: comparative-1-iter1-Qwen3-0.6B-gsm8k
  Method: comparative-1
  Model: Qwen3-0.6B
  Dataset: gsm8k
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Qwen3-0.6B-gsm8k
method: comparative-1  # REACTOR RL scheduler
model:
  name: Qwen/Qwen3-0.6B
  dtype: bfloat16
  lora:
    r: 64
    alpha: 16
    dropout: 0.05
  parameter_count: 0.6B

dataset:
  name: openai/gsm8k
  train_split: train
  eval_split: test
  max_tokens: 2048
  streaming: true
  shuffle_buffer: 10000

training:
  budget_minutes: 30
  global_batch_size: 64
  micro_batch_size: 8
  gradient_accumulation_steps: 8
  total_steps: 2000
  mixed_precision: bf16
  gradient_checkpointing: true
  clip_grad_norm: 1.0

  optimizer:
    type: adamw
    lr_init: 2e-4            # initial LR before REACTOR scaling
    betas: [0.9, 0.98]
    weight_decay: 0.01

  scheduler:
    type: reactor
    controller_lr: 1e-4      # policy-gradient step size
    baseline_decay: 0.99     # EMA baseline for REINFORCE
    entropy_bonus: 0.01

evaluation:
  interval_steps: 100
  metrics:
    - exact_match

optuna:
  n_trials: 50
  direction: maximize
  search_space:
    adapter_lr_init:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    controller_lr:
      type: loguniform
      low: 5.0e-5
      high: 5.0e-4
    lora_r:
      type: categorical
      choices: [32, 64, 96]

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n\n# ---------------- CLI overrides --------------- #\nrun: ???\nmode: full   # full | trial\nresults_dir: ./outputs\n\n# ---------------- WandB ----------------------- #\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online\n\n# ---------------- Optuna ---------------------- #\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# ---------------- Dataset --------------------- #\ndataset:\n  name: null\n  train_split: train\n  eval_split: test\n  max_tokens: 2048\n  streaming: false\n  shuffle_buffer: 10000\n\n# ---------------- Training -------------------- #\ntraining:\n  budget_minutes: 30\n  global_batch_size: 64\n  micro_batch_size: 8\n  gradient_accumulation_steps: 8\n  total_steps: 2000\n  max_epochs: 100000\n  mixed_precision: bf16\n  clip_grad_norm: 1.0\n\n  optimizer:\n    lr_init: 2e-4\n    betas: [0.9, 0.95]\n    weight_decay: 0.01\n\n  scheduler:\n    type: graff   # graff | reactor\n    outer_lr: 1e-3\n    controller_lr: 1e-4\n    stability_guard: 1.9\n    baseline_decay: 0.99\n    entropy_bonus: 0.01\n\n# ---------------- Evaluation ------------------ #\nevaluation:\n  interval_steps: 100\n  metrics: [exact_match]", "evaluate_py": "import argparse\nimport json\nfrom collections import Counter\nfrom itertools import combinations\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom sklearn.metrics import confusion_matrix, ttest_ind\n\nPRIMARY_METRIC_NAME = \"Exact-match accuracy at 30 min wall-clock.\"\n\n###############################################################################\n#                               UTILITIES                                     #\n###############################################################################\n\ndef _save_json(obj: Dict[str, Any], path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef _plot_learning_curve(run_id: str, history: pd.DataFrame, out_dir: Path) -\u003e Path:\n    plt.figure(figsize=(6, 4))\n    if \"dev_exact_match\" in history.columns:\n        sns.lineplot(data=history, x=\"_step\", y=\"dev_exact_match\")\n        plt.ylabel(\"Dev EM (%)\")\n    else:\n        sns.lineplot(data=history, x=\"_step\", y=\"train_loss\")\n        plt.ylabel(\"Training loss\")\n    plt.title(f\"Learning curve \u2013 {run_id}\")\n    plt.xlabel(\"Optimiser step\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n\ndef _plot_confusion(run_id: str, preds: List[str], answers: List[str], out_dir: Path) -\u003e Path | None:\n    # build confusion matrix on the 20 most frequent answers to avoid huge matrices\n    answer_counts = Counter(answers)\n    top_answers = [a for a, _ in answer_counts.most_common(20)]\n    if len(top_answers) \u003c 2:\n        return None  # nothing meaningful to plot\n\n    filtered_y_true, filtered_y_pred = [], []\n    for gt, pr in zip(answers, preds):\n        if gt in top_answers and pr in top_answers:\n            filtered_y_true.append(gt)\n            filtered_y_pred.append(pr)\n    if len(filtered_y_true) == 0:\n        return None\n\n    labels_sorted = sorted(set(top_answers))\n    cm = confusion_matrix(filtered_y_true, filtered_y_pred, labels=labels_sorted)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\", fmt=\"d\", xticklabels=labels_sorted, yticklabels=labels_sorted)\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.title(f\"Confusion (top-20 answers) \u2013 {run_id}\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_confusion_top20.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n\ndef _plot_comparison_bar(metric_values: Dict[str, float], out_dir: Path) -\u003e Path:\n    df = pd.DataFrame({\"run_id\": list(metric_values.keys()), \"value\": list(metric_values.values())})\n    plt.figure(figsize=(max(6, len(metric_values) * 1.2), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"value\", palette=\"viridis\")\n    for idx, v in enumerate(df.value):\n        plt.text(idx, v + 0.5, f\"{v:.2f}\", ha=\"center\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(PRIMARY_METRIC_NAME)\n    plt.tight_layout()\n    fname = out_dir / \"comparison_primary_metric_bar.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n\ndef _plot_boxplot_dev_em(history_dict: Dict[str, pd.DataFrame], out_dir: Path) -\u003e Path | None:\n    frames = []\n    for rid, hist in history_dict.items():\n        if \"dev_exact_match\" in hist.columns:\n            tmp = hist[[\"_step\", \"dev_exact_match\"]].copy()\n            tmp[\"run_id\"] = rid\n            frames.append(tmp)\n    if not frames:\n        return None\n\n    df = pd.concat(frames)\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(data=df, x=\"run_id\", y=\"dev_exact_match\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Dev EM (%)\")\n    plt.tight_layout()\n    fname = out_dir / \"comparison_dev_em_boxplot.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n###############################################################################\n#                                 MAIN                                        #\n###############################################################################\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory to store evaluation artefacts\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON string list of run_ids to evaluate\")\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).absolute()\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------- load global WandB settings ---------------- #\n    cfg_root = OmegaConf.load(Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\")\n    entity = cfg_root.wandb.entity\n    project = cfg_root.wandb.project\n\n    api = wandb.Api()\n\n    aggregated: Dict[str, Any] = {\"primary_metric\": PRIMARY_METRIC_NAME, \"metrics\": {}}\n    primary_metric_values: Dict[str, float] = {}\n    histories_for_stats: Dict[str, pd.DataFrame] = {}\n\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history()  # pandas DataFrame\n        histories_for_stats[rid] = history.copy()\n        summary = dict(run.summary)\n        config = dict(run.config)\n\n        # ---------------- per-run directory \u0026 artefacts -------------- #\n        out_dir = results_root / rid\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        preds_path = out_dir / \"dev_predictions.json\"\n        preds, answers = [], []\n        if preds_path.is_file():\n            with open(preds_path) as fp:\n                obj = json.load(fp)\n                preds, answers = obj.get(\"pred\", []), obj.get(\"answer\", [])\n\n        # figures\n        lc_path = _plot_learning_curve(rid, history, out_dir)\n        cm_path = _plot_confusion(rid, preds, answers, out_dir) if preds else None\n        print(lc_path)\n        if cm_path:\n            print(cm_path)\n\n        # metrics.json\n        _save_json({\"history\": history.to_dict(orient=\"list\"), \"summary\": summary, \"config\": config}, out_dir / \"metrics.json\")\n\n        # aggregation\n        primary_val = float(summary.get(\"final_dev_em\", summary.get(\"dev_exact_match\", 0.0)))\n        primary_metric_values[rid] = primary_val\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                aggregated.setdefault(\"metrics\", {}).setdefault(k, {})[rid] = float(v)\n\n    # ---------------- select best runs ---------------- #\n    best_prop = max(((r, v) for r, v in primary_metric_values.items() if \"proposed\" in r or \"graff\" in r), key=lambda x: x[1])\n    best_base = max(((r, v) for r, v in primary_metric_values.items() if \"comparative\" in r or \"baseline\" in r or \"reactor\" in r), key=lambda x: x[1])\n    gap_pct = (best_prop[1] - best_base[1]) / (best_base[1] + 1e-9) * 100\n\n    aggregated.update({\n        \"best_proposed\": {\"run_id\": best_prop[0], \"value\": best_prop[1]},\n        \"best_baseline\": {\"run_id\": best_base[0], \"value\": best_base[1]},\n        \"gap\": gap_pct,\n    })\n\n    # ---------------- statistical tests ---------------- #\n    p_values = {}\n    for r1, r2 in combinations(histories_for_stats.keys(), 2):\n        h1 = histories_for_stats[r1].get(\"dev_exact_match\", pd.Series(dtype=float)).dropna()\n        h2 = histories_for_stats[r2].get(\"dev_exact_match\", pd.Series(dtype=float)).dropna()\n        if len(h1) and len(h2):\n            t_stat, p_val = ttest_ind(h1, h2, equal_var=False)\n            p_values[f\"{r1}_vs_{r2}\"] = {\"t\": float(t_stat), \"p\": float(p_val)}\n    if p_values:\n        aggregated[\"significance_tests\"] = p_values\n\n    _save_json(aggregated, comparison_dir / \"aggregated_metrics.json\")\n\n    # ---------------- comparison figures ---------------- #\n    bar_path = _plot_comparison_bar(primary_metric_values, comparison_dir)\n    box_path = _plot_boxplot_dev_em(histories_for_stats, comparison_dir)\n    print(bar_path)\n    if box_path:\n        print(box_path)\n\n\nif __name__ == \"__main__\":\n    main()", "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig\n\n###############################################################################\n#                               ORCHESTRATOR                                  #\n###############################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):  # pragma: no cover \u2013 CLI entry\n    repo_root = Path(get_original_cwd()).absolute()\n    run_yaml = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.is_file():\n        raise FileNotFoundError(f\"Unknown run id \u0027{cfg.run}\u0027. YAML not found: {run_yaml}\")\n\n    # ensure subprocess can import `src` despite Hydra changing cwd\n    env = os.environ.copy()\n    pythonpath = env.get(\"PYTHONPATH\", \"\")\n    env[\"PYTHONPATH\"] = os.pathsep.join(filter(None, [str(repo_root), pythonpath]))\n\n    cmd = [sys.executable, \"-u\", \"-m\", \"src.train\", f\"run={cfg.run}\", f\"mode={cfg.mode}\", f\"results_dir={cfg.results_dir}\"]\n    print(\"[Launcher]\", \" \".join(cmd))\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "from typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n###############################################################################\n#                               MODEL                                         #\n###############################################################################\n\ndef build_model(model_cfg):\n    tokenizer = AutoTokenizer.from_pretrained(model_cfg.name, cache_dir=\".cache/\", trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_cfg.name,\n        torch_dtype=torch.bfloat16 if str(model_cfg.dtype).lower().startswith(\"bf\") else None,\n        device_map=\"auto\",\n        cache_dir=\".cache/\",\n        trust_remote_code=True,\n    )\n\n    l_conf = LoraConfig(\n        r=model_cfg.lora.r,\n        lora_alpha=model_cfg.lora.alpha,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n        lora_dropout=model_cfg.lora.dropout,\n    )\n    model = get_peft_model(model, l_conf)\n    return model, tokenizer\n\n###############################################################################\n#                              HELPERS                                        #\n###############################################################################\n\ndef compute_token_entropy(logits: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"Return scalar mean entropy of token distribution.\"\"\"\n    probs = F.softmax(logits.float(), dim=-1)\n    log_p = torch.log(probs + 1e-9)\n    ent = -(probs * log_p).sum(-1).mean()  # scalar\n    return ent.detach()\n\n\ndef compute_curvature(model) -\u003e torch.Tensor:\n    vals: List[torch.Tensor] = []\n    for n, p in model.named_parameters():\n        if (\"lora_A\" in n or \"lora_B\" in n) and p.grad is not None:\n            vals.append(p.grad.detach().pow(2).mean())\n    if not vals:\n        return torch.zeros(1, device=next(model.parameters()).device)\n    return torch.stack(vals)\n\n###############################################################################\n#                         LR PLANNERS                                         #\n###############################################################################\n\nclass _GNNLayer(nn.Module):\n    def __init__(self, d: int):\n        super().__init__()\n        self.msg = nn.Linear(d * 2, d)\n        self.upd = nn.GRUCell(d, d)\n\n    def forward(self, h):\n        N, D = h.size()\n        h_i = h.unsqueeze(1).expand(N, N, D)\n        h_j = h.unsqueeze(0).expand(N, N, D)\n        m = self.msg(torch.cat([h_i, h_j], dim=-1)).mean(1)\n        h_new = self.upd(m, h)\n        return h_new\n\n\nclass GraffScheduler(nn.Module):\n    \"\"\"Differentiable LR planner introduced in the paper.\"\"\"\n\n    def __init__(self, model, cfg):\n        super().__init__()\n        self.n_nodes = sum(1 for n, _ in model.named_parameters() if (\"lora_A\" in n or \"lora_B\" in n)) or 1\n        self.h = nn.Parameter(torch.zeros(self.n_nodes, 16))\n        self.encoder = nn.Linear(3, 16)\n        self.gnn = _GNNLayer(16)\n        self.decoder = nn.Linear(16, 1)\n        self.stab_guard = cfg.training.scheduler.stability_guard\n\n    def step(self, curvature: torch.Tensor, entropy: torch.Tensor, budget_left: float) -\u003e float:\n        # broadcast features per node\n        if curvature.numel() \u003c self.n_nodes:\n            curvature = curvature.mean().expand(self.n_nodes)\n        if entropy.numel() \u003c self.n_nodes:\n            entropy = entropy.expand(self.n_nodes)\n        budget_vec = curvature.new_full((self.n_nodes,), budget_left)\n\n        features = torch.stack([curvature, entropy, budget_vec], dim=-1)\n        self.h = self.gnn(self.h + self.encoder(features))\n        lr_log = self.decoder(self.h.mean(0)).squeeze()\n        eta = torch.exp(lr_log).clamp(max=10.0)\n\n        # edge-of-stability safeguard\n        c_max = curvature.max().item() if curvature.numel() else 1.0\n        if eta * c_max \u003e self.stab_guard:\n            eta = self.stab_guard / max(c_max, 1e-6)\n        return eta.item()\n\n\nclass ReactorScheduler(nn.Module):\n    \"\"\"REACTOR RL-based LR controller (baseline).\"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.lr_log = nn.Parameter(torch.tensor(0.0))\n        self.baseline = 0.0\n        self.decay = cfg.training.scheduler.baseline_decay\n        self.entropy_bonus = cfg.training.scheduler.entropy_bonus\n\n    def step(self, loss_val: torch.Tensor) -\u003e float:\n        reward = -loss_val.item()\n        advantage = reward - self.baseline\n        self.baseline = self.baseline * self.decay + reward * (1 - self.decay)\n        self.lr_log.grad = torch.tensor(-advantage, device=self.lr_log.device)\n        return torch.exp(self.lr_log).item()", "preprocess_py": "from functools import partial\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\n###############################################################################\n#                              TOKENISATION                                   #\n###############################################################################\n\ndef _format_example(example: Dict, tokenizer: AutoTokenizer, max_tokens: int):\n    \"\"\"Convert a GSM8K row into LM-style training sample.\"\"\"\n\n    question = example[\"question\"].strip()\n    answer = example[\"answer\"].split(\"####\")[-1].strip()\n    prompt = f\"Question: {question}\\nAnswer: \"\n    full_text = f\"{prompt}{answer}\"\n\n    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_tokens)[\"input_ids\"]\n    full_enc = tokenizer(full_text, truncation=True, max_length=max_tokens)\n\n    return {\n        \"input_ids\": full_enc[\"input_ids\"],\n        \"labels\": full_enc[\"input_ids\"],\n        \"attention_mask\": full_enc[\"attention_mask\"],\n        \"prompt_ids\": prompt_ids,\n        \"prompt_mask\": [1] * len(prompt_ids),\n        \"answer_text\": answer,\n    }\n\n###############################################################################\n#                                 COLLATE                                     #\n###############################################################################\n\ndef _collate_fn(batch: List[Dict], pad_token_id: int):\n    keys_to_pad = [\"input_ids\", \"labels\", \"prompt_ids\", \"prompt_mask\"]\n    out: Dict[str, torch.Tensor] = {}\n    for k in keys_to_pad:\n        seqs = [torch.tensor(x[k], dtype=torch.long) for x in batch]\n        pad_val = pad_token_id if k != \"prompt_mask\" else 0\n        out[k] = pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n    out[\"attention_mask\"] = (out[\"input_ids\"] != pad_token_id).long()\n    out[\"answers\"] = [x[\"answer_text\"] for x in batch]\n    return out\n\n###############################################################################\n#                              DATALOADERS                                    #\n###############################################################################\n\ndef build_dataloaders(tokenizer: AutoTokenizer, cfg: DictConfig) -\u003e Tuple[DataLoader, DataLoader]:\n    \"\"\"Return train \u0026 eval PyTorch DataLoaders according to cfg.\"\"\"\n\n    cache_dir = \".cache/\"\n\n    # -------- train dataset -------- #\n    train_ds = load_dataset(\n        cfg.dataset.name,\n        split=cfg.dataset.train_split,\n        streaming=cfg.dataset.streaming,\n        cache_dir=cache_dir,\n    )\n    train_ds = train_ds.map(\n        partial(_format_example, tokenizer=tokenizer, max_tokens=cfg.dataset.max_tokens),\n        remove_columns=None if cfg.dataset.streaming else train_ds.column_names,\n    )\n    if cfg.dataset.streaming and cfg.dataset.shuffle_buffer \u003e 0:\n        train_ds = train_ds.shuffle(cfg.dataset.shuffle_buffer, seed=42)\n\n    # -------- eval dataset --------- #\n    eval_ds = load_dataset(\n        cfg.dataset.name,\n        split=cfg.dataset.eval_split,\n        streaming=False,\n        cache_dir=cache_dir,\n    )\n    eval_ds = eval_ds.map(\n        partial(_format_example, tokenizer=tokenizer, max_tokens=cfg.dataset.max_tokens),\n        remove_columns=eval_ds.column_names,\n    )\n\n    # -------- collate \u0026 loaders ---- #\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    collate = partial(_collate_fn, pad_token_id=tokenizer.pad_token_id)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.micro_batch_size,\n        collate_fn=collate,\n        shuffle=not cfg.dataset.streaming,\n    )\n    eval_loader = DataLoader(\n        eval_ds,\n        batch_size=cfg.training.micro_batch_size,\n        collate_fn=collate,\n        shuffle=False,\n    )\n    return train_loader, eval_loader\n\n###############################################################################\n#                                METRICS                                      #\n###############################################################################\n\nclass EMAccuracy:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n        self.last_value = 0.0\n\n    def update(self, preds: List[str], refs: List[str]):\n        for p, r in zip(preds, refs):\n            self.correct += int(p.strip() == r.strip())\n            self.total += 1\n        if self.total:\n            self.last_value = 100.0 * self.correct / self.total\n\n    def compute(self) -\u003e float:\n        return self.last_value", "pyproject_toml": "[project]\nname = \"graff-experiments\"\nversion = \"0.1.0\"\ndescription = \"GRAFF learning-rate planner experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.2.0\"\ntransformers = \"^4.38.0\"\npeft = \"^0.9.0\"\ndatasets = \"^2.17.0\"\nwandb = \"^0.16.4\"\noptuna = \"^3.5.0\"\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.2\"\npandas = \"^2.2.1\"\nnumpy = \"^1.26.4\"\nscikit-learn = \"^1.4.1\"\nscipy = \"^1.12.0\"", "train_py": "import json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport hydra\nimport optuna\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom .model import (\n    GraffScheduler,\n    ReactorScheduler,\n    build_model,\n    compute_curvature,\n    compute_token_entropy,\n)\nfrom .preprocess import EMAccuracy, build_dataloaders\n\n###############################################################################\n#                                UTILITIES                                    #\n###############################################################################\n\ndef _apply_mode_overrides(cfg: DictConfig) -\u003e None:\n    \"\"\"Dynamically adapt the cfg for `trial` or `full` execution modes.\"\"\"\n\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.total_steps = max(2, cfg.training.gradient_accumulation_steps)\n        cfg.training.budget_minutes = 0.05  # ~3 seconds\n        cfg.evaluation.interval_steps = 1\n        cfg.training.max_epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"cfg.mode must be \u0027trial\u0027 or \u0027full\u0027.\")\n\n\ndef _init_wandb(cfg: DictConfig):\n    \"\"\"Initialise WandB if enabled and return the run handle or None.\"\"\"\n\n    if cfg.wandb.mode == \"disabled\":\n        os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n        return None\n\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run.run_id if \"run\" in cfg and \"run_id\" in cfg.run else cfg.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"[wandb] Run initialised \u2013 {run.get_url()}\")\n    return run\n\n\n###############################################################################\n#                              TRAINING LOOP                                  #\n###############################################################################\n\ndef _single_training_run(cfg: DictConfig) -\u003e float:\n    \"\"\"Execute one end-to-end training job and return final dev EM.\"\"\"\n\n    # ---------- directories ---------- #\n    results_root = Path(cfg.results_dir).absolute()\n    run_dir = results_root / cfg.run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    OmegaConf.save(cfg, str(run_dir / \"resolved_config.yaml\"))\n\n    # ---------- model \u0026 data ---------- #\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model, tokenizer = build_model(cfg.model)\n    model.to(device)\n    model.train()\n\n    train_loader, eval_loader = build_dataloaders(tokenizer, cfg)\n\n    # ---------- optimisers ---------- #\n    base_lr = cfg.training.optimizer.lr_init\n    inner_opt = torch.optim.AdamW(\n        (p for p in model.parameters() if p.requires_grad),\n        lr=base_lr,\n        betas=tuple(cfg.training.optimizer.betas),\n        weight_decay=cfg.training.optimizer.weight_decay,\n    )\n\n    if cfg.training.scheduler.type == \"graff\":\n        lr_planner = GraffScheduler(model, cfg)\n        outer_opt = torch.optim.Adam(lr_planner.parameters(), lr=cfg.training.scheduler.outer_lr)\n    elif cfg.training.scheduler.type == \"reactor\":\n        lr_planner = ReactorScheduler(cfg)\n        outer_opt = torch.optim.Adam(lr_planner.parameters(), lr=cfg.training.scheduler.controller_lr)\n    else:\n        raise ValueError(f\"Unknown scheduler type: {cfg.training.scheduler.type}\")\n\n    scaler = torch.cuda.amp.GradScaler(enabled=\"bf16\" in str(cfg.training.mixed_precision).lower() and device.type == \"cuda\")\n\n    # ---------- trackers ---------- #\n    em_meter = EMAccuracy()\n    start_wall = time.time()\n    wall_budget = cfg.training.budget_minutes * 60 if cfg.training.budget_minutes \u003e 0 else float(\"inf\")\n\n    wandb_run = _init_wandb(cfg)\n\n    step, accum = 0, 0\n    while step \u003c cfg.training.total_steps and (time.time() - start_wall) \u003c wall_budget:\n        for batch in train_loader:\n            if step \u003e= cfg.training.total_steps or (time.time() - start_wall) \u003e= wall_budget:\n                break\n\n            # ---------------- forward \u0026 backward ---------------- #\n            with torch.autocast(device_type=device.type, dtype=torch.bfloat16 if \"bf16\" in str(cfg.training.mixed_precision).lower() else torch.float32):\n                outputs = model(\n                    input_ids=batch[\"input_ids\"].to(device),\n                    attention_mask=batch[\"attention_mask\"].to(device),\n                    labels=batch[\"labels\"].to(device),\n                )\n                loss = outputs.loss / cfg.training.gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum += 1\n\n            if accum \u003c cfg.training.gradient_accumulation_steps:\n                continue  # gradient accumulation\n            accum = 0\n\n            # ---------------- planner update -------------------- #\n            curvature = compute_curvature(model)\n            entropy = compute_token_entropy(outputs.logits.detach())\n            budget_left = max(wall_budget - (time.time() - start_wall), 0.0) / wall_budget\n\n            outer_opt.zero_grad(set_to_none=True)\n            if cfg.training.scheduler.type == \"graff\":\n                lr_factor = lr_planner.step(curvature, entropy, budget_left)\n            else:\n                lr_factor = lr_planner.step(loss.detach())\n            outer_opt.step()\n\n            new_lr = base_lr * lr_factor\n            for g in inner_opt.param_groups:\n                g[\"lr\"] = new_lr\n\n            # ---------------- weight update -------------------- #\n            scaler.unscale_(inner_opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.clip_grad_norm)\n            scaler.step(inner_opt)\n            scaler.update()\n            inner_opt.zero_grad(set_to_none=True)\n\n            # ---------------- logging -------------------------- #\n            if wandb_run is not None:\n                wandb_run.log({\n                    \"train_loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                    \"lr\": new_lr,\n                }, step=step)\n\n            step += 1\n\n            # ---------------- periodic eval -------------------- #\n            if step % cfg.evaluation.interval_steps == 0 or step == cfg.training.total_steps:\n                model.eval()\n                em_meter.reset()\n                all_preds, all_ans = [], []\n                with torch.no_grad():\n                    for e_idx, e_batch in enumerate(eval_loader):\n                        gen_ids = model.generate(\n                            input_ids=e_batch[\"prompt_ids\"].to(device),\n                            attention_mask=e_batch[\"prompt_mask\"].to(device),\n                            max_new_tokens=128,\n                            do_sample=False,\n                        )\n                        p_len = e_batch[\"prompt_ids\"].shape[1]\n                        preds = tokenizer.batch_decode(gen_ids[:, p_len:], skip_special_tokens=True)\n                        em_meter.update(preds, e_batch[\"answers\"])\n                        all_preds.extend(preds)\n                        all_ans.extend(e_batch[\"answers\"])\n                        if cfg.mode == \"trial\" and e_idx \u003e= 1:\n                            break\n\n                dev_em = em_meter.compute()\n                json.dump({\"pred\": all_preds, \"answer\": all_ans}, open(run_dir / \"dev_predictions.json\", \"w\"))\n\n                if wandb_run is not None:\n                    wandb_run.log({\"dev_exact_match\": dev_em}, step=step)\n\n                model.train()\n\n    final_em = em_meter.last_value\n\n    json.dump({\"final_dev_em\": final_em}, open(run_dir / \"final_metrics.json\", \"w\"), indent=2)\n    if wandb_run is not None:\n        wandb_run.summary[\"final_dev_em\"] = final_em\n        wandb_run.finish()\n\n    return final_em\n\n\n###############################################################################\n#                         OPTUNA SEARCH + FINAL RUN                           #\n###############################################################################\n\ndef _run_optuna(cfg: DictConfig):\n    \"\"\"Perform Optuna search and then run best configuration once with WandB.\"\"\"\n\n    search_space: Dict[str, Any] = cfg.optuna.search_space  # type: ignore[arg-type]\n\n    def _suggest(trial: optuna.Trial, path: str, definition: Dict[str, Any]):\n        kind = definition[\"type\"]\n        if kind == \"loguniform\":\n            return trial.suggest_float(path, definition[\"low\"], definition[\"high\"], log=True)\n        if kind == \"uniform\":\n            return trial.suggest_float(path, definition[\"low\"], definition[\"high\"], log=False)\n        if kind == \"categorical\":\n            return trial.suggest_categorical(path, definition[\"choices\"])\n        raise ValueError(f\"Unsupported Optuna space type: {kind}\")\n\n    def objective(trial: optuna.Trial):\n        trial_cfg = OmegaConf.deepcopy(cfg)\n        trial_cfg.wandb.mode = \"disabled\"  # silent trials\n        for hp_path, hp_def in search_space.items():\n            sampled = _suggest(trial, hp_path, hp_def)\n            OmegaConf.update(trial_cfg, hp_path, sampled, merge=True)\n        return _single_training_run(trial_cfg)\n\n    study = optuna.create_study(direction=cfg.optuna.direction)\n    study.optimize(objective, n_trials=cfg.optuna.n_trials)\n\n    # merge best params back \u0026 launch authenticated run\n    for k, v in study.best_params.items():\n        OmegaConf.update(cfg, k, v, merge=True)\n\n    _single_training_run(cfg)\n\n\n###############################################################################\n#                               HYDRA ENTRY                                   #\n###############################################################################\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef train_app(cfg: DictConfig):  # pragma: no cover \u2013 entry-point\n    repo_root = Path(get_original_cwd())\n    run_yaml = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.is_file():\n        raise FileNotFoundError(f\"Run-specific YAML not found: {run_yaml}\")\n\n    run_cfg = OmegaConf.load(run_yaml)\n    cfg = OmegaConf.merge(cfg, run_cfg)\n\n    # absolute results dir + run id\n    cfg.results_dir = str(Path(cfg.results_dir).absolute())\n    cfg.run_id = cfg.get(\"run_id\", cfg.run_id if \"run_id\" in cfg else cfg.run)\n\n    _apply_mode_overrides(cfg)\n\n    if cfg.optuna.n_trials \u003e 0:\n        _run_optuna(cfg)\n    else:\n        _single_training_run(cfg)\n\n\nif __name__ == \"__main__\":\n    train_app()"}



Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "The repository does not satisfy the “production-ready / publication-worthy” bar required by the validation checklist.\n\nKey blocking issues\n1.  Incomplete / incorrect implementation of the proposed GRAFF algorithm\n    • GraffScheduler.step only returns a scalar LR; it never computes a differentiable objective nor back-propagates any loss through the planner parameters.  outer_opt.zero_grad() is called before GraffScheduler.step, and no gradients are created afterwards, so outer_opt.step() performs a no-op – W is never updated.  The closed-form differentiable training claimed in the paper is therefore missing.\n    • self.h is registered as nn.Parameter but is overwritten with a plain Tensor, so it is removed from the module’s parameter list after the very first call.  This breaks any future optimisation and violates PyTorch’s parameter semantics.\n\n2.  WandB logging is insufficient\n    • Only \"train_loss\", \"lr\" and occasional \"dev_exact_match\" are logged.  The spec requires *all* relevant time-series metrics to be logged at the highest sensible frequency.\n\n3.  File-level side outputs created by train.py\n    • train.py writes “final_metrics.json” and “dev_predictions.json”.  The requirements explicitly forbid any results.json-style dumps from train.py.\n\n4.  GRAFF / REACTOR schedulers are not fully faithful to the described baselines (e.g. REACTOR does not implement policy-gradient updates, entropy bonus, or baseline subtraction correctly).\n\n5.  Minor but critical robustness gaps\n    • id passed to wandb.init uses `cfg.run.run_id`, but `cfg.run` is a string, so the attribute access would raise AttributeError if the first branch of the conditional is taken.\n    • Data-loader for streaming datasets is used with torch.utils.data.DataLoader without `batch_size=None`, which will crash for an IterableDataset.\n\nBecause of these blocking problems the code is **not** ready for immediate execution in research experiments."
}
