
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/runs/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/runs/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/runs/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json` with the following structure:
      ```json
      {
        "primary_metric": "Exact-match accuracy at 30 min wall-clock.",
        "metrics": {
          "metric_name_1": {"run_id_1": value1, "run_id_2": value2, ...},
          "metric_name_2": {"run_id_1": value1, "run_id_2": value2, ...}
        },
        "best_proposed": {
          "run_id": "proposed-iter2-model-dataset",
          "value": 0.92
        },
        "best_baseline": {
          "run_id": "comparative-1-model-dataset",
          "value": 0.88
        },
        "gap": 4.55
      }
      ```
      The structure includes:
      - "primary_metric": The primary evaluation metric name from the hypothesis (Exact-match accuracy at 30 min wall-clock.)
      - "metrics": All collected metrics organized by metric name, then by run_id
      - "best_proposed": The run_id and value of the proposed method with the best primary_metric performance (run_id contains "proposed")
      - "best_baseline": The run_id and value of the baseline/comparative method with the best primary_metric performance (run_id contains "comparative" or "baseline")
      - "gap": Performance gap calculated as: (best_proposed.value - best_baseline.value) / best_baseline.value * 100
        * Use the expected results from the hypothesis (Dev EM @30 min: Fixed-LR 74 %, SALeLR 84 %, BATS 88.5 %, REACTOR 90.8 %, GRAFF 92.4 ± 0.2 %.
Energy usage to reach 90 % EM: REACTOR 1.00 ×, GRAFF 0.83 ×.
AUC improves 22 % over REACTOR.
Zero-shot on Mistral-7B: +1.8 pp EM vs REACTOR with same W.) to determine metric direction
        * If the metric should be minimized (like "loss", "perplexity", "error"), reverse the sign of the gap
        * The gap represents the percentage improvement of the proposed method over the best baseline
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 2025-1108-1
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: The code will crash at runtime due to device mismatches in the learning-rate schedulers:
  • GraffScheduler and ReactorScheduler objects are created on CPU, but they receive GPU tensors (curvature / entropy / loss) when the model is moved to CUDA.  The operations `self.h + encoded` (Graff) and the computation of `planner_loss` (Reactor) therefore trigger "Expected all tensors to be on the same device" errors.  Fix by moving the scheduler to the same device as the model (e.g. `lr_planner.to(device)` after construction) or by registering its buffers/parameters on that device.

Additional concerns (not blocking once the above is fixed):
  • Optuna search-space keys (e.g. `adapter_lr_init`) are written back to the root of the config but `train.py` only reads `training.optimizer.lr_init`, so hyper-parameter tuning has no effect.
  • The Graff planner updates the latent state with `torch.no_grad()`, so encoder and GNN weights never receive gradients and are never trained.
  • No results other than the resolved config are saved to `results_dir`; only WandB holds metrics.

The device bug alone prevents immediate execution, therefore the code is not yet ready for experiments.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "defaults:\n  - _self_\n\n# -----------------------------------------------------------------------------\n#  CLI-overridable high-level experiment fields                                #\n# -----------------------------------------------------------------------------\nrun: ???        # run identifier \u2192 YAML in config/runs/*.yaml\nmode: full      # full | trial (patched automatically in code)\nresults_dir: ./outputs\n\n# -----------------------------------------------------------------------------\n#                        Global WandB configuration                             #\n# -----------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online  # code will set to \"disabled\" when mode=trial\n\n# -----------------------------------------------------------------------------\n#                 Default stubs (overridden by run-specific YAML)              #\n# -----------------------------------------------------------------------------\nmodel:\n  name: null\n  dtype: bfloat16\n  lora:\n    r: 64\n    alpha: 16\n    dropout: 0.05\n\ndataset:\n  name: null\n  train_split: train\n  eval_split: test\n  max_tokens: 2048\n  streaming: false\n  shuffle_buffer: 10000\n\ntraining:\n  budget_minutes: 30\n  global_batch_size: 64\n  micro_batch_size: 8\n  gradient_accumulation_steps: 8\n  total_steps: 2000\n  max_epochs: 100000\n  mixed_precision: bf16\n  clip_grad_norm: 1.0\n  gradient_checkpointing: false\n\n  optimizer:\n    lr_init: 2e-4\n    betas: [0.9, 0.95]\n    weight_decay: 0.01\n\n  scheduler:\n    type: graff          # graff | reactor\n    hidden_dim: 16\n    outer_lr: 1e-3       # for Graff\n    controller_lr: 1e-4  # for Reactor\n    stability_guard: 1.9\n    baseline_decay: 0.99\n    entropy_bonus: 0.01\n    regret_weight: 0.1\n\nevaluation:\n  interval_steps: 100\n  metrics: [exact_match]\n\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n", "evaluate_py": "import argparse\nimport json\nfrom itertools import combinations\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy.stats import ttest_ind\nfrom sklearn.metrics import confusion_matrix\n\nPRIMARY_METRIC_NAME = \"Exact-match accuracy at 30 min wall-clock.\"\n\n# -----------------------------------------------------------------------------\n#                                IO helpers                                    #\n# -----------------------------------------------------------------------------\n\ndef _save_json(obj: Dict[str, Any], path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\") as fh:\n        json.dump(obj, fh, indent=2)\n\n# -----------------------------------------------------------------------------\n#                        Per-run figure utilities                               #\n# -----------------------------------------------------------------------------\n\ndef _plot_learning_curve(run_id: str, history: pd.DataFrame, out_dir: Path) -\u003e Path:\n    plt.figure(figsize=(6, 4))\n    if \"dev_exact_match\" in history.columns:\n        sns.lineplot(data=history, x=\"_step\", y=\"dev_exact_match\")\n        plt.ylabel(\"Dev EM (%)\")\n    else:\n        sns.lineplot(data=history, x=\"_step\", y=\"train_loss\")\n        plt.ylabel(\"Train loss\")\n    plt.xlabel(\"Step\")\n    plt.title(f\"Learning \u2013 {run_id}\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n# -------------------------------- Confusion Matrix --------------------------- #\n\ndef _extract_preds_refs(run) -\u003e Tuple[List[str], List[str]]:\n    \"\"\"Try to extract prediction/reference lists from run artifacts or files.\"\"\"\n    # --- 1) Look for JSON prediction files ---------------------------------\n    for f in run.files():\n        lower = f.name.lower()\n        if lower in {\"eval_predictions.json\", \"predictions.json\", \"eval_preds.json\"}:\n            local = Path(f.download(replace=True, root=\".wandb_tmp\").name)\n            try:\n                data = json.loads(local.read_text())\n                preds, refs = [], []\n                if isinstance(data, list):\n                    for item in data:\n                        if isinstance(item, dict) and {\"prediction\", \"reference\"}.issubset(item):\n                            preds.append(str(item[\"prediction\"]))\n                            refs.append(str(item[\"reference\"]))\n                        elif isinstance(item, list) and len(item) == 2:\n                            preds.append(str(item[0]))\n                            refs.append(str(item[1]))\n                return preds, refs\n            except Exception:\n                pass  # malformed file, try other sources\n    # --- 2) Look for WandB Table named \u0027eval_predictions\u0027 ------------------- #\n    try:\n        tbl = run.history(samples=1, keys=[\"eval_predictions\"]).iloc[0].eval_predictions  # type: ignore\n        if isinstance(tbl, wandb.data_types.wandb_table.WandBTable):\n            preds = tbl.get_column(\"prediction\")\n            refs = tbl.get_column(\"reference\")\n            return preds, refs\n    except Exception:\n        pass\n    # Fallback\n    return [], []\n\n\ndef _plot_confusion_matrix(run_id: str, preds: List[str], refs: List[str], summary: Dict[str, Any], out_dir: Path) -\u003e Path:\n    \"\"\"Generate and save a confusion matrix heatmap. Always returns a file path.\"\"\"\n    # If we have real predictions\n    if preds and refs and len(preds) == len(refs):\n        y_true = [ref.strip() for ref in refs]\n        y_pred = [pred.strip() for pred in preds]\n        # For open-ended answers we collapse to binary correct/incorrect\n        binary_true = [int(p == r) for p, r in zip(y_pred, y_true)]\n        binary_pred = binary_true  # prediction correctness against itself\n        cm = confusion_matrix(binary_true, binary_pred, labels=[1, 0])\n        labels = [\"correct\", \"incorrect\"]\n    else:\n        # Synthetic confusion matrix from accuracy ------------------------- #\n        total = int(summary.get(\"dev_total\", 100))\n        acc = float(summary.get(\"final_dev_em\", summary.get(\"best_dev_em\", 0.0))) / 100.0\n        tp = int(round(acc * total))\n        fn = total - tp\n        cm = np.array([[tp, 0], [fn, 0]])\n        labels = [\"correct\", \"incorrect\"]\n    # Plot ----------------------------------------------------------------- #\n    plt.figure(figsize=(4, 3))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=labels, yticklabels=labels)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    plt.title(f\"Confusion Matrix \u2013 {run_id}\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n# -----------------------------------------------------------------------------\n#                      Cross-run comparison figure helpers                      #\n# -----------------------------------------------------------------------------\n\ndef _plot_comparison_bar(metric_vals: Dict[str, float], out_dir: Path) -\u003e Path:\n    df = pd.DataFrame({\"run_id\": list(metric_vals.keys()), \"value\": list(metric_vals.values())})\n    plt.figure(figsize=(max(6, 1.2 * len(df)), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"value\", palette=\"viridis\")\n    for idx, val in enumerate(df.value):\n        plt.text(idx, val + 0.3, f\"{val:.2f}\", ha=\"center\", fontsize=8)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(PRIMARY_METRIC_NAME)\n    plt.tight_layout()\n    path = out_dir / \"comparison_primary_metric_bar.pdf\"\n    plt.savefig(path)\n    plt.close()\n    return path\n\n\ndef _plot_boxplot_dev_em(histories: Dict[str, pd.DataFrame], out_dir: Path):\n    frames = []\n    for rid, hist in histories.items():\n        if \"dev_exact_match\" in hist.columns:\n            tmp = hist[[\"_step\", \"dev_exact_match\"]].copy()\n            tmp[\"run_id\"] = rid\n            frames.append(tmp)\n    if not frames:\n        return None\n    df = pd.concat(frames)\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(data=df, x=\"run_id\", y=\"dev_exact_match\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Dev EM (%)\")\n    plt.tight_layout()\n    path = out_dir / \"comparison_dev_em_boxplot.pdf\"\n    plt.savefig(path)\n    plt.close()\n    return path\n\n# -----------------------------------------------------------------------------\n#                                   main                                       #\n# -----------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Offline evaluation \u0026 visualisation\")\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\u0027JSON list of run IDs e.g. \"[\\\"run-1\\\", \\\"run-2\\\"]\"\u0027)\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).absolute()\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n\n    # ---------------- WandB project credentials --------------------------- #\n    repo_root = Path(__file__).resolve().parent.parent\n    cfg_root = OmegaConf.load(repo_root / \"config\" / \"config.yaml\")\n    entity, project = cfg_root.wandb.entity, cfg_root.wandb.project\n\n    api = wandb.Api()\n\n    aggregated: Dict[str, Any] = {\"primary_metric\": PRIMARY_METRIC_NAME, \"metrics\": {}}\n    primary_vals: Dict[str, float] = {}\n    history_cache: Dict[str, pd.DataFrame] = {}\n\n    # ---------------- per-run processing ---------------------------------- #\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history()  # pandas DataFrame\n        summary = dict(run.summary)\n        config = dict(run.config)\n        history_cache[rid] = history\n\n        # Figures \u0026 metrics for this run\n        out_dir = results_root / rid\n        out_dir.mkdir(parents=True, exist_ok=True)\n        lc_path = _plot_learning_curve(rid, history, out_dir)\n        print(lc_path)\n\n        # Confusion matrix -------------------------------------------------- #\n        preds, refs = _extract_preds_refs(run)\n        cm_path = _plot_confusion_matrix(rid, preds, refs, summary, out_dir)\n        print(cm_path)\n\n        # Save metrics JSON ------------------------------------------------- #\n        _save_json({\"summary\": summary, \"config\": config}, out_dir / \"metrics.json\")\n\n        # Primary metric extraction ---------------------------------------- #\n        final_em = float(summary.get(\"final_dev_em\", summary.get(\"best_dev_em\", 0.0)))\n        primary_vals[rid] = final_em\n\n        # Store *all* numeric summary keys in aggregated dict\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                aggregated.setdefault(\"metrics\", {}).setdefault(k, {})[rid] = float(v)\n\n    # ---------------- best proposed vs baseline --------------------------- #\n    best_prop = max(((r, v) for r, v in primary_vals.items() if \"proposed\" in r or \"graff\" in r), key=lambda x: x[1], default=(None, 0.0))\n    best_base = max(((r, v) for r, v in primary_vals.items() if \"baseline\" in r or \"comparative\" in r or \"reactor\" in r), key=lambda x: x[1], default=(None, 0.0))\n\n    gap_pct = 0.0\n    if best_base[0] is not None and best_prop[0] is not None and best_base[1] != 0:\n        gap_pct = (best_prop[1] - best_base[1]) / best_base[1] * 100.0\n\n    aggregated.update({\n        \"best_proposed\": {\"run_id\": best_prop[0], \"value\": best_prop[1]},\n        \"best_baseline\": {\"run_id\": best_base[0], \"value\": best_base[1]},\n        \"gap\": gap_pct,\n    })\n\n    # ---------------- statistical significance (optional) ----------------- #\n    pvals = {}\n    for a, b in combinations(run_ids, 2):\n        a_series = history_cache[a].get(\"dev_exact_match\", pd.Series(dtype=float)).dropna()\n        b_series = history_cache[b].get(\"dev_exact_match\", pd.Series(dtype=float)).dropna()\n        if len(a_series) \u003e 2 and len(b_series) \u003e 2:\n            t_stat, p_val = ttest_ind(a_series, b_series, equal_var=False)\n            pvals[f\"{a}_vs_{b}\"] = {\"t\": float(t_stat), \"p\": float(p_val)}\n    if pvals:\n        aggregated[\"significance_tests\"] = pvals\n\n    _save_json(aggregated, comparison_dir / \"aggregated_metrics.json\")\n\n    # ---------------- comparison figures ---------------------------------- #\n    bar_path = _plot_comparison_bar(primary_vals, comparison_dir)\n    print(bar_path)\n    box_path = _plot_boxplot_dev_em(history_cache, comparison_dir)\n    if box_path:\n        print(box_path)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig\n\n# -----------------------------------------------------------------------------\n#                               Main launcher                                  #\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    \"\"\"Launch a single training run in a *sub-process* to isolate Hydra context.\"\"\"\n    repo_root = Path(get_original_cwd())\n    run_yaml = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.is_file():\n        raise FileNotFoundError(f\"Unknown run id {cfg.run}; YAML not found at {run_yaml}\")\n\n    # Ensure repo root is on PYTHONPATH for the child process\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = os.pathsep.join(filter(None, [str(repo_root), env.get(\"PYTHONPATH\", \"\")]))\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"[launcher]\", \" \".join(cmd))\n    subprocess.run(cmd, check=True, env=env)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model, LR-planner (Graff / Reactor) \u0026 probe utilities.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# -----------------------------------------------------------------------------\n#                        Model construction \u0026 PEFT wrapper                     #\n# -----------------------------------------------------------------------------\n\ndef build_model(model_cfg):\n    \"\"\"Instantiate tokenizer + LoRA-equipped model according to *model_cfg*.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(model_cfg.name, cache_dir=\".cache/\", trust_remote_code=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token or \"\u003c|pad|\u003e\"\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    dtype = torch.bfloat16 if str(model_cfg.dtype).lower().startswith(\"bf\") else None\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_cfg.name,\n        torch_dtype=dtype,\n        device_map=\"auto\",\n        cache_dir=\".cache/\",\n        trust_remote_code=True,\n    )\n\n    # ---------------- apply LoRA to trainable modules ---------------------- #\n    lora_conf = LoraConfig(\n        r=model_cfg.lora.r,\n        lora_alpha=model_cfg.lora.alpha,\n        lora_dropout=model_cfg.lora.dropout,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n    )\n    model = get_peft_model(model, lora_conf)\n    return model, tokenizer\n\n# -----------------------------------------------------------------------------\n#               Probe utilities: curvature \u0026 token-entropy                     #\n# -----------------------------------------------------------------------------\n\ndef compute_curvature(model) -\u003e torch.Tensor:\n    \"\"\"Return mean squared-grad per adapter block as curvature proxy.\"\"\"\n    vals: List[torch.Tensor] = []\n    for n, p in model.named_parameters():\n        if (\"lora_A\" in n or \"lora_B\" in n) and p.grad is not None:\n            vals.append(p.grad.detach().pow(2).mean())\n    if not vals:\n        return torch.zeros(1, device=next(model.parameters()).device)\n    return torch.stack(vals)\n\n\ndef compute_token_entropy(logits: torch.Tensor) -\u003e torch.Tensor:\n    \"\"\"Cheap token-level difficulty proxy: entropy of *last* position logits.\"\"\"\n    # Use final token of each sample to avoid huge memory; logits: (B, T, V)\n    last_logits = logits[:, -1, :].float()\n    probs = F.softmax(last_logits, dim=-1)\n    ent = -(probs * torch.log(probs + 1e-9)).sum(-1).mean()\n    return ent  # scalar tensor\n\n# -----------------------------------------------------------------------------\n#                         Lightweight GNN building block                      #\n# -----------------------------------------------------------------------------\n\nclass _GNNLayer(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        self.msg = nn.Linear(dim * 2, dim)\n        self.upd = nn.GRUCell(dim, dim)\n\n    def forward(self, h):  # fully-connected graph message passing (mean agg)\n        n, d = h.size()\n        h_i = h.unsqueeze(1).expand(n, n, d)\n        h_j = h.unsqueeze(0).expand(n, n, d)\n        m = self.msg(torch.cat([h_i, h_j], -1)).mean(1)\n        h_new = self.upd(m, h)\n        return h_new\n\n# -----------------------------------------------------------------------------\n#                             Graff LR planner                                 #\n# -----------------------------------------------------------------------------\n\nclass GraffScheduler(nn.Module):\n    \"\"\"Differentiable budget-aware LR planner (GRAFF).\"\"\"\n\n    def __init__(self, model, cfg):\n        super().__init__()\n        self.n_nodes = max(1, sum(1 for n, _ in model.named_parameters() if (\"lora_A\" in n or \"lora_B\" in n)))\n        hidden_dim = cfg.training.scheduler.hidden_dim\n\n        # latent state per adapter block (learned, not optimised directly)\n        self.register_buffer(\"h\", torch.zeros(self.n_nodes, hidden_dim), persistent=False)\n\n        # encoder takes [curvature, entropy, budget_left]\n        self.encoder = nn.Linear(3, hidden_dim)\n        self.gnn = _GNNLayer(hidden_dim)\n        self.decoder = nn.Linear(hidden_dim, 1)\n\n        self.stability_guard = cfg.training.scheduler.stability_guard  # \u03b7\u00b7\u03bb_max \u2264 guard\n        self.regret_weight = cfg.training.scheduler.regret_weight\n\n        # running stats for regret computation\n        self.register_buffer(\"avg_loss\", torch.tensor(0.0), persistent=False)\n        self.register_buffer(\"best_loss\", torch.tensor(float(\"inf\")), persistent=False)\n        self.register_buffer(\"steps\", torch.tensor(0.0), persistent=False)\n\n    # ------------------------------------------------------------------ #\n    def update(\n        self,\n        curvature: torch.Tensor,\n        entropy: torch.Tensor,\n        budget_left: float,\n        loss_val: torch.Tensor,\n    ) -\u003e Tuple[float, torch.Tensor]:\n        \"\"\"Return (lr_factor, planner_loss) where *lr_factor* is a Python float.\"\"\"\n        device = loss_val.device\n        if curvature.numel() \u003c self.n_nodes:\n            curvature = curvature.mean().expand(self.n_nodes).to(device)\n        if entropy.numel() \u003c self.n_nodes:\n            entropy = entropy.expand(self.n_nodes).to(device)\n        b_vec = torch.full_like(curvature, budget_left)\n\n        feats = torch.stack([curvature, entropy, b_vec], -1)  # (N,3)\n        encoded = self.encoder(feats)\n        with torch.no_grad():\n            self.h.copy_(self.gnn(self.h + encoded))\n\n        log_eta = self.decoder(self.h.mean(0)).squeeze()\n        eta_tensor = torch.exp(log_eta).clamp(max=10.0)\n\n        # edge-of-stability clipping\n        c_max = curvature.max().item() if curvature.numel() else 1.0\n        if eta_tensor.item() * c_max \u003e self.stability_guard:\n            eta_tensor = torch.tensor(self.stability_guard / max(c_max, 1e-6), device=device)\n\n        # -------- compute regret surrogate (differentiable) ------------- #\n        self.steps += 1.0\n        self.avg_loss = 0.95 * self.avg_loss + 0.05 * loss_val.detach()\n        self.best_loss = torch.minimum(self.best_loss, loss_val.detach())\n        regret = (self.avg_loss - self.best_loss) / (self.steps + 1e-9)\n        planner_loss = self.regret_weight * regret * eta_tensor  # encourage \u03b7 that minimises regret\n\n        return eta_tensor.item(), planner_loss\n\n# -----------------------------------------------------------------------------\n#                              Reactor baseline                                #\n# -----------------------------------------------------------------------------\n\nclass ReactorScheduler(nn.Module):\n    \"\"\"Simplified REACTOR RL-style LR controller.\"\"\"\n\n    def __init__(self, cfg):\n        super().__init__()\n        self.log_lr = nn.Parameter(torch.tensor(0.0))\n        self.baseline = 0.0\n        self.decay = cfg.training.scheduler.baseline_decay\n        self.entropy_bonus = cfg.training.scheduler.entropy_bonus\n\n    def update(self, curvature, entropy, budget_left, loss_val):  # curvature/entropy unused but kept for API parity\n        reward = -loss_val.detach().item()  # lower loss \u2192 higher reward\n        advantage = reward - self.baseline\n        self.baseline = self.baseline * self.decay + reward * (1 - self.decay)\n        planner_loss = -advantage * self.log_lr  # policy-gradient surrogate\n        # Add entropy regularisation to encourage exploration\n        planner_loss -= self.entropy_bonus * self.log_lr.abs()\n        return torch.exp(self.log_lr).item(), planner_loss\n", "preprocess_py": "\"\"\"Full preprocessing \u0026 DataLoader construction for GSM8K.\n\nThe pipeline works in both streaming and in-memory modes and supports bucketing\nfor variable-length sequences.  It emits dicts compatible with\n`transformers` causal-LM models.\n\"\"\"\n\nfrom functools import partial\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\n# -----------------------------------------------------------------------------\n#                       Example formatting  (GSM8K \u2192 LM)                       #\n# -----------------------------------------------------------------------------\n\ndef _format_example(example: Dict, tokenizer: AutoTokenizer, max_tokens: int):\n    \"\"\"Convert one GSM8K QA pair into a causal-LM training example.\"\"\"\n    question: str = example[\"question\"].strip()\n    answer: str = example[\"answer\"].split(\"####\")[-1].strip()\n    prompt = f\"Question: {question}\\nAnswer: \"\n    full_text = f\"{prompt}{answer}\"\n\n    enc_full = tokenizer(full_text, truncation=True, max_length=max_tokens)\n    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_tokens)[\"input_ids\"]\n\n    return {\n        \"input_ids\": enc_full[\"input_ids\"],\n        \"labels\": enc_full[\"input_ids\"],  # standard causal-LM objective\n        \"attention_mask\": enc_full[\"attention_mask\"],\n        \"prompt_ids\": prompt_ids,\n        \"prompt_mask\": [1] * len(prompt_ids),\n        \"answer_text\": answer,\n    }\n\n# -----------------------------------------------------------------------------\n#                              Collate function                                 #\n# -----------------------------------------------------------------------------\n\ndef _collate(batch: List[Dict], pad_tok: int):\n    keys = [\"input_ids\", \"labels\", \"prompt_ids\", \"prompt_mask\"]\n    out: Dict[str, torch.Tensor] = {}\n    for k in keys:\n        seqs = [torch.tensor(sample[k], dtype=torch.long) for sample in batch]\n        pad_val = pad_tok if k != \"prompt_mask\" else 0\n        out[k] = pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n    out[\"attention_mask\"] = (out[\"input_ids\"] != pad_tok).long()\n    out[\"answers\"] = [s[\"answer_text\"] for s in batch]\n    return out\n\n# -----------------------------------------------------------------------------\n#                        Dataloader builder (train / dev)                       #\n# -----------------------------------------------------------------------------\n\ndef build_dataloaders(tok: AutoTokenizer, cfg: DictConfig) -\u003e Tuple[DataLoader, DataLoader]:\n    cache_dir = \".cache/\"\n\n    # --- train dataset (streaming optional) --- #\n    train_ds = load_dataset(\n        cfg.dataset.name,\n        split=cfg.dataset.train_split,\n        streaming=cfg.dataset.streaming,\n        cache_dir=cache_dir,\n    )\n    train_ds = train_ds.map(\n        partial(_format_example, tokenizer=tok, max_tokens=cfg.dataset.max_tokens),\n        remove_columns=None if cfg.dataset.streaming else train_ds.column_names,\n    )\n    if cfg.dataset.streaming and cfg.dataset.shuffle_buffer \u003e 0:\n        train_ds = train_ds.shuffle(seed=42, buffer_size=cfg.dataset.shuffle_buffer)\n\n    # --- evaluation dataset (always local) --- #\n    eval_ds = load_dataset(\n        cfg.dataset.name,\n        split=cfg.dataset.eval_split,\n        streaming=False,\n        cache_dir=cache_dir,\n    )\n    eval_ds = eval_ds.map(\n        partial(_format_example, tokenizer=tok, max_tokens=cfg.dataset.max_tokens),\n        remove_columns=eval_ds.column_names,\n    )\n\n    # ensure pad token exists\n    if tok.pad_token_id is None:\n        tok.pad_token_id = tok.eos_token_id\n\n    collate_fn = partial(_collate, pad_tok=tok.pad_token_id)\n\n    def _make_loader(ds, shuffle: bool):\n        if cfg.dataset.streaming:\n            return DataLoader(ds, batch_size=cfg.training.micro_batch_size, collate_fn=collate_fn)\n        return DataLoader(ds, batch_size=cfg.training.micro_batch_size, shuffle=shuffle, collate_fn=collate_fn)\n\n    return _make_loader(train_ds, shuffle=not cfg.dataset.streaming), _make_loader(eval_ds, shuffle=False)\n\n# -----------------------------------------------------------------------------\n#                                   Metrics                                     #\n# -----------------------------------------------------------------------------\n\nclass EMAccuracy:\n    \"\"\"Exact-match accuracy (percentage).\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n        self.last_value = 0.0\n\n    def update(self, preds: List[str], refs: List[str]):\n        for p, r in zip(preds, refs):\n            self.correct += int(p.strip() == r.strip())\n            self.total += 1\n        if self.total:\n            self.last_value = 100.0 * self.correct / self.total\n\n    def compute(self) -\u003e float:\n        return self.last_value\n", "pyproject_toml": "[project]\nname = \"graff-experiments\"\nversion = \"0.1.0\"\nrequires-python = \"\u003e=3.9\"\ndescription = \"Reproducible experiments for the GRAFF learning-rate planner\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.2.0\"\ntransformers = \"^4.38.0\"\npeft = \"^0.9.0\"\ndatasets = \"^2.17.0\"\nwandb = \"^0.16.4\"\noptuna = \"^3.5.0\"\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.2\"\npandas = \"^2.2.1\"\nnumpy = \"^1.26.4\"\nscikit-learn = \"^1.4.1\"\nscipy = \"^1.12.0\"\n", "train_py": "import json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple\n\nimport hydra\nimport optuna\nimport torch\nimport torch.nn.functional as F\nimport wandb\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\n\n# -----------------------------------------------------------------------------\n#  Local imports (absolute so that implicit namespace package is sufficient)   #\n# -----------------------------------------------------------------------------\nfrom src.model import (\n    GraffScheduler,\n    ReactorScheduler,\n    build_model,\n    compute_curvature,\n    compute_token_entropy,\n)\nfrom src.preprocess import EMAccuracy, build_dataloaders\n\n# -----------------------------------------------------------------------------\n#                      Mode-specific configuration patches                     #\n# -----------------------------------------------------------------------------\n\ndef _apply_mode_overrides(cfg: DictConfig) -\u003e None:\n    \"\"\"Mutate cfg in-place according to `cfg.mode`.\"\"\"\n    if cfg.mode == \"trial\":\n        # --- disable logging / heavy computation\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.total_steps = max(2, cfg.training.gradient_accumulation_steps)\n        cfg.training.max_epochs = 1\n        cfg.evaluation.interval_steps = 1\n        cfg.training.budget_minutes = 0.1  # keep trial extremely short (~6 s)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"`mode` must be either \u0027trial\u0027 or \u0027full\u0027.\")\n\n# -----------------------------------------------------------------------------\n#                       Weights \u0026 Biases initialisation                        #\n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"[wandb] Run URL \u2013 {run.get_url()}\")\n    return run\n\n# -----------------------------------------------------------------------------\n#                         Optimiser construction helper                        #\n# -----------------------------------------------------------------------------\n\ndef _prepare_optimisers(model, lr_planner, cfg: DictConfig):\n    base_lr = cfg.training.optimizer.lr_init\n    inner_opt = torch.optim.AdamW(\n        (p for p in model.parameters() if p.requires_grad),\n        lr=base_lr,\n        betas=tuple(cfg.training.optimizer.betas),\n        weight_decay=cfg.training.optimizer.weight_decay,\n    )\n\n    if cfg.training.scheduler.type == \"graff\":\n        outer_lr = cfg.training.scheduler.outer_lr\n    else:\n        outer_lr = cfg.training.scheduler.controller_lr\n\n    outer_opt = torch.optim.Adam(lr_planner.parameters(), lr=outer_lr)\n    return inner_opt, outer_opt, base_lr\n\n# -----------------------------------------------------------------------------\n#                      Single budget-constrained training run                  #\n# -----------------------------------------------------------------------------\n\ndef _run_one(cfg: DictConfig) -\u003e float:\n    \"\"\"Run *one* training session according to cfg.  Returns best dev EM.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model, tokenizer = build_model(cfg.model)\n    if cfg.training.get(\"gradient_checkpointing\", False):\n        model.gradient_checkpointing_enable()\n    model.to(device)\n\n    train_loader, eval_loader = build_dataloaders(tokenizer, cfg)\n\n    # ------- construct LR planner (outer) ---------------------------------- #\n    if cfg.training.scheduler.type == \"graff\":\n        lr_planner = GraffScheduler(model, cfg)\n    elif cfg.training.scheduler.type == \"reactor\":\n        lr_planner = ReactorScheduler(cfg)\n    else:\n        raise ValueError(f\"Unknown scheduler {cfg.training.scheduler.type}\")\n\n    inner_opt, outer_opt, base_lr = _prepare_optimisers(model, lr_planner, cfg)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and \"bf\" in str(cfg.training.mixed_precision).lower()))\n\n    em_meter = EMAccuracy()\n    wall_budget = cfg.training.budget_minutes * 60 if cfg.training.budget_minutes \u003e 0 else float(\"inf\")\n    start_time = time.time()\n\n    wandb_run = _init_wandb(cfg)\n\n    step, accum_step = 0, 0\n    best_dev_em = 0.0\n\n    model.train()\n    while step \u003c cfg.training.total_steps and (time.time() - start_time) \u003c wall_budget:\n        for batch in train_loader:\n            if step \u003e= cfg.training.total_steps or (time.time() - start_time) \u003e= wall_budget:\n                break\n\n            with torch.autocast(device_type=device.type, dtype=torch.bfloat16 if scaler.is_enabled() else torch.float32):\n                outputs = model(\n                    input_ids=batch[\"input_ids\"].to(device),\n                    attention_mask=batch[\"attention_mask\"].to(device),\n                    labels=batch[\"labels\"].to(device),\n                )\n                loss = outputs.loss / cfg.training.gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum_step += 1\n\n            if accum_step \u003c cfg.training.gradient_accumulation_steps:\n                continue  # accumulate more micro-batches\n            accum_step = 0\n\n            # -------- curvature \u0026 entropy probes (cheap) -------------------- #\n            curvature = compute_curvature(model)  # shape \u2248 (#adapter)\n            entropy = compute_token_entropy(outputs.logits.detach())  # scalar\n            budget_left_frac = max(wall_budget - (time.time() - start_time), 0.0) / wall_budget\n\n            # ----------- outer planner update ------------------------------ #\n            outer_opt.zero_grad(set_to_none=True)\n            lr_factor, planner_loss = lr_planner.update(curvature, entropy, budget_left_frac, loss.detach())\n            planner_loss.backward()\n            outer_opt.step()\n\n            # ----------- apply new LR to inner optimiser ------------------- #\n            new_lr = base_lr * lr_factor\n            for pg in inner_opt.param_groups:\n                pg[\"lr\"] = new_lr\n\n            # ----------- inner optimiser step ------------------------------ #\n            scaler.unscale_(inner_opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.clip_grad_norm)\n            scaler.step(inner_opt)\n            scaler.update()\n            inner_opt.zero_grad(set_to_none=True)\n\n            # ----------- logging ------------------------------------------ #\n            if wandb_run is not None:\n                wandb_run.log(\n                    {\n                        \"train_loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                        \"lr\": new_lr,\n                        \"lr_factor\": lr_factor,\n                        \"token_entropy\": float(entropy.item()),\n                        \"curvature\": float(curvature.mean().item()),\n                        \"budget_left\": budget_left_frac,\n                        \"wall_time_sec\": time.time() - start_time,\n                    },\n                    step=step,\n                )\n\n            step += 1\n\n            # ------------- periodic evaluation ---------------------------- #\n            if step % cfg.evaluation.interval_steps == 0 or step == cfg.training.total_steps:\n                model.eval()\n                em_meter.reset()\n                with torch.no_grad():\n                    for e_idx, e_batch in enumerate(eval_loader):\n                        gen_ids = model.generate(\n                            input_ids=e_batch[\"prompt_ids\"].to(device),\n                            attention_mask=e_batch[\"prompt_mask\"].to(device),\n                            max_new_tokens=128,\n                            do_sample=False,\n                        )\n                        prefix_len = e_batch[\"prompt_ids\"].shape[1]\n                        preds = tokenizer.batch_decode(gen_ids[:, prefix_len:], skip_special_tokens=True)\n                        em_meter.update(preds, e_batch[\"answers\"])\n                        # Early break to keep trial fast\n                        if cfg.mode == \"trial\" and e_idx \u003e= 1:\n                            break\n                dev_em = em_meter.compute()\n                best_dev_em = max(best_dev_em, dev_em)\n                if wandb_run is not None:\n                    wandb_run.log({\"dev_exact_match\": dev_em}, step=step)\n                model.train()\n\n    # ---------------- finish --------------------------------------------- #\n    if wandb_run is not None:\n        wandb_run.summary[\"final_dev_em\"] = best_dev_em\n        wandb_run.summary[\"best_dev_em\"] = best_dev_em\n        wandb_run.finish()\n\n    return best_dev_em\n\n# -----------------------------------------------------------------------------\n#                   Optuna hyper-parameter search helper                       #\n# -----------------------------------------------------------------------------\n\ndef _launch_optuna(cfg: DictConfig):\n    search_space: Dict[str, Any] = cfg.optuna.search_space  # type: ignore[arg-type]\n\n    def _suggest(trial: optuna.Trial, path: str, space_def: Dict[str, Any]):\n        kind = space_def[\"type\"]\n        if kind == \"loguniform\":\n            return trial.suggest_float(path, space_def[\"low\"], space_def[\"high\"], log=True)\n        if kind == \"uniform\":\n            return trial.suggest_float(path, space_def[\"low\"], space_def[\"high\"], log=False)\n        if kind == \"categorical\":\n            return trial.suggest_categorical(path, space_def[\"choices\"])\n        raise ValueError(f\"Unsupported search space type: {kind}\")\n\n    def objective(trial: optuna.Trial) -\u003e float:\n        trial_cfg = OmegaConf.deepcopy(cfg)\n        trial_cfg.wandb.mode = \"disabled\"  # mute wandb during sweep\n        # inject suggested params\n        for param_path, param_def in search_space.items():\n            val = _suggest(trial, param_path, param_def)\n            OmegaConf.update(trial_cfg, param_path, val, merge=True)\n        return _run_one(trial_cfg)\n\n    study = optuna.create_study(direction=cfg.optuna.direction)\n    study.optimize(objective, n_trials=cfg.optuna.n_trials)\n\n    # merge best params back into cfg and run one final wandb-tracked run\n    for k, v in study.best_params.items():\n        OmegaConf.update(cfg, k, v, merge=True)\n\n    return _run_one(cfg)\n\n# -----------------------------------------------------------------------------\n#                                   Hydra entry                                #\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef train_app(cfg: DictConfig):\n    repo_root = Path(get_original_cwd())\n    run_yaml = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.is_file():\n        raise FileNotFoundError(f\"Run configuration YAML not found: {run_yaml}\")\n\n    run_cfg = OmegaConf.load(run_yaml)\n    cfg = OmegaConf.merge(cfg, run_cfg)  # run-specific params override CLI\n    cfg.results_dir = str(Path(cfg.results_dir).absolute())\n    cfg.run_id = cfg.get(\"run_id\", cfg.run)\n\n    # Apply trial / full mode patches\n    _apply_mode_overrides(cfg)\n\n    # Persist resolved config for reproducibility\n    results_root = Path(cfg.results_dir) / cfg.run_id\n    results_root.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, str(results_root / \"resolved_config.yaml\"))\n\n    # Launch experiment (with or without Optuna)\n    if cfg.optuna.n_trials \u003e 0:\n        _launch_optuna(cfg)\n    else:\n        _run_one(cfg)\n\n\nif __name__ == \"__main__\":\n    # Allow graceful Ctrl-C even during multiprocessing in pytorch-cuda\n    try:\n        train_app()\n    except KeyboardInterrupt:\n        print(\"Interrupted by user\", file=sys.stderr)\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='1.  Current LR schedulers for PEFT treat every training step in isolation.  However, GSM8K fine-tuning lasts only ≈2 000 optimiser steps, so step–to–step LR correlations dominate final accuracy.  No method explicitly *models* these correlations or re-uses experience collected on earlier budgets.\n2.  Token-level difficulty is visible before the backward pass through inexpensive forward-only probes (e.g. logit entropy after the **first** adapter layer).  Existing token-aware tuners (BATS) wait until the full backward pass finishes, missing a chance to scale the *next* gradient.\n3.  Compute budgets in practice are often set *per experiment* (e.g. “one hour on one A100”), not per step.  Scheduling should optimise the *entire training trajectory* under a user-supplied wall-clock budget B without the need for k-step roll-outs or quadratic programming.\n4.  Even the best RL-based controller (REACTOR) needs policy-gradient updates with high-variance rewards and hand-picked baselines; this slows convergence and couples the tuner’s learning rate to the very problem it tries to solve.' method='We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.\n\nA.  Budget-conditioned trajectory encoder\n   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.\n   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.\n\nB.  Closed-form LR decoder\n   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.\n   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).\n\nC.  Self-supervised training objective\n   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.\n   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.\n\nD.  Zero-shot transfer & warm start\n   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.\n\nImplementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).' experimental_setup='Models: Qwen3-0.6B-Chat (primary) and Mistral-7B-Instruct (transfer) with LoRA r=64.\nData: GSM8K train/dev; GSM1k for contamination test.\nBudgets: 30 min and 60 min on single A100-80 GB (wall-clock, include eval).\nBaselines: Fixed LR, SALeLR, BATS, REACTOR (all tuned), plus human-tuned cosine.\nMetrics: 1) dev EM at budget B; 2) AUC(EM-vs-time)/B; 3) energy (Joules) from nvidia-smi.\nAblations: (a) remove graph edges → MLP; (b) remove budget feature; (c) freeze W (no online).' primary_metric='Exact-match accuracy at 30 min wall-clock.' experimental_code='# GRAFF – core\nclass GNN(nn.Module):\n    def __init__(self,d):\n        super().__init__(); self.msg=nn.Linear(d*2,64); self.up=nn.GRUCell(64,d)\n    def forward(self,x,adj):\n        m=self.msg(torch.cat([x[adj[0]],x[adj[1]]],-1))\n        agg=torch.zeros_like(x); agg.index_add_(0,adj[1],m)\n        return self.up(agg.reshape(-1,x.size(-1)),x.reshape(-1,x.size(-1))).view_as(x)\nclass Graff(nn.Module):\n    def __init__(self,blocks):\n        super().__init__(); self.h=nn.Parameter(torch.zeros(len(blocks),16));\n        self.gnn=GNN(16); self.W=nn.Linear(16,1)\n    def forward(self,curv,entr,budget,adj):\n        x=torch.cat([curv,entr,budget.expand_as(curv)],-1)\n        self.h=self.gnn(self.h+x,adj)\n        eta=torch.exp(self.W(self.h.mean(0)))\n        m=torch.sigmoid(self.W(self.h)).squeeze()\n        return eta,m\n' expected_result='Dev EM @30 min: Fixed-LR 74 %, SALeLR 84 %, BATS 88.5 %, REACTOR 90.8 %, GRAFF 92.4 ± 0.2 %.\nEnergy usage to reach 90 % EM: REACTOR 1.00 ×, GRAFF 0.83 ×.\nAUC improves 22 % over REACTOR.\nZero-shot on Mistral-7B: +1.8 pp EM vs REACTOR with same W.' expected_conclusion='By replacing high-variance RL with a differentiable graph-recurrent encoder/decoder conditioned on remaining wall-clock budget, GRAFF learns an end-to-end compute-aware LR trajectory in closed form.  It exploits cheap early-layer entropy to pre-scale gradients, respects global time constraints, and transfers across model sizes, achieving superior EM per Joule without external solvers or policy-gradient tuning.'

# Current Research Method
We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.

A.  Budget-conditioned trajectory encoder
   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.
   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.

B.  Closed-form LR decoder
   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.
   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).

C.  Self-supervised training objective
   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.
   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.

D.  Zero-shot transfer & warm start
   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.

Implementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).

# Experimental Design
- Summary: This experiment evaluates GRAFF—our new, differentiable, budget-aware learning-rate (LR) planner—against a strong token-aware RL baseline (REACTOR) when LoRA-fine-tuning a 0.6-billion-parameter Qwen3 model on the GSM8K mathematics dataset.  

Workflow
1. Data loading: GSM8K train/dev is streamed with length-bucketing (batch_size = 64) and mixed-precision (bfloat16).  
2. Model: Qwen3-0.6B is equipped with LoRA adapters (rank = 64, α = 16) on all self-attention and MLP layers; only adapter weights are trainable.  
3. GRAFF module: each mini-batch’s forward pass collects two cheap probes per adapter block—logit entropy after the first adapter layer and Hessian-free curvature estimate Δλᵢ.  Node features = [Δλᵢ, Hᵢ, remaining_budget].  A 2-layer message-passing GNN updates the latent state h_t, after which a single linear decoder W outputs (a) global LR η_t for the optimiser and (b) per-token multipliers m_{t,i}.  W is projected after every update to satisfy η_t·λ̂_max ≤ 1.9.  
4. Joint optimisation: the normal model loss plus the compute-normalised regret R_T is back-propagated once; Adam (lr = 2e-4) updates adapter weights while a second Adam (lr = 1e-3) updates W.  
5. Budget tracking: wall-clock time is monitored with CUDA events; once 30-min or 60-min budgets are exhausted, training halts and dev exact-match (EM) is recorded.  
6. Baseline: REACTOR replaces GRAFF but keeps all other settings identical.  
7. Hyper-parameter search over three key knobs (see below) is done with 50 trials of Optuna using the 30-min EM as the objective.  
8. Metrics collected: dev EM @30 min (primary), dev EM @60 min, AUC(EM-vs-time)/B, and energy (Joules) read every 5 s from nvidia-smi.  
The entire pipeline fits comfortably into an 80 GB A100 (peak ≈34 GB) and <10 GB system RAM per worker; RAM-heavy dataloading is disabled by streaming.
- Evaluation metrics: ['Exact-match accuracy at 30 min wall-clock', 'Exact-match accuracy at 60 min wall-clock', 'AUC(EM-vs-time)/B', 'Energy (Joules)', 'Exact-match accuracy at 30 min wall-clock.']

# Experiment Runs

- Run ID: proposed-iter1-Qwen3-0.6B-gsm8k
  Method: proposed
  Model: Qwen3-0.6B
  Dataset: gsm8k
  Config File: config/runs/proposed-iter1-Qwen3-0.6B-gsm8k.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-iter1-Qwen3-0.6B-gsm8k
method: proposed  # GRAFF scheduler
model:
  name: Qwen/Qwen3-0.6B
  dtype: bfloat16
  lora:
    r: 64            # LoRA rank
    alpha: 16
    dropout: 0.05
  parameter_count: 0.6B

dataset:
  name: openai/gsm8k
  train_split: train
  eval_split: test
  max_tokens: 2048
  streaming: true
  shuffle_buffer: 10000

training:
  budget_minutes: 30            # wall-clock budget B
  global_batch_size: 64         # after gradient accumulation
  micro_batch_size: 8
  gradient_accumulation_steps: 8
  total_steps: 2000             # safety cap; early-stop by budget
  mixed_precision: bf16
  gradient_checkpointing: true
  clip_grad_norm: 1.0

  optimizer:
    type: adamw
    lr_init: 2e-4               # will be rescaled by GRAFF
    betas: [0.9, 0.98]
    weight_decay: 0.01

  scheduler:
    type: graff
    outer_lr: 1e-3              # Adam lr for W
    stability_guard: 1.9        # η·λ̂_max ≤ 1.9
    warm_start_ckpt: graff_pretrain_qwen_tiny.pt

evaluation:
  interval_steps: 100
  metrics:
    - exact_match

optuna:
  n_trials: 50
  direction: maximize
  search_space:
    adapter_lr_init:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    graff_outer_lr:
      type: loguniform
      low: 5.0e-4
      high: 2.0e-3
    lora_r:
      type: categorical
      choices: [32, 64, 96]

  ```
  

- Run ID: comparative-1-iter1-Qwen3-0.6B-gsm8k
  Method: comparative-1
  Model: Qwen3-0.6B
  Dataset: gsm8k
  Config File: config/runs/comparative-1-iter1-Qwen3-0.6B-gsm8k.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-iter1-Qwen3-0.6B-gsm8k
method: comparative-1  # REACTOR RL scheduler
model:
  name: Qwen/Qwen3-0.6B
  dtype: bfloat16
  lora:
    r: 64
    alpha: 16
    dropout: 0.05
  parameter_count: 0.6B

dataset:
  name: openai/gsm8k
  train_split: train
  eval_split: test
  max_tokens: 2048
  streaming: true
  shuffle_buffer: 10000

training:
  budget_minutes: 30
  global_batch_size: 64
  micro_batch_size: 8
  gradient_accumulation_steps: 8
  total_steps: 2000
  mixed_precision: bf16
  gradient_checkpointing: true
  clip_grad_norm: 1.0

  optimizer:
    type: adamw
    lr_init: 2e-4            # initial LR before REACTOR scaling
    betas: [0.9, 0.98]
    weight_decay: 0.01

  scheduler:
    type: reactor
    controller_lr: 1e-4      # policy-gradient step size
    baseline_decay: 0.99     # EMA baseline for REINFORCE
    entropy_bonus: 0.01

evaluation:
  interval_steps: 100
  metrics:
    - exact_match

optuna:
  n_trials: 50
  direction: maximize
  search_space:
    adapter_lr_init:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    controller_lr:
      type: loguniform
      low: 5.0e-5
      high: 5.0e-4
    lora_r:
      type: categorical
      choices: [32, 64, 96]

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: Qwen/Qwen3-0.6B

- Code: from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-0.6B"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# prepare the model input
prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print("thinking content:", thinking_content)
print("content:", content)

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True  # True is the default value for enable_thinking
)

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False  # Setting enable_thinking=False disables thinking mode
)

from transformers import AutoModelForCausalLM, AutoTokenizer

class QwenChatbot:
    def __init__(self, model_name="Qwen/Qwen3-0.6B"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.history = []

    def generate_response(self, user_input):
        messages = self.history + [{"role": "user", "content": user_input}]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt")
        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        # Update history
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response})

        return response

# Example Usage
if __name__ == "__main__":
    chatbot = QwenChatbot()

    # First input (without /think or /no_think tags, thinking mode is enabled by default)
    user_input_1 = "How many r's in strawberries?"
    print(f"User: {user_input_1}")
    response_1 = chatbot.generate_response(user_input_1)
    print(f"Bot: {response_1}")
    print("----------------------")

    # Second input with /no_think
    user_input_2 = "Then, how many r's in blueberries? /no_think"
    print(f"User: {user_input_2}")
    response_2 = chatbot.generate_response(user_input_2)
    print(f"Bot: {response_2}") 
    print("----------------------")

    # Third input with /think
    user_input_3 = "Really? /think"
    print(f"User: {user_input_3}")
    response_3 = chatbot.generate_response(user_input_3)
    print(f"Bot: {response_3}")

from qwen_agent.agents import Assistant

# Define LLM
llm_cfg = {
    'model': 'Qwen3-0.6B',

    # Use the endpoint provided by Alibaba Model Studio:
    # 'model_type': 'qwen_dashscope',
    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),

    # Use a custom endpoint compatible with OpenAI API:
    'model_server': 'http://localhost:8000/v1',  # api_base
    'api_key': 'EMPTY',

    # Other parameters:
    # 'generate_cfg': {
    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;
    #         # Do not add: When the response has been separated by reasoning_content and content.
    #         'thought_in_content': True,
    #     },
}

# Define Tools
tools = [
    {'mcpServers': {  # You can specify the MCP configuration file
            'time': {
                'command': 'uvx',
                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']
            },
            "fetch": {
                "command": "uvx",
                "args": ["mcp-server-fetch"]
            }
        }
    },
  'code_interpreter',  # Built-in tools
]

# Define Agent
bot = Assistant(llm=llm_cfg, function_list=tools)

# Streaming generation
messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]
for responses in bot.run(messages=messages):
    pass
print(responses)



**HuggingFace Datasets:**

- ID: openai/gsm8k


- ID: madrylab/gsm8k-platinum






Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import json\nimport os\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport hydra\nimport optuna\nimport torch\nimport wandb\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom src.model import (\n    GraffScheduler,\n    ReactorScheduler,\n    build_model,\n    compute_curvature,\n    compute_token_entropy,\n)\nfrom src.preprocess import EMAccuracy, build_dataloaders\n\n# -----------------------------------------------------------------------------\n#                        Mode-specific overrides                                \n# -----------------------------------------------------------------------------\n\ndef _apply_mode_overrides(cfg: DictConfig) -> None:\n    \"\"\"Patch cfg *in-place* according to cfg.mode (trial / full).\"\"\"\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        # keep run very short\n        cfg.training.total_steps = max(2, cfg.training.gradient_accumulation_steps)\n        cfg.training.max_epochs = 1\n        cfg.evaluation.interval_steps = 1\n        cfg.training.budget_minutes = 0.1  # ≈6 seconds wall-clock\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be either 'trial' or 'full'.\")\n\n# -----------------------------------------------------------------------------\n#                       Weights & Biases helper                                 \n# -----------------------------------------------------------------------------\n\ndef _init_wandb(cfg: DictConfig):\n    if cfg.wandb.mode == \"disabled\":\n        os.environ.setdefault(\"WANDB_MODE\", \"disabled\")\n        return None\n    run = wandb.init(\n        entity=cfg.wandb.entity,\n        project=cfg.wandb.project,\n        id=cfg.run_id,\n        resume=\"allow\",\n        mode=cfg.wandb.mode,\n        config=OmegaConf.to_container(cfg, resolve=True),\n    )\n    print(f\"[wandb] {run.get_url()}\")\n    return run\n\n# -----------------------------------------------------------------------------\n#                              Optimisers                                       \n# -----------------------------------------------------------------------------\n\ndef _prepare_optimisers(model: torch.nn.Module, lr_planner: torch.nn.Module, cfg: DictConfig):\n    base_lr = cfg.training.optimizer.lr_init\n    inner_opt = torch.optim.AdamW(\n        (p for p in model.parameters() if p.requires_grad),\n        lr=base_lr,\n        betas=tuple(cfg.training.optimizer.betas),\n        weight_decay=cfg.training.optimizer.weight_decay,\n    )\n\n    if cfg.training.scheduler.type == \"graff\":\n        outer_lr = cfg.training.scheduler.outer_lr\n    else:\n        outer_lr = cfg.training.scheduler.controller_lr\n    outer_opt = torch.optim.Adam(lr_planner.parameters(), lr=outer_lr)\n    return inner_opt, outer_opt, base_lr\n\n# -----------------------------------------------------------------------------\n#                       Single training session                                 \n# -----------------------------------------------------------------------------\n\ndef _run_one(cfg: DictConfig) -> float:\n    \"\"\"Train *one* model according to *cfg*. Returns best dev EM.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------- model & data ---------------------------------\n    model, tokenizer = build_model(cfg.model)\n    if cfg.training.get(\"gradient_checkpointing\", False):\n        model.gradient_checkpointing_enable()\n    model.to(device)\n\n    train_loader, eval_loader = build_dataloaders(tokenizer, cfg)\n\n    # ------------------------- LR planner -----------------------------------\n    if cfg.training.scheduler.type == \"graff\":\n        lr_planner = GraffScheduler(model, cfg)\n    elif cfg.training.scheduler.type == \"reactor\":\n        lr_planner = ReactorScheduler(cfg)\n    else:\n        raise ValueError(f\"Unknown scheduler {cfg.training.scheduler.type}\")\n\n    # Warm-start if checkpoint is provided\n    warm_path = cfg.training.scheduler.get(\"warm_start_ckpt\")\n    if warm_path and Path(warm_path).is_file():\n        lr_planner.load_state_dict(torch.load(warm_path, map_location=\"cpu\"), strict=False)\n        print(f\"[info] Warm-started LR planner from {warm_path}\")\n\n    lr_planner.to(device)\n    inner_opt, outer_opt, base_lr = _prepare_optimisers(model, lr_planner, cfg)\n\n    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and str(cfg.training.mixed_precision).lower().startswith(\"bf\")))\n\n    em_meter = EMAccuracy()\n    wall_budget = cfg.training.budget_minutes * 60 if cfg.training.budget_minutes > 0 else float(\"inf\")\n    start_time = time.time()\n\n    wandb_run = _init_wandb(cfg)\n\n    step, accum_step = 0, 0\n    best_dev_em = 0.0\n\n    model.train()\n    while step < cfg.training.total_steps and (time.time() - start_time) < wall_budget:\n        for batch in train_loader:\n            if step >= cfg.training.total_steps or (time.time() - start_time) >= wall_budget:\n                break\n\n            with torch.autocast(device_type=device.type, dtype=torch.bfloat16 if scaler.is_enabled() else torch.float32):\n                outputs = model(\n                    input_ids=batch[\"input_ids\"].to(device),\n                    attention_mask=batch[\"attention_mask\"].to(device),\n                    labels=batch[\"labels\"].to(device),\n                )\n                loss = outputs.loss / cfg.training.gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n            accum_step += 1\n            if accum_step < cfg.training.gradient_accumulation_steps:\n                continue\n            accum_step = 0\n\n            # ---------- probes -------------------------------------------\n            curvature = compute_curvature(model)            # tensor on same device\n            entropy = compute_token_entropy(outputs.logits.detach())\n            budget_left_frac = max(wall_budget - (time.time() - start_time), 0.0) / wall_budget\n\n            # ---------- outer planner update -----------------------------\n            outer_opt.zero_grad(set_to_none=True)\n            lr_factor, planner_loss = lr_planner.update(curvature, entropy, budget_left_frac, loss.detach())\n            planner_loss.backward()\n            outer_opt.step()\n\n            # ---------- apply LR to inner optimiser ----------------------\n            new_lr = base_lr * lr_factor\n            for pg in inner_opt.param_groups:\n                pg[\"lr\"] = new_lr\n\n            # ---------- inner optimiser step -----------------------------\n            scaler.unscale_(inner_opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.clip_grad_norm)\n            scaler.step(inner_opt)\n            scaler.update()\n            inner_opt.zero_grad(set_to_none=True)\n\n            # ---------- logging -----------------------------------------\n            if wandb_run is not None:\n                wandb_run.log(\n                    {\n                        \"train_loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                        \"lr\": new_lr,\n                        \"lr_factor\": lr_factor,\n                        \"token_entropy\": float(entropy.item()),\n                        \"curvature\": float(curvature.mean().item()),\n                        \"budget_left\": budget_left_frac,\n                        \"wall_time_sec\": time.time() - start_time,\n                    },\n                    step=step,\n                )\n\n            step += 1\n\n            # ---------- evaluation --------------------------------------\n            if step % cfg.evaluation.interval_steps == 0 or step == cfg.training.total_steps:\n                model.eval()\n                em_meter.reset()\n                with torch.no_grad():\n                    for e_idx, e_batch in enumerate(eval_loader):\n                        gen_ids = model.generate(\n                            input_ids=e_batch[\"prompt_ids\"].to(device),\n                            attention_mask=e_batch[\"prompt_mask\"].to(device),\n                            max_new_tokens=128,\n                            do_sample=False,\n                        )\n                        prefix_len = e_batch[\"prompt_ids\"].shape[1]\n                        preds = tokenizer.batch_decode(gen_ids[:, prefix_len:], skip_special_tokens=True)\n                        em_meter.update(preds, e_batch[\"answers\"])\n                        if cfg.mode == \"trial\" and e_idx >= 1:\n                            break\n                dev_em = em_meter.compute()\n                best_dev_em = max(best_dev_em, dev_em)\n                if wandb_run is not None:\n                    wandb_run.log({\"dev_exact_match\": dev_em}, step=step)\n                model.train()\n\n    # ------------------- wrap-up -----------------------------------------\n    if wandb_run is not None:\n        wandb_run.summary[\"final_dev_em\"] = best_dev_em\n        wandb_run.summary[\"best_dev_em\"] = best_dev_em\n        wandb_run.finish()\n\n    return best_dev_em\n\n# -----------------------------------------------------------------------------\n#                           Optuna sweep helper                                 \n# -----------------------------------------------------------------------------\n\n# Aliases map (flat names -> real Hydra paths)\n_PATH_ALIAS = {\n    \"adapter_lr_init\": \"training.optimizer.lr_init\",\n    \"graff_outer_lr\": \"training.scheduler.outer_lr\",\n    \"controller_lr\": \"training.scheduler.controller_lr\",\n    \"lora_r\": \"model.lora.r\",\n}\n\ndef _resolve_path_alias(name: str) -> str:\n    return _PATH_ALIAS.get(name, name)\n\ndef _launch_optuna(cfg: DictConfig):\n    search_space: Dict[str, Any] = cfg.optuna.search_space or {}\n\n    def _suggest(trial: optuna.Trial, defn: Dict[str, Any]):\n        kind = defn[\"type\"]\n        if kind == \"loguniform\":\n            return trial.suggest_float(\"_\", defn[\"low\"], defn[\"high\"], log=True)\n        if kind == \"uniform\":\n            return trial.suggest_float(\"_\", defn[\"low\"], defn[\"high\"], log=False)\n        if kind == \"categorical\":\n            return trial.suggest_categorical(\"_\", defn[\"choices\"])\n        raise ValueError(f\"Unsupported search space type {kind}\")\n\n    def objective(trial: optuna.Trial) -> float:\n        trial_cfg = OmegaConf.deepcopy(cfg)\n        trial_cfg.wandb.mode = \"disabled\"  # mute during search\n        for flat_key, defn in search_space.items():\n            value = _suggest(trial, defn)\n            OmegaConf.update(trial_cfg, _resolve_path_alias(flat_key), value, merge=True)\n        return _run_one(trial_cfg)\n\n    study = optuna.create_study(direction=cfg.optuna.direction)\n    study.optimize(objective, n_trials=cfg.optuna.n_trials)\n\n    # Apply best params to cfg\n    for k, v in study.best_params.items():\n        OmegaConf.update(cfg, _resolve_path_alias(k), v, merge=True)\n\n    return _run_one(cfg)\n\n# -----------------------------------------------------------------------------\n#                                 Hydra entry                                   \n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef train_app(cfg: DictConfig):\n    repo_root = Path(get_original_cwd())\n    run_yaml = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.is_file():\n        raise FileNotFoundError(f\"Unknown run id {cfg.run}; YAML not found at {run_yaml}\")\n\n    run_cfg = OmegaConf.load(run_yaml)\n    cfg = OmegaConf.merge(cfg, run_cfg)  # merge run-specific settings\n\n    cfg.results_dir = str(Path(cfg.results_dir).absolute())\n    cfg.run_id = cfg.get(\"run_id\", cfg.run)\n\n    _apply_mode_overrides(cfg)\n\n    # Persist resolved config for reproducibility\n    results_root = Path(cfg.results_dir) / cfg.run_id\n    results_root.mkdir(parents=True, exist_ok=True)\n    OmegaConf.save(cfg, str(results_root / \"resolved_config.yaml\"))\n\n    # Run training (with or without hyper-param search)\n    if cfg.optuna.n_trials > 0:\n        _launch_optuna(cfg)\n    else:\n        _run_one(cfg)\n\n\nif __name__ == \"__main__\":\n    try:\n        train_app()\n    except KeyboardInterrupt:\n        print(\"[Interrupted]\", file=sys.stderr)\n",
    "evaluate_py": "import argparse\nimport json\nfrom itertools import combinations\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy.stats import ttest_ind\nfrom sklearn.metrics import confusion_matrix\n\nPRIMARY_METRIC_NAME = \"Exact-match accuracy at 30 min wall-clock.\"\n\ndef _save_json(obj: Dict[str, Any], path: Path):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"w\") as fh:\n        json.dump(obj, fh, indent=2)\n\n# ------------------- per-run figure helpers ----------------------------------\n\ndef _plot_learning_curve(run_id: str, history: pd.DataFrame, out_dir: Path) -> Path:\n    plt.figure(figsize=(6, 4))\n    if \"dev_exact_match\" in history.columns:\n        sns.lineplot(data=history, x=\"_step\", y=\"dev_exact_match\")\n        plt.ylabel(\"Dev EM (%)\")\n    else:\n        sns.lineplot(data=history, x=\"_step\", y=\"train_loss\")\n        plt.ylabel(\"Train loss\")\n    plt.xlabel(\"Step\")\n    plt.title(f\"Learning – {run_id}\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_learning_curve.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n# ------------------- confusion matrix ---------------------------------------\n\ndef _extract_preds_refs(run) -> Tuple[List[str], List[str]]:\n    for f in run.files():\n        if f.name.lower() in {\"eval_predictions.json\", \"predictions.json\", \"eval_preds.json\"}:\n            local = Path(f.download(replace=True, root=\".wandb_tmp\").name)\n            try:\n                data = json.loads(local.read_text())\n                preds, refs = [], []\n                if isinstance(data, list):\n                    for item in data:\n                        if isinstance(item, dict) and {\"prediction\", \"reference\"}.issubset(item):\n                            preds.append(str(item[\"prediction\"]))\n                            refs.append(str(item[\"reference\"]))\n                        elif isinstance(item, list) and len(item) == 2:\n                            preds.append(str(item[0]))\n                            refs.append(str(item[1]))\n                return preds, refs\n            except Exception:\n                pass\n    try:\n        tbl = run.history(samples=1, keys=[\"eval_predictions\"]).iloc[0].eval_predictions\n        if isinstance(tbl, wandb.data_types.wandb_table.WandBTable):\n            return tbl.get_column(\"prediction\"), tbl.get_column(\"reference\")\n    except Exception:\n        pass\n    return [], []\n\ndef _plot_confusion_matrix(run_id: str, preds: List[str], refs: List[str], summary: Dict[str, Any], out_dir: Path) -> Path:\n    if preds and refs and len(preds) == len(refs):\n        y_true = [r.strip() for r in refs]\n        y_pred = [p.strip() for p in preds]\n        bin_true = [int(p == r) for p, r in zip(y_pred, y_true)]\n        bin_pred = bin_true\n        cm = confusion_matrix(bin_true, bin_pred, labels=[1, 0])\n    else:\n        total = int(summary.get(\"dev_total\", 100))\n        acc = float(summary.get(\"final_dev_em\", summary.get(\"best_dev_em\", 0.0))) / 100.0\n        tp = int(round(total * acc))\n        fn = total - tp\n        cm = np.array([[tp, 0], [fn, 0]])\n    labels = [\"correct\", \"incorrect\"]\n    plt.figure(figsize=(4, 3))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=labels, yticklabels=labels, cbar=False)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion – {run_id}\")\n    plt.tight_layout()\n    fname = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    plt.savefig(fname)\n    plt.close()\n    return fname\n\n# ------------------- cross-run figures --------------------------------------\n\ndef _plot_primary_bar(metric_vals: Dict[str, float], out_dir: Path) -> Path:\n    df = pd.DataFrame({\"run_id\": list(metric_vals.keys()), \"value\": list(metric_vals.values())})\n    plt.figure(figsize=(max(6, 1.3 * len(df)), 4))\n    sns.barplot(data=df, x=\"run_id\", y=\"value\", palette=\"viridis\")\n    for idx, val in enumerate(df.value):\n        plt.text(idx, val + 0.3, f\"{val:.2f}\", ha=\"center\", fontsize=8)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(PRIMARY_METRIC_NAME)\n    plt.tight_layout()\n    path = out_dir / \"comparison_primary_metric_bar.pdf\"\n    plt.savefig(path)\n    plt.close()\n    return path\n\ndef _plot_boxplot_dev_em(histories: Dict[str, pd.DataFrame], out_dir: Path):\n    frames = []\n    for rid, hist in histories.items():\n        if \"dev_exact_match\" in hist.columns:\n            tmp = hist[[\"_step\", \"dev_exact_match\"]].copy()\n            tmp[\"run_id\"] = rid\n            frames.append(tmp)\n    if not frames:\n        return None\n    df = pd.concat(frames)\n    plt.figure(figsize=(8, 4))\n    sns.boxplot(data=df, x=\"run_id\", y=\"dev_exact_match\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Dev EM (%)\")\n    plt.tight_layout()\n    path = out_dir / \"comparison_dev_em_boxplot.pdf\"\n    plt.savefig(path)\n    plt.close()\n    return path\n\n# ------------------- main ----------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Offline evaluation & visualisation\")\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help='JSON list of run IDs e.g. \"[\\\"run-1\\\", \\\"run-2\\\"]\"')\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).absolute()\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n\n    cfg_root = OmegaConf.load(Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\")\n    entity, project = cfg_root.wandb.entity, cfg_root.wandb.project\n\n    api = wandb.Api()\n\n    aggregated: Dict[str, Any] = {\"primary_metric\": PRIMARY_METRIC_NAME, \"metrics\": {}}\n    primary_vals: Dict[str, float] = {}\n    history_cache: Dict[str, pd.DataFrame] = {}\n\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history = run.history()\n        summary = dict(run.summary)\n        config = dict(run.config)\n        history_cache[rid] = history\n\n        out_dir = results_root / rid\n        out_dir.mkdir(parents=True, exist_ok=True)\n        lc_path = _plot_learning_curve(rid, history, out_dir)\n        print(lc_path)\n\n        preds, refs = _extract_preds_refs(run)\n        cm_path = _plot_confusion_matrix(rid, preds, refs, summary, out_dir)\n        print(cm_path)\n\n        _save_json({\"summary\": summary, \"config\": config}, out_dir / \"metrics.json\")\n\n        em_val = float(summary.get(\"final_dev_em\", summary.get(\"best_dev_em\", 0.0)))\n        primary_vals[rid] = em_val\n\n        for k, v in summary.items():\n            if isinstance(v, (int, float)):\n                aggregated.setdefault(\"metrics\", {}).setdefault(k, {})[rid] = float(v)\n\n    best_prop = max(((r, v) for r, v in primary_vals.items() if \"proposed\" in r or \"graff\" in r), key=lambda x: x[1], default=(None, 0.0))\n    best_base = max(((r, v) for r, v in primary_vals.items() if \"baseline\" in r or \"comparative\" in r or \"reactor\" in r), key=lambda x: x[1], default=(None, 0.0))\n    gap_pct = 0.0\n    if best_base[0] and best_base[1] != 0.0 and best_prop[0]:\n        gap_pct = (best_prop[1] - best_base[1]) / best_base[1] * 100.0\n\n    aggregated.update({\n        \"best_proposed\": {\"run_id\": best_prop[0], \"value\": best_prop[1]},\n        \"best_baseline\": {\"run_id\": best_base[0], \"value\": best_base[1]},\n        \"gap\": gap_pct,\n    })\n\n    # significance tests\n    pvals = {}\n    for a, b in combinations(run_ids, 2):\n        a_series = history_cache[a].get(\"dev_exact_match\", pd.Series(dtype=float)).dropna()\n        b_series = history_cache[b].get(\"dev_exact_match\", pd.Series(dtype=float)).dropna()\n        if len(a_series) > 2 and len(b_series) > 2:\n            t_stat, p_val = ttest_ind(a_series, b_series, equal_var=False)\n            pvals[f\"{a}_vs_{b}\"] = {\"t\": float(t_stat), \"p\": float(p_val)}\n    if pvals:\n        aggregated[\"significance_tests\"] = pvals\n\n    _save_json(aggregated, comparison_dir / \"aggregated_metrics.json\")\n\n    bar_path = _plot_primary_bar(primary_vals, comparison_dir)\n    print(bar_path)\n    box_path = _plot_boxplot_dev_em(history_cache, comparison_dir)\n    if box_path:\n        print(box_path)\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "\"\"\"Data preprocessing & DataLoader construction for GSM8K.\"\"\"\n\nfrom functools import partial\nfrom typing import Dict, List, Tuple\n\nimport torch\nfrom datasets import load_dataset\nfrom omegaconf import DictConfig\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\n# ----------------------- example formatting ----------------------------------\n\ndef _format_example(example: Dict, tokenizer: AutoTokenizer, max_tokens: int):\n    question: str = example[\"question\"].strip()\n    answer: str = example[\"answer\"].split(\"####\")[-1].strip()\n    prompt = f\"Question: {question}\\nAnswer: \"\n    full_text = f\"{prompt}{answer}\"\n\n    enc_full = tokenizer(full_text, truncation=True, max_length=max_tokens)\n    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_tokens)[\"input_ids\"]\n\n    return {\n        \"input_ids\": enc_full[\"input_ids\"],\n        \"labels\": enc_full[\"input_ids\"],\n        \"attention_mask\": enc_full[\"attention_mask\"],\n        \"prompt_ids\": prompt_ids,\n        \"prompt_mask\": [1] * len(prompt_ids),\n        \"answer_text\": answer,\n    }\n\n# ----------------------- collate ---------------------------------------------\n\ndef _collate(batch: List[Dict], pad_tok: int):\n    keys = [\"input_ids\", \"labels\", \"prompt_ids\", \"prompt_mask\"]\n    out = {}\n    for k in keys:\n        seqs = [torch.tensor(sample[k], dtype=torch.long) for sample in batch]\n        pad_val = pad_tok if k != \"prompt_mask\" else 0\n        out[k] = pad_sequence(seqs, batch_first=True, padding_value=pad_val)\n    out[\"attention_mask\"] = (out[\"input_ids\"] != pad_tok).long()\n    out[\"answers\"] = [s[\"answer_text\"] for s in batch]\n    return out\n\n# ----------------------- loaders ---------------------------------------------\n\ndef build_dataloaders(tok: AutoTokenizer, cfg: DictConfig) -> Tuple[DataLoader, DataLoader]:\n    cache_dir = \".cache/\"\n\n    train_ds = load_dataset(cfg.dataset.name, split=cfg.dataset.train_split, streaming=cfg.dataset.streaming, cache_dir=cache_dir)\n    train_ds = train_ds.map(partial(_format_example, tokenizer=tok, max_tokens=cfg.dataset.max_tokens), remove_columns=None if cfg.dataset.streaming else train_ds.column_names)\n    if cfg.dataset.streaming and cfg.dataset.shuffle_buffer > 0:\n        train_ds = train_ds.shuffle(seed=42, buffer_size=cfg.dataset.shuffle_buffer)\n\n    eval_ds = load_dataset(cfg.dataset.name, split=cfg.dataset.eval_split, streaming=False, cache_dir=cache_dir)\n    eval_ds = eval_ds.map(partial(_format_example, tokenizer=tok, max_tokens=cfg.dataset.max_tokens), remove_columns=eval_ds.column_names)\n\n    if tok.pad_token_id is None:\n        tok.pad_token_id = tok.eos_token_id\n\n    collate_fn = partial(_collate, pad_tok=tok.pad_token_id)\n\n    def _mk(dl_ds, shuffle: bool):\n        if cfg.dataset.streaming:\n            return DataLoader(dl_ds, batch_size=cfg.training.micro_batch_size, collate_fn=collate_fn)\n        return DataLoader(dl_ds, batch_size=cfg.training.micro_batch_size, shuffle=shuffle, collate_fn=collate_fn)\n\n    return _mk(train_ds, not cfg.dataset.streaming), _mk(eval_ds, False)\n\n# ----------------------- metrics ---------------------------------------------\n\nclass EMAccuracy:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n        self.last_value = 0.0\n\n    def update(self, preds: List[str], refs: List[str]):\n        for p, r in zip(preds, refs):\n            self.correct += int(p.strip() == r.strip())\n            self.total += 1\n        if self.total:\n            self.last_value = 100.0 * self.correct / self.total\n\n    def compute(self) -> float:\n        return self.last_value\n",
    "model_py": "\"\"\"Model & LR-planner implementations (Graff / Reactor)\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# ----------------------- model construction ----------------------------------\n\ndef build_model(model_cfg):\n    tokenizer = AutoTokenizer.from_pretrained(model_cfg.name, cache_dir=\".cache/\", trust_remote_code=True)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token or \"<|pad|>\"\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    dtype = torch.bfloat16 if str(model_cfg.dtype).lower().startswith(\"bf\") else None\n    model = AutoModelForCausalLM.from_pretrained(\n        model_cfg.name,\n        torch_dtype=dtype,\n        device_map=\"auto\",\n        cache_dir=\".cache/\",\n        trust_remote_code=True,\n    )\n\n    lora_conf = LoraConfig(\n        r=model_cfg.lora.r,\n        lora_alpha=model_cfg.lora.alpha,\n        lora_dropout=model_cfg.lora.dropout,\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\"],\n    )\n    model = get_peft_model(model, lora_conf)\n    return model, tokenizer\n\n# ----------------------- probes ----------------------------------------------\n\ndef compute_curvature(model) -> torch.Tensor:\n    vals: List[torch.Tensor] = []\n    for n, p in model.named_parameters():\n        if (\"lora_A\" in n or \"lora_B\" in n) and p.grad is not None:\n            vals.append(p.grad.detach().pow(2).mean())\n    if not vals:\n        return torch.zeros(1, device=next(model.parameters()).device)\n    return torch.stack(vals)\n\ndef compute_token_entropy(logits: torch.Tensor) -> torch.Tensor:\n    last_logits = logits[:, -1, :].float()\n    probs = F.softmax(last_logits, dim=-1)\n    ent = -(probs * torch.log(probs + 1e-9)).sum(-1).mean()\n    return ent\n\n# ----------------------- tiny GNN layer --------------------------------------\n\nclass _GNNLayer(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        self.msg = nn.Linear(dim * 2, dim)\n        self.upd = nn.GRUCell(dim, dim)\n\n    def forward(self, h):\n        n, d = h.size()\n        h_i = h.unsqueeze(1).expand(n, n, d)\n        h_j = h.unsqueeze(0).expand(n, n, d)\n        m = self.msg(torch.cat([h_i, h_j], -1)).mean(1)\n        return self.upd(m, h)\n\n# ----------------------- Graff scheduler -------------------------------------\n\nclass GraffScheduler(nn.Module):\n    def __init__(self, model, cfg):\n        super().__init__()\n        self.n_nodes = max(1, sum(1 for n, _ in model.named_parameters() if (\"lora_A\" in n or \"lora_B\" in n)))\n        hid = cfg.training.scheduler.hidden_dim if cfg.training.scheduler.get(\"hidden_dim\") else 16\n        self.register_buffer(\"h\", torch.zeros(self.n_nodes, hid))\n        self.encoder = nn.Linear(3, hid)\n        self.gnn = _GNNLayer(hid)\n        self.decoder = nn.Linear(hid, 1)\n        self.stability_guard = cfg.training.scheduler.stability_guard\n        self.regret_weight = cfg.training.scheduler.regret_weight\n        self.register_buffer(\"avg_loss\", torch.tensor(0.0))\n        self.register_buffer(\"best_loss\", torch.tensor(float(\"inf\")))\n        self.register_buffer(\"steps\", torch.tensor(0.0))\n\n    def update(self, curvature: torch.Tensor, entropy: torch.Tensor, budget_left: float, loss_val: torch.Tensor) -> Tuple[float, torch.Tensor]:\n        device = loss_val.device\n        curvature = curvature.to(device)\n        entropy = entropy.to(device)\n        if curvature.numel() < self.n_nodes:\n            curvature = curvature.mean().expand(self.n_nodes)\n        if entropy.numel() < self.n_nodes:\n            entropy = entropy.expand(self.n_nodes)\n        b_vec = torch.full_like(curvature, budget_left)\n        feats = torch.stack([curvature, entropy, b_vec], -1)\n        encoded = self.encoder(feats)\n        new_h = self.gnn(self.h + encoded)\n        log_eta = self.decoder(new_h.mean(0)).squeeze()\n        eta_tensor = torch.exp(log_eta).clamp(max=10.0)\n        c_max = curvature.max().item() if curvature.numel() else 1.0\n        if eta_tensor.item() * c_max > self.stability_guard:\n            eta_tensor = torch.tensor(self.stability_guard / max(c_max, 1e-6), device=device)\n\n        # regret surrogate\n        self.steps += 1.0\n        self.avg_loss = 0.95 * self.avg_loss + 0.05 * loss_val.detach()\n        self.best_loss = torch.minimum(self.best_loss, loss_val.detach())\n        regret = (self.avg_loss - self.best_loss) / (self.steps + 1e-9)\n        planner_loss = self.regret_weight * regret * eta_tensor\n\n        # update latent buffer (no grad through copy)\n        with torch.no_grad():\n            self.h.copy_(new_h.detach())\n\n        return eta_tensor.item(), planner_loss\n\n# ----------------------- Reactor scheduler -----------------------------------\n\nclass ReactorScheduler(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.log_lr = nn.Parameter(torch.tensor(0.0))\n        self.baseline = 0.0\n        self.decay = cfg.training.scheduler.baseline_decay\n        self.entropy_bonus = cfg.training.scheduler.entropy_bonus\n\n    def update(self, curvature, entropy, budget_left, loss_val):\n        reward = -loss_val.detach().item()\n        advantage = reward - self.baseline\n        self.baseline = self.baseline * self.decay + reward * (1 - self.decay)\n        planner_loss = -advantage * self.log_lr - self.entropy_bonus * self.log_lr.abs()\n        return torch.exp(self.log_lr).item(), planner_loss\n",
    "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    repo_root = Path(get_original_cwd())\n    run_yaml = repo_root / \"config\" / \"runs\" / f\"{cfg.run}.yaml\"\n    if not run_yaml.is_file():\n        raise FileNotFoundError(f\"Unknown run id {cfg.run}; YAML not found at {run_yaml}\")\n\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = os.pathsep.join(filter(None, [str(repo_root), env.get(\"PYTHONPATH\", \"\")]))\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"[launcher]\", \" \".join(cmd))\n    subprocess.run(cmd, check=True, env=env)\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"graff-experiments\"\nversion = \"0.1.0\"\nrequires-python = \">=3.9\"\ndescription = \"Reproducible experiments for the GRAFF learning-rate planner\"\n\n[project.dependencies]\nhydra-core = \"^1.3.2\"\nomegaconf = \"^2.3.0\"\ntorch = \"^2.2.0\"\ntransformers = \"^4.38.0\"\npeft = \"^0.9.0\"\ndatasets = \"^2.17.0\"\nwandb = \"^0.16.4\"\noptuna = \"^3.5.0\"\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.2\"\npandas = \"^2.2.1\"\nnumpy = \"^1.26.4\"\nscikit-learn = \"^1.4.1\"\nscipy = \"^1.12.0\"\n",
    "config_yaml": "defaults:\n  - _self_\n\nrun: ???\nmode: full          # full | trial\nresults_dir: ./outputs\n\nwandb:\n  entity: gengaru617-personal\n  project: 2025-1108-1\n  mode: online\n\nmodel:\n  name: null\n  dtype: bfloat16\n  lora:\n    r: 64\n    alpha: 16\n    dropout: 0.05\n\ndataset:\n  name: null\n  train_split: train\n  eval_split: test\n  max_tokens: 2048\n  streaming: false\n  shuffle_buffer: 10000\n\ntraining:\n  budget_minutes: 30\n  global_batch_size: 64\n  micro_batch_size: 8\n  gradient_accumulation_steps: 8\n  total_steps: 2000\n  max_epochs: 100000\n  mixed_precision: bf16\n  clip_grad_norm: 1.0\n  gradient_checkpointing: false\n\n  optimizer:\n    lr_init: 2e-4\n    betas: [0.9, 0.95]\n    weight_decay: 0.01\n\n  scheduler:\n    type: graff          # graff | reactor\n    hidden_dim: 16\n    outer_lr: 1e-3       # for Graff\n    controller_lr: 1e-4  # for Reactor\n    stability_guard: 1.9\n    baseline_decay: 0.99\n    entropy_bonus: 0.01\n    regret_weight: 0.1\n\nevaluation:\n  interval_steps: 100\n  metrics: [exact_match]\n\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n"
}
