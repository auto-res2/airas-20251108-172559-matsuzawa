
LLM Name: o3-2025-04-16
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='1.  Current LR schedulers for PEFT treat every training step in isolation.  However, GSM8K fine-tuning lasts only ≈2 000 optimiser steps, so step–to–step LR correlations dominate final accuracy.  No method explicitly *models* these correlations or re-uses experience collected on earlier budgets.\n2.  Token-level difficulty is visible before the backward pass through inexpensive forward-only probes (e.g. logit entropy after the **first** adapter layer).  Existing token-aware tuners (BATS) wait until the full backward pass finishes, missing a chance to scale the *next* gradient.\n3.  Compute budgets in practice are often set *per experiment* (e.g. “one hour on one A100”), not per step.  Scheduling should optimise the *entire training trajectory* under a user-supplied wall-clock budget B without the need for k-step roll-outs or quadratic programming.\n4.  Even the best RL-based controller (REACTOR) needs policy-gradient updates with high-variance rewards and hand-picked baselines; this slows convergence and couples the tuner’s learning rate to the very problem it tries to solve.' method='We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.\n\nA.  Budget-conditioned trajectory encoder\n   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.\n   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.\n\nB.  Closed-form LR decoder\n   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.\n   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).\n\nC.  Self-supervised training objective\n   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.\n   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.\n\nD.  Zero-shot transfer & warm start\n   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.\n\nImplementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).' experimental_setup='Models: Qwen3-0.6B-Chat (primary) and Mistral-7B-Instruct (transfer) with LoRA r=64.\nData: GSM8K train/dev; GSM1k for contamination test.\nBudgets: 30 min and 60 min on single A100-80 GB (wall-clock, include eval).\nBaselines: Fixed LR, SALeLR, BATS, REACTOR (all tuned), plus human-tuned cosine.\nMetrics: 1) dev EM at budget B; 2) AUC(EM-vs-time)/B; 3) energy (Joules) from nvidia-smi.\nAblations: (a) remove graph edges → MLP; (b) remove budget feature; (c) freeze W (no online).' primary_metric='Exact-match accuracy at 30 min wall-clock.' experimental_code='# GRAFF – core\nclass GNN(nn.Module):\n    def __init__(self,d):\n        super().__init__(); self.msg=nn.Linear(d*2,64); self.up=nn.GRUCell(64,d)\n    def forward(self,x,adj):\n        m=self.msg(torch.cat([x[adj[0]],x[adj[1]]],-1))\n        agg=torch.zeros_like(x); agg.index_add_(0,adj[1],m)\n        return self.up(agg.reshape(-1,x.size(-1)),x.reshape(-1,x.size(-1))).view_as(x)\nclass Graff(nn.Module):\n    def __init__(self,blocks):\n        super().__init__(); self.h=nn.Parameter(torch.zeros(len(blocks),16));\n        self.gnn=GNN(16); self.W=nn.Linear(16,1)\n    def forward(self,curv,entr,budget,adj):\n        x=torch.cat([curv,entr,budget.expand_as(curv)],-1)\n        self.h=self.gnn(self.h+x,adj)\n        eta=torch.exp(self.W(self.h.mean(0)))\n        m=torch.sigmoid(self.W(self.h)).squeeze()\n        return eta,m\n' expected_result='Dev EM @30 min: Fixed-LR 74 %, SALeLR 84 %, BATS 88.5 %, REACTOR 90.8 %, GRAFF 92.4 ± 0.2 %.\nEnergy usage to reach 90 % EM: REACTOR 1.00 ×, GRAFF 0.83 ×.\nAUC improves 22 % over REACTOR.\nZero-shot on Mistral-7B: +1.8 pp EM vs REACTOR with same W.' expected_conclusion='By replacing high-variance RL with a differentiable graph-recurrent encoder/decoder conditioned on remaining wall-clock budget, GRAFF learns an end-to-end compute-aware LR trajectory in closed form.  It exploits cheap early-layer entropy to pre-scale gradients, respects global time constraints, and transfers across model sizes, achieving superior EM per Joule without external solvers or policy-gradient tuning.'

# Current Research Method
We propose GRAFF (Graph-Recurrent  Allocation  of  Fine-tuning  Funds) – a budget-aware, differentiable LR planner that learns an *entire trajectory* in closed form using one extra forward pass per mini-batch.

A.  Budget-conditioned trajectory encoder
   • Maintain a latent state h_t updated by a lightweight Graph Neural Net whose nodes are adapter blocks and whose edge features are instantaneous curvature Δλᵢ and entropy Hᵢ after the *first* adapter layer.  This runs in the same forward pass as the loss – no HVPs.
   • Remaining budget b_t = B – time_so_far is appended to every node feature, letting the network reason about *global* constraints.

B.  Closed-form LR decoder
   • Decoder is a single linear map W applied to h_t that outputs log η_t and token multipliers m_{t,i}.  Because W is linear, gradients wrt W are available from the normal back-prop of the loss; no REINFORCE needed.
   • To prevent instability we project W after every update onto the half-space defined by η_t·λ̂_max≤1.9 (edge-of-stability guard).

C.  Self-supervised training objective
   • We derive the *compute-normalised regret* R_T = (ℒ_T – ℒ_best)/time_T where ℒ_T is running avg loss.  Its gradient wrt W is tractable because both numerator and denominator depend smoothly on W through η_t.
   • We optimise W with Adam (lr 1e-3) concurrently with model weights: two optimisers, one backward.

D.  Zero-shot transfer & warm start
   • Pre-train W for 300 steps on GSM8K with tiny (70 M) Qwen-tiny; store weights.  When fine-tuning a new model we warm-start W and keep “outer” optimiser active –  no policy search from scratch.

Implementation is <160 LoC PyTorch; adds one Jacobian-vector product (auto-diff) per step (≈2 % run-time).

# MODEL LIST
{
    "Llama-4-Scout-17B-16E": {
        "model_parameters": {
            "total_parameters": "109b",
            "active_parameters": "17b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "Llama-4-Maverick-17B-128E": {
        "model_parameters": {
            "total_parameters": "400b",
            "active_parameters": "17b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "Qwen3-0.6B": {
        "model_parameters": "0.6b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-1.7B": {
        "model_parameters": "1.7b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-1.7B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-4B": {
        "model_parameters": "4b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-4B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-8B": {
        "model_parameters": "8b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-8B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-14B": {
        "model_parameters": "14b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-14B",
        "language_distribution": "",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-32B": {
        "model_parameters": "32.8b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-32B",
        "language_distribution": "",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "DeepSeek-v3": {
        "model_parameters": {
            "total_parameters": "671b",
            "active_parameters": "37b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n    title={DeepSeek-V3 Technical Report},\n    author={DeepSeek-AI},\n    year={2024},\n    eprint={2412.19437},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2412.19437},\n}"
    },
    "DeepSeek-V3.1": {
        "model_parameters": {
            "total_parameters": "671B",
            "active_parameters": "37B"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "Text"
        ],
        "output_modalities": [
            "Text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n    title={DeepSeek-V3 Technical Report},\n    author={DeepSeek-AI},\n    year={2024},\n    eprint={2412.19437},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2412.19437},\n}"
    },
    "DeepSeek-V3.2-Exp": {
        "model_parameters": {
            "total_parameters": "671B",
            "active_parameters": "37B"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "Text"
        ],
        "output_modalities": [
            "Text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv32,\n    title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\n    author={DeepSeek-AI},\n    year={2025},\n}"
    },
    "gpt-oss-20b": {
        "model_parameters": {
            "total_parameters": "21b",
            "active_parameters": "3.6b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/openai/gpt-oss-20b",
        "context_length": "",
        "language_distribution": "multilingual",
        "input_modalities": "text",
        "output_modalities": "text",
        "dependent packages": [
            "accelerate",
            "transformers",
            "kernels"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@misc{openai2025gptoss120bgptoss20bmodel,\n    title={gpt-oss-120b & gpt-oss-20b Model Card},\n    author={OpenAI},\n    year={2025},\n    eprint={2508.10925},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2508.10925},\n}"
    },
    "gemma-3-1b-it": {
        "model_parameters": "1b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-1b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    },
    "gemma-3-4b-it": {
        "model_parameters": "4b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-4b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    },
    "gemma-3-27b-it": {
        "model_parameters": "27b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-27b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    }
}

# DATASET LIST
{
    "alpaca-cleaned": {
        "discription": "",
        "num_training_samples": "",
        "num_validation_samples": "",
        "huggingface_url": "https://huggingface.co/datasets/yahma/alpaca-cleaned",
        "language_distribution": "",
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "databricks-dolly-15k": "",
    "gsm8k": {
        "discription": "A dataset of elementary school math word problems requiring 2 to 8 steps to solve",
        "num_training_samples": 7473,
        "num_validation_samples": 1319,
        "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
        "language_distribution": "English",
        "dependent packages": [],
        "code": "",
        "citation": "@article{cobbe2021gsm8k,\ntitle={Training Verifiers to Solve Math Word Problems},\nauthor={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\njournal={arXiv preprint arXiv:2110.14168},\nyear={2021}\n}"
    },
    "MATH": {
        "discription": "The MATH dataset consists of approximately 12,500 mathematics problems ranging from middle school to early university level. Each problem includes a natural language question, a detailed step-by-step solution, and a final answer, and it is widely used to evaluate large language models (LLMs) in terms of their abilities in mathematical reasoning, logical inference, and step-by-step problem solving.",
        "num_training_samples": 12500,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/qwedsacf/competition_math",
        "language_distribution": "English",
        "dependent packages": [],
        "code": "",
        "example": "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n'level': 'Level 1',\n'type': 'Counting & Probability',\n'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\frac{5}{12}+\\frac{1}{3}+x$, from which we have $x=\\boxed{\\frac{1}{4}}$.'}",
        "data_structure": "A data instance consists of a competition math problem and its step-by-step solution written in LaTeX and natural language. The step-by-step solution contains the final answer enclosed in LaTeX's \boxed tag.\n- problem: The competition math problem.\n- solution: The step-by-step solution.\n- level: The problem's difficulty level from 'Level 1' to 'Level 5', where a subject's easiest problems for humans are assigned to 'Level 1' and a subject's hardest problems are assigned to 'Level 5'.\n- type: The subject of the problem: Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra and Precalculus.",
        "citation": "@article{hendrycksmath2021,\ntitle={Measuring Mathematical Problem Solving With the MATH Dataset},\nauthor={Dan Hendrycks\nand Collin Burns\nand Saurav Kadavath\nand Akul Arora\nand Steven Basart\nand Eric Tang\nand Dawn Song\nand Jacob Steinhardt},\njournal={arXiv preprint arXiv:2103.03874},\nyear={2021}\n}"
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
  - The primary metric specified in the hypothesis (Exact-match accuracy at 30 min wall-clock.) MUST be included in this list.
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- proposed_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - Output a list of objects, where each object contains "name" (hyperparameter name) and "range" (search range).
  - For example: [{"name": "learning_rate", "range": "0.001-0.01"}, {"name": "batch_size", "range": "16,32,64"}, {"name": "weight_decay", "range": "0.0001-0.001"}]
  - Search ranges can be expressed as ranges (e.g., "0.001-0.01") or discrete values (e.g., "16,32,64").
Output:
{
    "experiment_summary": "This experiment evaluates GRAFF—our new, differentiable, budget-aware learning-rate (LR) planner—against a strong token-aware RL baseline (REACTOR) when LoRA-fine-tuning a 0.6-billion-parameter Qwen3 model on the GSM8K mathematics dataset.  \n\nWorkflow\n1. Data loading: GSM8K train/dev is streamed with length-bucketing (batch_size = 64) and mixed-precision (bfloat16).  \n2. Model: Qwen3-0.6B is equipped with LoRA adapters (rank = 64, α = 16) on all self-attention and MLP layers; only adapter weights are trainable.  \n3. GRAFF module: each mini-batch’s forward pass collects two cheap probes per adapter block—logit entropy after the first adapter layer and Hessian-free curvature estimate Δλᵢ.  Node features = [Δλᵢ, Hᵢ, remaining_budget].  A 2-layer message-passing GNN updates the latent state h_t, after which a single linear decoder W outputs (a) global LR η_t for the optimiser and (b) per-token multipliers m_{t,i}.  W is projected after every update to satisfy η_t·λ̂_max ≤ 1.9.  \n4. Joint optimisation: the normal model loss plus the compute-normalised regret R_T is back-propagated once; Adam (lr = 2e-4) updates adapter weights while a second Adam (lr = 1e-3) updates W.  \n5. Budget tracking: wall-clock time is monitored with CUDA events; once 30-min or 60-min budgets are exhausted, training halts and dev exact-match (EM) is recorded.  \n6. Baseline: REACTOR replaces GRAFF but keeps all other settings identical.  \n7. Hyper-parameter search over three key knobs (see below) is done with 50 trials of Optuna using the 30-min EM as the objective.  \n8. Metrics collected: dev EM @30 min (primary), dev EM @60 min, AUC(EM-vs-time)/B, and energy (Joules) read every 5 s from nvidia-smi.  \nThe entire pipeline fits comfortably into an 80 GB A100 (peak ≈34 GB) and <10 GB system RAM per worker; RAM-heavy dataloading is disabled by streaming.",
    "evaluation_metrics": [
        "Exact-match accuracy at 30 min wall-clock",
        "Exact-match accuracy at 60 min wall-clock",
        "AUC(EM-vs-time)/B",
        "Energy (Joules)"
    ],
    "models_to_use": [
        "Qwen3-0.6B"
    ],
    "datasets_to_use": [
        "gsm8k"
    ],
    "proposed_method": "GRAFF (Graph-Recurrent Allocation of Fine-tuning Funds) is a learning-rate scheduler that plans an entire training trajectory in closed form while respecting a user-supplied wall-clock budget B.\n\nKey components\n1. Budget-conditioned encoder: Each adapter block is a node in a lightweight graph neural network.  Node features include instantaneous curvature (Δλᵢ) estimated via a single Hessian-vector product, token-level entropy (Hᵢ) after the first adapter layer, and the remaining budget b_t = B – time_so_far.  One message-passing step updates latent states h_t.\n2. Linear decoder: A single weight matrix W maps the mean of h_t to log η_t (global LR) and each h_t to token multipliers m_{t,i}.  Because the decoder is linear, gradients wrt W flow through the normal loss; no REINFORCE or separate controller optimisation is needed.\n3. Stability guard: After every optimiser step, W is projected so that η_t·λ̂_max ≤ 1.9, keeping updates near the edge of stability.\n4. Objective: Compute-normalised regret R_T = (ℒ_T – ℒ_best)/time_T is differentiable in W through η_t, allowing simultaneous optimisation of adapter weights and scheduler parameters with standard back-prop.\n5. Warm-start: W is pre-trained for 300 mini-batches on a 70 M-parameter Qwen-tiny model; the same W is then continuously fine-tuned online when larger models are trained, enabling zero-shot transfer across sizes.\n\nAlgorithmic procedure per mini-batch\n(1) Forward pass through model and probes → collect Δλᵢ, Hᵢ.\n(2) GNN update → h_t.\n(3) Decode (η_t, m_{t,i}); scale optimiser LR and token gradients.\n(4) Backward pass; accumulate gradients of both adapter weights and W.\n(5) Adam steps for both parameter sets; project W; log time & energy.",
    "comparative_methods": [
        "REACTOR (RL-based LR scheduler)"
    ],
    "hyperparameters_to_search": [
        {
            "name": "adapter_lr_init",
            "range": "1e-4-5e-4"
        },
        {
            "name": "graff_outer_lr",
            "range": "5e-4-2e-3"
        },
        {
            "name": "LoRA_rank",
            "range": "32,64,96"
        }
    ]
}
