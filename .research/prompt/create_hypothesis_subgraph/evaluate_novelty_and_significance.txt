
LLM Name: o3-2025-04-16
Input:
You are an accomplished researcher in machine learning. You are considering a new hypothesis described in "New Hypothesis" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new hypothesis.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed hypothesis, and quantitatively evaluate them.

# Research Topic
Learning rate optimization for fine-tuning Qwen3-0.6B on GSM8K elementary math problems

# New Hypothesis
{
    "Open Problems": "A.  Even within the same GSM8K prompt the *information gain* of successive tokens varies by two orders of magnitude: early chain-of-thought (CoT) tokens specify reasoning context while the final answer token determines EM.  Existing LR tuners—BATS included—look only at local sharpness and therefore down-weight important but ‘flat’ answer tokens, hurting EM.\nB.  The true optimisation goal in budget–constrained PEFT is not “minimise average loss” but “maximise evaluation-time exact-match given ≤ T GPU seconds”.  No current scheduler links LR decisions to *future validation reward per unit compute*.\nC.  Validation reward can be estimated cheaply by self-consistency: with LoRA adapters at 0 .6 B it costs <20 ms to decode k=4 CoT samples per mini-batch.  This rich signal is ignored by curvature-based tuners.\nD.  Practical adoption of sophisticated controllers is limited by the need to install CVXPY or custom kernels; a pure-PyTorch, differentiable alternative is desirable.",
    "Methods": "We propose REACTOR (REward-And-Curvature-TrajectOry tRainer), a *differentiable*, two-head network that learns an LR policy online.\n\n1.  Inputs per optimisation step t\n   •  ϕₜ = [λ̂ₜ , σ̂ₜ , ΔAccₜ₋₁ , ΔComputeₜ₋₁] where λ̂ is max-Hessian on adapter, σ̂ grad noise, ΔAcc the self-consistency EM gain in last M steps, ΔCompute wall-time.\n   •  ψᵢ token features: logit variance vᵢ and ‘answer flag’ aᵢ (1 if token index = last).\n\n2.  Architecture\n   •  Token head h_token: a one-layer MLP mapping (vᵢ , aᵢ) → mᵢ ∈ (0.2,1.5).  Produces per-token LR multipliers as in BATS but is *trained online* via REINFORCE using future EM.\n   •  Global head h_global: a GRU(ϕ) followed by two linear outputs giving mean μₜ and log-scale ρₜ of a LogNormal distribution over scalar LR ηₜ.\n\n3.  Control loop (every step)\n   a) Sample ηₜ ~ LogNormal(μₜ , ρₜ); set effective LR ηᵢ = ηₜ·mᵢ.\n   b) Apply adapter update (AdamW).\n   c) Every k=25 steps decode k_sc=4 CoT self-consistent answers on 64-sample dev shard; obtain reward rₜ = (EM – target)/ΔCompute.\n   d) Compute policy gradients ∇_{θ_h} log p(ηₜ , mᵢ)·(rₜ – b) with running baseline b and update h_token , h_global with Adam (lr 1e-3) *outside* model graph ⇒ <4 % overhead.\n\n4.  Safety filter: if predicted ηₜ λ̂ₜ > 1.9 clip to edge-of-stability as in SALeLR.\n\n5.  Implementation: 180 LoC pure PyTorch; no CVXPY, no JIT.\n",
    "Experimental Setup": "Models: Qwen3-0.6B-Chat + LoRA (r=64, nf4).  Cross-model generalisation: Mistral-7B-Instruct.\nData: GSM8K train/dev; GSM1k for contamination check.\nBaselines (3 seeds): Fixed-LR, AutoLRS, SALeLR, BATS.\nCompute budgets: T∈{45 min, 90 min} on single A100-80GB.\nMetrics: 1) dev EM at wall-time T; 2) AUC of EM vs time; 3) tokens whose LR multipliers mᵢ>1 among answer tokens (alignment metric).",
    "Primary Metric": "Exact-match accuracy on GSM8K dev at 45 minutes total training time.",
    "Experimental Code": "# REACTOR – skeleton\nclass Reactor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gru = nn.GRU(input_size=4, hidden_size=32)\n        self.mu   = nn.Linear(32,1)\n        self.logr = nn.Linear(32,1)\n        self.token = nn.Sequential(nn.Linear(2,16), nn.SiLU(), nn.Linear(16,1))\n    def forward(self, phi, token_feats):\n        h,_ = self.gru(phi.unsqueeze(0))\n        mu, log_r = self.mu(h[-1]), self.logr(h[-1])\n        m = torch.sigmoid(self.token(token_feats))*1.3+0.2\n        return mu.squeeze(), log_r.squeeze(), m.squeeze()\n\nreactor = Reactor().to(device)\nopt_reactor = torch.optim.Adam(reactor.parameters(),1e-3)\n\nfor step,(x,y,ans_flag) in enumerate(loader):\n    out = model(x)\n    v = out.logits.var(dim=-1, unbiased=False)\n    token_feats = torch.stack([v, ans_flag],dim=-1)\n    mu, log_r, m = reactor(phi_t, token_feats)\n    eta = torch.distributions.LogNormal(mu,log_r.exp()).sample()\n    eta = clip_edge(eta, lam_t)\n    scaled_loss = (F.cross_entropy(out.logits,y,reduction='none')*m).mean()\n    scaled_loss.backward()\n    apply_lora_update(eta)\n    if step%k_eval==0:\n        em, dt = self_consistency_eval(model)\n        r = (em-target)/dt;  b = 0.9*b+0.1*r\n        pg = -log_prob(eta,m)*(r-b)\n        opt_reactor.zero_grad(); pg.backward(); opt_reactor.step()",
    "Expected Result": "At 45 min budget: Fixed 74 %, AutoLRS 80 %, SALeLR 85 %, BATS 88.5 %, REACTOR 91.0 ± 0.3 %.\nArea-under-curve EM-vs-time improves by 18 % over BATS.\nAnswer-token up-weighting (median m_ans): 1.00→1.14→1.32 (REACTOR).\nGeneralises zero-shot to Mistral-7B with +1.5 pp EM over BATS.",
    "Expected Conclusion": "By turning LR selection into an online reinforcement-learning problem that maximises evaluation reward per unit compute, REACTOR learns to allocate larger step sizes specifically to reward-critical answer tokens while globally steering the optimiser along safe, curvature-aware trajectories.  This drives faster EM gains under realistic GPU budgets and removes the need for hand-crafted MPC or QP solvers, delivering a practical, purely PyTorch solution for compute-aware fine-tuning of LLMs."
}

# Related Works
{
    "Title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "Main Contributions": "QLORA is an efficient finetuning approach that significantly reduces memory usage, enabling finetuning of a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. It backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The resulting Guanaco model family outperforms all previous openly released models on the Vicuna benchmark, achieving 99.3% of ChatGPT's performance level with only 24 hours of finetuning on a single GPU. QLORA introduces 4-bit NormalFloat (NF4), Double Quantization (DQ), and Paged Optimizers to achieve these memory savings without sacrificing performance.",
    "Methodology": "QLORA builds upon Low-rank Adapters (LoRA) by quantizing the pretrained base model to 4-bit precision. It uses a novel 4-bit NormalFloat (NF4) data type, which is information-theoretically optimal for normally distributed weights. To further reduce memory, Double Quantization (DQ) is introduced, which quantizes the quantization constants themselves, saving approximately 0.37 bits per parameter. Paged Optimizers are employed using NVIDIA unified memory to manage memory spikes during gradient checkpointing, preventing out-of-memory errors for large models. During finetuning, weights are stored in 4-bit NF4 but dequantized to BFloat16 for forward and backward passes, with gradients only computed and applied to the 16-bit LoRA parameters. Adapters are applied at every network layer to recover full 16-bit precision performance.",
    "Experimental Setup": "The research evaluated QLoRA across various models (RoBERTa-large, T5 models from 80M to 11B, LLaMA models from 7B to 65B, OPT, BLOOM, Pythia) and instruction datasets (GLUE, Super-NaturalInstructions, Flan v2, Alpaca, OASST1, HH-RLHF, Self-Instruct, Unnatural Instructions, Longform, Chip2). Performance was assessed using MMLU 5-shot accuracy, perplexity on Pile Common Crawl, RougeL for generation tasks, and a novel chatbot evaluation using tournament-style benchmarking. Chatbot performance was judged by both GPT-4 and human annotators, aggregated into Elo scores on the Vicuna and OA benchmarks. Hyperparameter searches were conducted for LoRA dropout, 'r' dimension, and layer placement, finding LoRA on all linear transformer block layers critical. Bias evaluation was performed on the CrowS dataset.",
    "Limitations": "The study did not conclusively establish that QLORA can match full 16-bit finetuning performance at 33B and 65B scales due to immense resource costs. The evaluation of instruction finetuning models was limited to MMLU, Vicuna, and OA benchmarks, without generalization to others like BigBench, RAFT, or HELM. The paper highlights that current chatbot benchmarks are not entirely trustworthy and that strong MMLU performance does not always imply strong Vicuna chatbot performance, and vice versa. Responsible AI evaluation was limited to a specific bias test (CrowS dataset) and did not cover other types of biases. The research also did not evaluate different bit-precisions (e.g., 3-bit base models) or other Parameter Efficient FineTuning (PEFT) methods besides LoRA.",
    "Future Research Directions": "Future work should aim to measure and characterize the circumstances under which slowdowns occur from the paging process of Paged Optimizers. Further exploration of the performance-precision trade-off for QLoRA tuning is needed to determine the exact limits. Investigating the degree to which multilingual training improves performance on non-English instructions and its impact on benchmarks like the OA benchmark is suggested. A deeper study into the trade-offs between simple cross-entropy loss and Reinforcement Learning from Human Feedback (RLHF) training, enabled by QLORA's efficiency, is also proposed. Further evaluation of analyzing biases in Guanaco and similar chatbots beyond the initial CrowS dataset test is necessary. Exploring other PEFT approaches or more aggressive quantization schemes (e.g., 3-bit GPTQ quantization with LoRA) to potentially yield even better performance-memory trade-offs is a promising avenue.",
    "Experiment Code": "def get_accelerate_model(args, checkpoint_dir):\n    # ... (memory and device setup omitted for brevity)\n\n    print(f'loading base model {args.model_name_or_path}...')\n    compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        load_in_4bit=args.bits == 4,\n        load_in_8bit=args.bits == 8,\n        device_map=device_map,\n        max_memory=max_memory,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.bits == 4,\n            load_in_8bit=args.bits == 8,\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=args.double_quant,\n            bnb_4bit_quant_type=args.quant_type,\n        ),\n        torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),\n        trust_remote_code=args.trust_remote_code,\n        use_auth_token=args.use_auth_token\n    )\n\n    # ... (tokenizer setup omitted for brevity)\n\n    if not args.full_finetune:\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n\n    if not args.full_finetune:\n        if checkpoint_dir is not None:\n            print(\"Loading adapters from checkpoint.\")\n            model = PeftModel.from_pretrained(model, join(checkpoint_dir, 'adapter_model'), is_trainable=True)\n        else:\n            print(f'adding LoRA modules...')\n            modules = find_all_linear_names(args, model)\n            config = LoraConfig(\n                r=args.lora_r,\n                lora_alpha=args.lora_alpha,\n                target_modules=modules,\n                lora_dropout=args.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            model = get_peft_model(model, config)\n\n    for name, module in model.named_modules():\n        if isinstance(module, LoraLayer):\n            if args.bf16:\n                module = module.to(torch.bfloat16)\n        if 'norm' in name:\n            module = module.to(torch.float32)\n        if 'lm_head' in name or 'embed_tokens' in name:\n            if hasattr(module, 'weight'):\n                if args.bf16 and module.weight.dtype == torch.float32:\n                    module = module.to(torch.bfloat16)\n    return model, tokenizer",
    "Experiment Result": "The model utilizes 4-bit quantization (`bits=4`) with the `nf4` quantization type (`quant_type=\"nf4\"`). Double Quantization (`double_quant=True`) is applied. The compute data type for 4-bit quantization is `torch.bfloat16` when supported by the GPU, otherwise `torch.float32` (or `torch.float16` if specified). LoRA adapters are configured with a rank `lora_r=64`, `lora_alpha=16`, and `lora_dropout=0.0`. The optimizer used is `paged_adamw_32bit` (`optim='paged_adamw_32bit'`) and `gradient_checkpointing=True` is enabled for memory efficiency. Training is performed with `per_device_train_batch_size=1` and `gradient_accumulation_steps=16`, implying an effective batch size of 16. The maximum memory per GPU is set to `max_memory_MB=80000`."
}{
    "Title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "Main Contributions": "QLORA is an efficient finetuning method enabling the finetuning of 65B parameter models on a single 48GB GPU while preserving full 16-bit performance. It backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Key innovations include: (a) 4-bit NormalFloat (NF4), an information-theoretically optimal data type for normally distributed weights; (b) Double Quantization, which quantizes quantization constants for further memory reduction; and (c) Paged Optimizers, utilizing NVIDIA unified memory to manage memory spikes. The resulting Guanaco model family achieves state-of-the-art results on the Vicuna benchmark, reaching 99.3% of ChatGPT's performance. The research also includes an extensive analysis of instruction following and chatbot performance across various datasets and model scales, finding data quality is more important than dataset size and that GPT-4 evaluations can be a reasonable, cheaper alternative to human evaluation.",
    "Methodology": "QLORA employs a novel high-precision 4-bit quantization technique combined with Low-rank Adapters (LoRA). The base pretrained model is quantized to 4-bit using NormalFloat (NF4), an asymmetric, information-theoretically optimal data type for zero-mean normal distributions. Quantization constants are further compressed via Double Quantization (DQ), which quantizes the initial 32-bit constants into 8-bit floats. Paged Optimizers, leveraging NVIDIA unified memory, prevent out-of-memory errors from gradient checkpointing memory spikes by automatically paging optimizer states between GPU and CPU RAM. During finetuning, the 4-bit NormalFloat weights are dequantized to BFloat16 for computation in both forward and backward passes, but gradients are only calculated and applied to the 16-bit LoRA parameters. LoRA adapters are applied to all linear transformer block layers to recover full 16-bit precision performance.",
    "Experimental Setup": "The study evaluated QLoRA across encoder (RoBERTa-large), encoder-decoder (T5-80M to 11B), and decoder-only (LLaMA 7B to 65B, OPT, BLOOM, Pythia 125M to 13B) architectures. Finetuning datasets included GLUE, Super-NaturalInstructions, Flan v2, Alpaca, OASST1, HH-RLHF, Self-Instruct, Unnatural Instructions, Longform, and Chip2. Performance was assessed using GLUE (accuracy), Super-NaturalInstructions (RougeL), 5-shot MMLU (accuracy), Pile Common Crawl (perplexity), and custom chatbot benchmarks (Vicuna prompts, OASST1 validation dataset). Evaluation methods involved GPT-4 ratings (scoring, pairwise comparisons, Elo ratings), human evaluations (Amazon Mechanical Turk, pairwise comparisons, Elo ratings), and qualitative analysis of model generations. Hyperparameter searches were conducted for LoRA configurations, learning rates, and batch sizes, with specific settings provided for different model scales and datasets. Ablation studies investigated the impact of training on source+target vs. target only and the relationship between dataset size and quality.",
    "Limitations": "The paper acknowledges several limitations: QLoRA's ability to match full 16-bit finetuning performance at 33B and 65B scales was not definitively established due to immense resource costs. The evaluation of instruction finetuning models was limited to MMLU, Vicuna, and OA benchmarks, with generalization to other benchmarks like BigBench, RAFT, and HELM not ensured. The performance on benchmarks was found to be highly dependent on the similarity between finetuning data and the benchmark dataset. Responsible AI evaluation was limited, primarily using the CrowS dataset, with a need for further assessment of other bias types. Different bit-precisions (e.g., 3-bit base models) and other Parameter-Efficient FineTuning (PEFT) methods beyond LoRA were not evaluated. Challenges in evaluation include moderate agreement among human annotators, subjective preferences in judging strong systems, and noticeable biases in automated evaluation systems like GPT-4 (e.g., order effects, self-preference). Qualitatively, Guanaco shows unreliability in factual recall for obscure questions, occasional refusal to follow simple instructions, and susceptibility to trickery for secret keeping, along with weaknesses in complex mathematics and unreliable Theory of Mind inferences.",
    "Future Research Directions": "Future work could explore the precise performance-precision trade-off for QLoRA tuning. Further research is needed to measure and characterize the circumstances under which slow-downs occur from the paging process of Paged Optimizers. Investigations into approaches to mitigate issues related to subjective preferences in human evaluation protocols and biases in automated evaluation systems are suggested. The impact of multilingual training on performance in non-English languages warrants further study. Research could also delve into the trade-offs between simple cross-entropy loss and Reinforcement Learning from Human Feedback (RLHF) training enabled by QLoRA's efficiency. Exploring the effectiveness and scalability of other Parameter-Efficient FineTuning (PEFT) methods with QLoRA is another direction. Finally, the potential of more aggressive quantization, such as 3-bit GPTQ with LoRA, to achieve 16-bit performance post-finetuning, and more extensive responsible AI evaluations of biases in chatbots, are identified as promising avenues.",
    "Experiment Code": "import torch\nimport transformers\nimport bitsandbytes as bnb\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\nfrom peft.tuners.lora import LoraLayer\n\ndef find_all_linear_names(args, model):\n    cls = bnb.nn.Linear4bit if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\ndef get_accelerate_model(args, checkpoint_dir):\n    # ... (device_map and max_memory setup)\n    compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        load_in_4bit=args.bits == 4,\n        load_in_8bit=args.bits == 8,\n        device_map=device_map,\n        max_memory=max_memory,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=args.bits == 4,\n            load_in_8bit=args.bits == 8,\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=args.double_quant,\n            bnb_4bit_quant_type=args.quant_type,\n        ),\n        torch_dtype=(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),\n        trust_remote_code=args.trust_remote_code,\n        use_auth_token=args.use_auth_token\n    )\n    # ... (tokenizer setup)\n\n    if not args.full_finetune:\n        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n\n    if not args.full_finetune:\n        if checkpoint_dir is not None:\n            model = PeftModel.from_pretrained(model, join(checkpoint_dir, 'adapter_model'), is_trainable=True)\n        else:\n            modules = find_all_linear_names(args, model)\n            config = LoraConfig(\n                r=args.lora_r,\n                lora_alpha=args.lora_alpha,\n                target_modules=modules,\n                lora_dropout=args.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            model = get_peft_model(model, config)\n\n    for name, module in model.named_modules():\n        if isinstance(module, LoraLayer):\n            if args.bf16:\n                module = module.to(torch.bfloat16)\n        if 'norm' in name:\n            module = module.to(torch.float32)\n        if 'lm_head' in name or 'embed_tokens' in name:\n            if hasattr(module, 'weight'):\n                if args.bf16 and module.weight.dtype == torch.float32:\n                    module = module.to(torch.bfloat16)\n    return model, tokenizer\n\n# Excerpt from the train function showing trainer initialization\n# trainer = Seq2SeqTrainer(\n#     model=model,\n#     tokenizer=tokenizer,\n#     args=training_args, # training_args contains the optim='paged_adamw_32bit'\n#     **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n# )",
    "Experiment Result": "The QLORA method is implemented in `qlora.py` for training large language models. The key experimental settings are:\n\n**Quantization:**\n- **Bits:** 4 bits (`args.bits = 4`)\n- **Quantization Type:** NormalFloat (NF4) (`args.quant_type = 'nf4'`)\n- **Double Quantization (DQ):** Enabled (`args.double_quant = True`)\n- **Computation Data Type:** BFloat16 (`bnb_4bit_compute_dtype=torch.bfloat16`)\n- **Model Loading:** `AutoModelForCausalLM.from_pretrained` is used with a `BitsAndBytesConfig` instance to apply 4-bit quantization, NF4, double quantization, and BFloat16 compute dtype.\n\n**Low-Rank Adapters (LoRA):**\n- **LoRA R Dimension:** 64 (`args.lora_r = 64`)\n- **LoRA Alpha:** 16 (`args.lora_alpha = 16`)\n- **LoRA Dropout:** 0.0 (`args.lora_dropout = 0.0`)\n- **Target Modules:** LoRA adapters are applied to all linear transformer block layers, identified dynamically by `find_all_linear_names`.\n- **Weight Precision:** LoRA parameters are kept at a higher precision (BFloat16 if `args.bf16` is true) while base model weights are 4-bit quantized.\n\n**Optimizers and Memory Management:**\n- **Optimizer:** Paged AdamW 32-bit (`args.optim = 'paged_adamw_32bit'`) is used to manage optimizer states, preventing out-of-memory errors.\n- **Gradient Checkpointing:** Enabled (`args.gradient_checkpointing = True`) to further reduce memory consumption during training.\n- **Max Memory per GPU:** Defaults to 80000 MB.\n\n**Training Parameters (default values from `TrainingArguments`):**\n- **Per Device Training Batch Size:** 1\n- **Gradient Accumulation Steps:** 16\n- **Maximum Training Steps:** 10000\n- **Learning Rate:** 0.0002\n- **Maximum Gradient Norm:** 0.3 (for gradient clipping)\n- **Learning Rate Scheduler:** Constant (`lr_scheduler_type='constant'`)\n- **Warmup Ratio:** 0.03\n- **Logging Steps:** 10\n- **Save Strategy:** 'steps'\n- **Save Steps:** 250\n- **Save Total Limit:** 40\n- **Base Model:** `EleutherAI/pythia-12b` (default `model_name_or_path`)\n- **Dataset:** 'alpaca' (default `dataset`)"
}{
    "Title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
    "Main Contributions": "The paper proposes Quantization-Aware Low-Rank Adaptation (QA-LoRA) to address the heavy computational burden of Large Language Models (LLMs), especially for deployment on edge devices. Its main contributions are: (i) it enables fine-tuning of LLMs with quantized weights (e.g., INT4), significantly reducing time and memory usage during the process; (ii) it allows the LLM and auxiliary weights to be naturally integrated into a fully quantized model after fine-tuning, without loss of accuracy and without needing a separate post-training quantization (PTQ) step; and (iii) it consistently outperforms QLoRA with PTQ and is on par with QLoRA without PTQ, while offering much faster inference speeds due to maintaining the quantized state.",
    "Methodology": "QA-LoRA builds upon Low-Rank Adaptation (LoRA) and min-max quantization, addressing the 'imbalanced degrees of freedom' between quantization and adaptation. The core idea is the introduction of group-wise operators: (i) for quantization, each column of the pre-trained weight matrix is partitioned into 'L' groups, and each group is quantized independently with its own scaling and zero factors (αl,j and βl,j), increasing the degree of freedom for low-bit quantization; (ii) for adaptation, the input vector 'x' is aggregated (summed) within each group using an 'nn.AvgPool1d' operation, reducing its dimension from Din to L. This allows the LoRA matrix A to be smaller (L × Dint instead of Din × Dint), effectively decreasing the degrees of freedom for adaptation. This balanced approach ensures that the quantized base weights (˜W) and the adapted low-rank weights (s·AB) can be merged into a single, fully quantized model (W′) without converting back to high-precision numbers like FP16, and without incurring accuracy loss from PTQ. The merging process involves updating the zero-point matrix B to B − sL1L2 ⊘ A.",
    "Experimental Setup": "The effectiveness of QA-LoRA was validated on the LLaMA (7B, 13B, 33B, 65B) and LLaMA2 (7B, 13B) model families. Evaluation metrics included 0-shot and 5-shot accuracy on the Massively Multitask Language Understanding (MMLU) benchmark and 0-shot common sense reasoning on tasks like HellaSwag, PIQA, WinoGrande, ARC, BoolQ, and OpenBookQA using lm-eval-harness. For quantization, GPTQ was adopted, specifically group-wise asymmetric quantization with a default group size of 32, act-order set to false, and true-sequential set to true, for INT2, INT3, and INT4 bit widths. Fine-tuning was performed on Alpaca (52K instruction-following data) and a 320K subset of FLAN v2. Training details included a paged AdamW optimizer, a maximum gradient norm of 0.3, a batch size of 16, and a constant learning rate (2e-5 for 7B/13B models, 1e-5 for 33B/65B models). Fine-tuning steps were 10K for Alpaca and 20K for FLAN v2. All experiments were conducted on Tesla V100 GPUs (1 GPU for 7B, 13B, 33B models, and 2 GPUs for 65B models). Comparisons were made against base LLaMA models, QLoRA (both original FP16 inference and QLoRA with GPTQ for quantized inference), and PEQA. Ablation studies explored the impact of quantization group size, various fine-tuning datasets (Self-instruct, Longform, Chip2), and the size of fine-tuning datasets.",
    "Limitations": "The paper primarily highlights how QA-LoRA overcomes the limitations of existing methods, rather than explicitly detailing weaknesses or constraints of QA-LoRA itself. However, implicit limitations include: (i) like all quantization methods, it involves an approximation of weights, which carries an inherent risk of accuracy degradation, though QA-LoRA aims to minimize this; (ii) the effectiveness, particularly for low-bit quantization, relies on selecting an appropriate group size (L), indicating a hyperparameter sensitivity, although the paper demonstrates that a 'moderate L' achieves satisfying accuracy; and (iii) for optimal performance in low-bit scenarios, sufficient fine-tuning data is required, as 'low-bit quantization asks for more data,' presenting a potential constraint on data availability.",
    "Future Research Directions": "Not explicitly mentioned in the paper. The conclusion summarizes the current work and its benefits without detailing future research paths.",
    "Experiment Code": "The provided repository content (`generate_instruction.py`, `train.py`, `utils.py`, `weight_diff.py`) does not contain direct implementation sections related to the QA-LoRA method described. The `train.py` script outlines a standard fine-tuning process for causal language models using Hugging Face Transformers, but it lacks specific code for group-wise quantization, `nn.AvgPool1d` for adaptation, or the unique merging strategy of QA-LoRA (e.g., modifying LoRA layers or implementing min-max quantization details). The `weight_diff.py` deals with full model weight differences, not low-rank adaptations or quantized components.",
    "Experiment Result": "The repository content does not provide specific experimental settings pertinent to the QA-LoRA method, such as group sizes for quantization (L), quantization bit-widths, specific LoRA ranks, or parameters for the unique merging process (e.g., the zero-point matrix update B − sL1L2 ⊘ A). The `TrainingArguments` in `train.py` define general training parameters like `model_name_or_path` (default \"facebook/opt-125m\"), `data_path`, `optim` (default \"adamw_torch\"), `model_max_length` (default 512), and `cache_dir`, which are standard for fine-tuning but not unique to QA-LoRA's methodology."
}{
    "Title": "QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation",
    "Main Contributions": "The paper introduces QuanTA (Quantum-informed Tensor Adaptation), a novel, easy-to-implement parameter-efficient fine-tuning (PEFT) method with no inference overhead for large language models (LLMs). QuanTA addresses the limitations of low-rank adaptation (LoRA) for complex downstream tasks by enabling efficient high-rank fine-tuning, leveraging quantum-inspired tensor operations. Theoretical support is provided through the universality theorem and the rank representation theorem, proving its ability to parameterize high-rank matrices. Experimentally, QuanTA demonstrates significant improvements in commonsense reasoning, arithmetic reasoning, and scalability, often outperforming traditional methods and other PEFT approaches with fewer trainable parameters.",
    "Methodology": "QuanTA is inspired by quantum circuit structures, parameterizing weight updates using tensor operations analogous to quantum gates. It reshapes the hidden vector of dimension `d` into a multi-dimensional tensor (e.g., `d1 x d2 x ... x dN`). The method consists of sequentially applying a collection of 'gates' (tensors `T(α)`) that operate on specific axes (e.g., two axes). For initialization, QuanTA uses two sets of tensors, `{T(α)}` (trainable) and `{S(α)}` (frozen), where `{T(α)}` is initialized to the same value as `{S(α)}`. The adapted layer is defined as `y = W0x + Tθx - Sx`, ensuring `y = W0x` at initialization, after which `W0 + S` can be merged into `W'0` to yield `y = W'0x + Tθx` with no inference overhead. The universality theorem proves that any arbitrary matrix can be decomposed into a finite sequence of QuanTA tensors, while the rank representation theorem characterizes the rank of the full QuanTA operator in terms of individual tensor ranks.",
    "Experimental Setup": "Experiments were conducted on various LLMs, including LLaMA2 (7B, 13B, 70B parameters), LLaMA (7B, 13B), LLaMA3 (8B), and RoBERTa. The datasets used for benchmarking include: the DROP dataset (reading comprehension), COMMONSENSE 170K (training for commonsense reasoning), and eight downstream commonsense reasoning tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-Easy, ARC-Challenge, OBQA). For arithmetic reasoning, MATH10K was used for training, with evaluation on AQuA, GSM8K, MAWPS, and SV AMP. Additional benchmarks were performed on five GLUE benchmark tasks (SST-2, MRPC, CoLA, RTE, STS-B) using the RoBERTa model. QuanTA was compared against full fine-tuning (FT), LoRA (with varying ranks), DoRA, VeRA, Prefix tuning, Series and Parallel Adapters, MoRA, LoRETTA, and KronA. Hyperparameters were optimized on validation sets created from the training data, and the best checkpoint (based on F1-score for DROP and Accuracy for reasoning tasks) was used for evaluation on the test sets. All experiments were averaged over multiple random seeds.",
    "Limitations": "Current limitations include potential underutilization of GPUs when tensors are too small due to the sequential application of tensors. The hyperparameters for QuanTA, such as the number of tensors applied on the same axes, have not yet been optimally tuned. The experimental evaluation focused primarily on the LLaMA model series, and a more thorough study across a wider range of LLMs would be beneficial if more computational resources become available.",
    "Future Research Directions": "Future research directions for QuanTA include expanding its application to a wider range of tasks and specialized domains to demonstrate versatility and robustness. Combining QuanTA with other PEFT methods or integrating it into ensemble models could further enhance performance, particularly for complex tasks. Exploring advanced optimization techniques specifically tailored for QuanTA could improve convergence rates and overall efficiency. Further design based on principles from quantum computing, such as entanglement and superposition, may lead to even more efficient fine-tuning methods.",
    "Experiment Code": "from dataclasses import asdict, dataclass, field\nfrom enum import Enum\nfrom math import prod\nfrom typing import List, Optional, Union\n\nimport opt_einsum as oe\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..utils import PeftConfig, PeftType\n\n@dataclass\nclass QuanTAConfig(PeftConfig):\n    d: int = field(default=1, metadata={\"help\": \"quanta number of dimensions\"})\n    quanta_dropout: float = field(default=0.0, metadata={\"help\": \"quanta dropout\"})\n    merge_weights: bool = field(default=False,\n                                metadata={\"help\": \"Merge weights of the original model and the Lora model\"})\n    fan_in_fan_out: bool = field(default=False,\n                                 metadata={\n                                     \"help\": \"Set this to True if the layer to replace stores weight like (fan_in, fan_out)\"}, )\n    per_dim_features: Optional[List[int]] = field(default=None, metadata={\n        \"help\": \"List of the number of features per dimension. If not provided, the features are equally divided.\"}, )\n    per_dim_features2: Optional[List[int]] = field(default=None, metadata={\n        \"help\": \"List of the number of features per dimension for the output. If not provided, the features are set to per_dim_features.\"}, )\n    sum_mode: bool = field(default=False, metadata={\"help\": \"Set this to True if the quanta is in sum mode\"})\n    initialize_mode: str = field(default=\"sum_opposite_freeze_one\",\n                                 metadata={\n                                     \"help\": \"Initialization mode for the quanta weights. Can be 'sum_opposite_freeze_one'\"})\n    bias: str = field(default=\"none\", metadata={\"help\": \"Bias type for Lora. Can be 'none', 'all' or 'lora_only'\"})\n    target_modules: Optional[Union[List[str], str]] = field(default=None,\n                                                            metadata={\n                                                                \"help\": \"List of module names or regex expression of the module names to replace with Lora.\"\n                                                                        \"For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$' \"}, )  # not sure if this is needed legacy support for now\n    enable_lora: Optional[List[bool]] = field(default=None, metadata={\n        \"help\": \"Used with `lora.MergedLinear`.\"})\n    tensor_rank: int = 5\n\n    def __post_init__(self):\n        self.peft_type = PeftType.QUANTA\n\n\nclass QuanTAModel(torch.nn.Module):\n\n    def __init__(self, config, model):\n        super().__init__()\n        self.peft_config = config\n        self.model = model\n        self._find_and_replace()\n        mark_lora_layernorm_cls_trainable(self.model, self.peft_config.task_type, self.peft_config.tensor_rank,\n                                          self.peft_config.bias)\n        self.forward = self.model.forward\n\n    def _find_and_replace(self):\n        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n        if loaded_in_8bit and not is_bnb_available():\n            raise ImportError(\"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"\n                              \"You can install it with `pip install bitsandbytes`.\")\n        is_target_modules_in_base_model = False\n        is_hf_device_map_available = hasattr(self.model, \"hf_device_map\")\n        kwargs = {\"d\": self.peft_config.d, \"per_dim_features\": self.peft_config.per_dim_features,\n                  \"per_dim_features2\": self.peft_config.per_dim_features2,\n                  \"quanta_dropout\": self.peft_config.quanta_dropout,\n                  \"fan_in_fan_out\": self.peft_config.fan_in_fan_out,\n                  \"merge_weights\": (self.peft_config.merge_weights or self.peft_config.inference_mode),\n                  \"sum_mode\": self.peft_config.sum_mode, \"initialize_mode\": self.peft_config.initialize_mode, }\n        key_list = [key for key, _ in self.model.named_modules()]\n        for key in key_list:\n            if isinstance(self.peft_config.target_modules, str):\n                target_module_found = re.fullmatch(self.peft_config.target_modules, key)\n            else:\n                target_module_found = any(key.endswith(target_key) for target_key in self.peft_config.target_modules)\n            if target_module_found:\n                if not is_target_modules_in_base_model:\n                    is_target_modules_in_base_model = True\n                parent, target, target_name = self._get_submodules(key)\n                bias = target.bias is not None\n                if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n                    raise NotImplementedError\n                elif isinstance(target, torch.nn.Linear) and self.peft_config.enable_lora is None:\n                    new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)\n                elif self.peft_config.enable_lora is not None:\n                    raise NotImplementedError\n                self._replace_module(parent, target_name, new_module, target)\n        if not is_target_modules_in_base_model:\n            raise ValueError(f\"Target modules {self.peft_config.target_modules} not found in the base model. \"\n                             f\"Please check the target modules and try again.\")\n\n\nclass MergeBuffer(nn.Module):\n\n    def __init__(self, default=False):\n        super(MergeBuffer, self).__init__()\n        self.register_buffer('merged', torch.tensor(default))\n\n    def __bool__(self):\n        return self.merged.item()\n\n    def set(self, value):\n        self.merged.fill_(value)\n\n\nclass QuanTALayer:\n    def __init__(self, d: int, quanta_dropout: float, merge_weights: bool, sum_mode: bool = False, ):\n        self.d = d\n        self.sum_mode = sum_mode\n        if quanta_dropout > 0.:\n            self.quanta_dropout = nn.Dropout(p=quanta_dropout)\n        else:\n            self.quanta_dropout = lambda x: x\n        self.merged = MergeBuffer(default=False)\n        self.frozen_merged = MergeBuffer(default=False)\n        self.merge_weights = merge_weights\n\n\nclass BufferDict(nn.Module):\n    def __init__(self, init_dict=None):\n        super(BufferDict, self).__init__()\n        self.buffer_names = []\n        if init_dict is not None:\n            for name, tensor in init_dict.items():\n                self.add_buffer(name, tensor)\n\n    def add_buffer(self, name, tensor):\n        self.register_buffer(name, tensor)\n        if name not in self.buffer_names:\n            self.buffer_names.append(name)\n\n    def __getitem__(self, name):\n        return getattr(self, name)\n\n    def __setitem__(self, name, tensor):\n        self.add_buffer(name, tensor)\n\n    def items(self):\n        for name in self.buffer_names:\n            yield name, getattr(self, name)\n\n    def keys(self):\n        return self.buffer_names\n\n    def values(self):\n        return [getattr(self, name) for name in self.buffer_names]\n\n\nclass Linear(nn.Linear, QuanTALayer):\n    def __init__(self, in_features: int, out_features: int, d: int = 1, quanta_dropout: float = 0.,\n                 fan_in_fan_out: bool = False,\n                 per_dim_features: list = None,\n                 per_dim_features2: list = None,\n                 merge_weights: bool = False, sum_mode: bool = False, initialize_mode: str = 'scale_by_softplus_zero',\n                 **kwargs):\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n\n        self.fan_in_fan_out = fan_in_fan_out\n        self.initialize_mode = initialize_mode\n        if per_dim_features is not None:\n            d = len(per_dim_features)\n        QuanTALayer.__init__(self, d=d, quanta_dropout=quanta_dropout, sum_mode=sum_mode, merge_weights=merge_weights)\n\n        if d > 1:\n            self.max_features = max(in_features, out_features)\n            if per_dim_features is not None:\n                self.per_dim_features = per_dim_features\n            else:\n                self.per_dim_features = [math.ceil(self.max_features ** (1 / d))] * d\n            if per_dim_features2 is not None:\n                raise NotImplementedError('per_dim_features2 is not implemented yet')\n            else:\n                self.per_dim_features2 = self.per_dim_features\n\n            self.total_features = prod(self.per_dim_features)\n            self.total_features2 = prod(self.per_dim_features2)\n\n            if self.total_features != in_features:\n                warnings.warn(\n                    f'per_dim_features={self.per_dim_features} does not match in_features={in_features}, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.')\n            if self.total_features2 != out_features:\n                warnings.warn(\n                    f'per_dim_features2={self.per_dim_features2} does not match out_features={out_features}, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.')\n\n            quanta_weights = {}\n            for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):\n                quanta_weights[f'{dim1} {dim2}'] = nn.Parameter(\n                    self.weight.new_zeros(self.per_dim_features2[dim2], self.per_dim_features2[dim1],\n                                          self.per_dim_features[dim2], self.per_dim_features[\n                                              dim1]))\n            self.quanta_weights = nn.ParameterDict(quanta_weights)\n            if initialize_mode == 'sum_opposite_freeze_one':\n                quanta_weights2 = {}\n                for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):\n                    quanta_weights2[f'{dim1} {dim2}'] = self.weight.new_zeros(self.per_dim_features2[dim2],\n                                                                              self.per_dim_features2[dim1],\n                                                                              self.per_dim_features[dim2],\n                                                                              self.per_dim_features[dim1])\n                self.quanta_weights2 = BufferDict(quanta_weights2)\n\n            else:\n                assert False, f'initialize_mode={initialize_mode} not implemented'\n\n            self.weight.requires_grad = False\n\n        self.reset_parameters()\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.transpose(0, 1)\n\n        self.gen_einsum_expr_train()\n        self.gen_einsum_expr_eval()\n\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        if hasattr(self, 'quanta_weights'):\n            for k, v in self.quanta_weights.items():\n                nn.init.kaiming_uniform_(v.view(v.shape[0] * v.shape[1], v.shape[2] * v.shape[3]), a=math.sqrt(5),\n                                         nonlinearity='linear')\n            if self.initialize_mode == 'last_layer_zero':\n                nn.init.zeros_(self.quanta_weights[f'{-self.d + 1} {-self.d}'])\n            if self.initialize_mode == 'add_local_layer_zero' or self.initialize_mode == 'add_local_layer_zero_sum':\n                for k, v in self.local_weights.items():\n                    nn.init.zeros_(v)\n            if self.initialize_mode == 'sum_opposite':\n                for k, v in self.quanta_weights2.items():\n                    v.data[:] = self.quanta_weights[k].data\n            if self.initialize_mode == 'sum_opposite_freeze_one':\n                for k, v in self.quanta_weights2.items():\n                    v[:] = self.quanta_weights[k].data\n\n    def train(self, mode: bool = True):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n\n        nn.Linear.train(self, mode)\n        if mode:\n            if self.merge_weights and self.merged and self.initialize_mode in ['sum_opposite_freeze_one']:\n                if self.d > 0:\n                    full_quanta_weights = F.pad(self.einsum_expr_eval(\n                        *[self.quanta_weights[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in\n                          itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,\n                                                                                          self.total_features),\n                                                (0, self.in_features - self.total_features, 0,\n                                                 self.out_features - self.total_features2),\n                                                'constant', 0.)\n                    self.weight.data -= T(full_quanta_weights)\n                self.merged.set(False)\n        else:\n            if self.merge_weights and not self.merged and self.initialize_mode in ['sum_opposite_freeze_one']:\n                if self.d > 0:\n                    full_quanta_weights = F.pad(self.einsum_expr_eval(\n                        *[self.quanta_weights[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in\n                          itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,\n                                                                                          self.total_features),\n                                                (0, self.in_features - self.total_features, 0,\n                                                 self.out_features - self.total_features2),\n                                                'constant', 0.)\n                    self.weight.data += T(full_quanta_weights)\n                    if not self.frozen_merged:\n                        self.merge_frozen_weights()\n                self.merged.set(True)\n\n    def merge_frozen_weights(self):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n\n        if self.frozen_merged:\n            warnings.warn('The frozen weights are already merged, ignoring the request to merge the frozen weights')\n        else:\n            full_quanta_weights = -F.pad(self.einsum_expr_eval(\n                *[self.quanta_weights2[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in\n                  itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,\n                                                                                  self.total_features),\n                                         (0, self.in_features - self.total_features, 0,\n                                          self.out_features - self.total_features2), 'constant',\n                                         0.)\n            self.weight.data += T(full_quanta_weights)\n            self.frozen_merged.set(True)\n\n    def unmerge_frozen_weights(self):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n\n        if not self.frozen_merged:\n            warnings.warn('The frozen weights are already unmerged, ignoring the request to unmerge the frozen weights')\n        else:\n            full_quanta_weights = -F.pad(self.einsum_expr_eval(\n                *[self.quanta_weights2[f'{dim1} {dim2}'].to(self.weight.dtype) for (dim1, dim2) in\n                  itertools.combinations(range(-1, -self.d - 1, -1), 2)]).reshape(self.total_features2,\n                                                                                  self.total_features),\n                                         (0, self.in_features - self.total_features, 0,\n                                          self.out_features - self.total_features2), 'constant',\n                                         0.)\n            self.weight.data -= T(full_quanta_weights)\n            self.frozen_merged.set(False)\n\n    def gen_einsum_expr_train(self):\n        d = self.d\n        current_symbols_inds = list(range(d))\n\n        eq = '...'\n        for i in current_symbols_inds:\n            eq += oe.get_symbol(i)\n\n        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):\n            symbol_ind1 = current_symbols_inds[dim1]\n            symbol_ind2 = current_symbols_inds[dim2]\n            symbol_ind3 = symbol_ind1 + d\n            symbol_ind4 = symbol_ind2 + d\n            eq += ',' + oe.get_symbol(symbol_ind4) + oe.get_symbol(symbol_ind3) + oe.get_symbol(\n                symbol_ind2) + oe.get_symbol(\n                symbol_ind1)\n            current_symbols_inds[dim1] = symbol_ind3\n            current_symbols_inds[dim2] = symbol_ind4\n\n        eq += '->...'\n        for i in current_symbols_inds:\n            eq += oe.get_symbol(i)\n\n        shapes = [(100,) + tuple(self.per_dim_features)]\n        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):\n            shapes.append((self.per_dim_features[dim2], self.per_dim_features[dim1], self.per_dim_features[dim2],\n                           self.per_dim_features[dim1]))\n\n        optimize = 'optimal' if d <= 4 else 'branch-all' if d <= 5 else 'branch-2' if d <= 7 else 'auto'\n        expr = oe.contract_expression(eq, *shapes, optimize=optimize)\n\n        self.einsum_eq_train = eq\n        self.einsum_expr_train = expr\n\n    def gen_einsum_expr_eval(self):\n        d = self.d\n        current_symbols_inds = list(range(d))\n        init_symbols_inds = [i for i in current_symbols_inds]\n\n        eq = ''\n\n        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):\n            symbol_ind1 = current_symbols_inds[dim1]\n            symbol_ind2 = current_symbols_inds[dim2]\n            symbol_ind3 = symbol_ind1 + d\n            symbol_ind4 = symbol_ind2 + d\n            eq += ',' + oe.get_symbol(symbol_ind4) + oe.get_symbol(symbol_ind3) + oe.get_symbol(\n                symbol_ind2) + oe.get_symbol(symbol_ind1)\n            current_symbols_inds[dim1] = symbol_ind3\n            current_symbols_inds[dim2] = symbol_ind4\n\n        eq += '->'\n        for i in current_symbols_inds:\n            eq += oe.get_symbol(i)\n        for i in init_symbols_inds:\n            eq += oe.get_symbol(\n                i)\n        eq = eq[1:]\n\n        shapes = []\n        for (dim1, dim2) in itertools.combinations(range(-1, -d - 1, -1), 2):\n            shapes.append((self.per_dim_features[dim2], self.per_dim_features[dim1], self.per_dim_features[dim2],\n                           self.per_dim_features[dim1]))\n\n        optimize = 'optimal' if d <= 4 else 'branch-all' if d <= 5 else 'branch-2' if d <= 7 else 'auto'\n        expr = oe.contract_expression(eq, *shapes, optimize=optimize)\n\n        self.einsum_eq_eval = eq\n        self.einsum_expr_eval = expr\n\n    def forward_quanta_weights(self, x, quanta_weights):\n        return self.einsum_expr_train(x, *[quanta_weights[f'{dim1} {dim2}'].to(x.dtype) for (dim1, dim2) in\n                                           itertools.combinations(range(-1, -self.d - 1, -1), 2)])\n\n    def forward_sum_opposite(self, x: torch.Tensor):\n        assert self.initialize_mode == 'sum_opposite_freeze_one', f'this function is only for sum_opposite_freeze_one, but got {self.initialize_mode=}'\n        assert not self.sum_mode, f'this function only works for sum_mode=False, but got {self.sum_mode=}'\n        if not self.frozen_merged:\n            self.merge_frozen_weights()\n        previous_dtype = self.weight.dtype\n\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n\n        if self.d > 1 and not self.merged:\n            result = F.linear(x, T(self.weight), bias=self.bias.to(previous_dtype) if self.bias is not None else None)\n            x = self.quanta_dropout(x)\n            x = F.pad(x, (0, self.total_features - self.in_features), 'constant', 0.)\n            x_shape = x.shape\n            x = x.view(-1, *self.per_dim_features)\n\n            x = self.forward_quanta_weights(x, self.quanta_weights).reshape(*x_shape[:-1], self.total_features2)\n\n            result += F.pad(x, (0, self.out_features - self.total_features2), 'constant', 0.)\n\n            return result\n        else:\n            return F.linear(x, T(self.weight), bias=self.bias.to(previous_dtype) if self.bias is not None else None)\n\n    def forward(self, x: torch.Tensor):\n        if self.initialize_mode == 'sum_opposite_freeze_one':\n            return self.forward_sum_opposite(x)\n        else:\n            raise NotImplementedError(\n                f'initialize_mode={self.initialize_mode} not implemented, only sum_opposite_freeze_one is implemented at the moment')",
    "Experiment Result": "{\n  \"quanta_d\": 4,\n  \"quanta_per_dim_features\": [\n    16,\n    8,\n    8,\n    4\n  ],\n  \"quanta_per_dim_features2\": None,\n  \"quanta_sum_mode\": false,\n  \"quanta_initialize_mode\": \"sum_opposite_freeze_one\",\n  \"quanta_dropout\": 0.0,\n  \"target_modules\": null\n}"
}{
    "Title": "Evaluating Quantized Large Language Models",
    "Main Contributions": "This paper presents a comprehensive evaluation of Post-Training Quantization (PTQ) on Large Language Models (LLMs) to guide the selection of quantization methods for efficiency and performance across diverse scenarios. It evaluates the effect of PTQ on Weight, Activation, and KV Cache across 11 model families (125M to 180B parameters) and five task types (basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks). Key findings include: larger models generally show higher tolerance for Weight-only and KV Cache Quantization but lower tolerance for Activation Quantization; sensitivity to quantization varies significantly across different tensor positions; relative rankings of quantized LLMs are mostly consistent with FP16 models for bit-widths higher than W4, W4A8, and KV4; emergent abilities like Multi-Step Reasoning and Self-Calibration are less tolerant to quantization; dialogue tasks exhibit sentence-level and token-level repetition or randomness at lower bit-widths; and long-context tasks are more sensitive to Weight and KV Cache quantization as text length increases.",
    "Methodology": "The study focuses on uniform post-training quantization (PTQ), including Weight-only, Weight-Activation, and KV Cache Quantization. It utilizes asymmetric group-wise quantization for Weight and KV Cache tensors, and symmetric per-token quantization for Activation tensors. The evaluation employs two primary methods: 'Evaluating in the PPL mode' for multiple-choice tasks (calculating perplexity for options) and 'Evaluating in the Gen mode' for generative tasks (generating replies based on prompts). Statistical analysis using maximum absolute value (AbsMax), standard deviation (Std), and kurtosis is applied to analyze tensor characteristics and their amenability to quantization. State-of-the-art quantization methods like AWQ and SmoothQuant are also evaluated for their applicability.",
    "Experimental Setup": "The evaluation spans 11 LLM families: OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. Five types of tasks are covered using various benchmarks: 1. Basic NLP (Language Modeling: CHID, Winogrande; Understanding: RACE, LAMBADA; Reasoning: SIQA, PIQA). 2. Emergent Abilities (In-Context Learning: MMLU, CEval; Multi-Step Reasoning: GSM8K, StrategyQA; Instruction-Following: Hellaswag, ARC; Self-Calibration: MMLU). 3. Trustworthiness (Ethics: ETHICS; Hallucination: TruthfulQA; Adversarial Robustness: AdvGLUE). 4. Dialogue (MT-bench, evaluated by GPT-4 score). 5. Long-Context (Key-value retrieval: Longeval up to 16K tokens; Multi-document QA up to 6K tokens).",
    "Limitations": "The research focuses solely on Post-training Quantization (PTQ) and does not explore Quantization-Aware Training (QAT). Detailed ablation studies on hyperparameters, such as different group sizes in group-wise quantization, were not conducted. Not all newly available LLMs in the 1B to 3B parameter range were evaluated. The specific bit-width recommendations provided are context-dependent and may not generalize well to novel tasks or LLMs.",
    "Future Research Directions": "Future research should explore advanced quantization schemes or Quantization-Aware Training (QAT) methods to enable extremely low bit-width quantization (e.g., W2, W4A4, KV2). Investigating the impact of quantization on multi-turn dialogues beyond two turns is also suggested. Additionally, strategies for simultaneously leveraging quantization and Mixture-of-Experts (MoE) techniques to enhance inference efficiency are promising. More studies on mix-precision quantization methods are needed to optimize the trade-off between hardware efficiency and performance, particularly considering varying sensitivities of different tensor types and positions. Research to ensure low-bit LLMs maintain robust problem-solving logic, especially for mathematical reasoning, is also crucial.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "Main Contributions": "The paper addresses the significant manual effort and computational cost associated with tuning learning rate (LR) schedules in Deep Neural Network (DNN) training, proposing that existing pre-defined schedules and adaptive optimizers still introduce new hyperparameters requiring tuning. It introduces AutoLRS, an efficient, automatic method to optimize the LR for each training stage without human involvement, modeling training dynamics using Bayesian Optimization. AutoLRS achieves significant speedups (1.22x for ResNet-50, 1.43x for Transformer, and 1.5x for BERT) over original hand-tuned LR schedules and an average of 1.31x over state-of-the-art heavily-tuned LR schedules, while maintaining or improving test-set performance.",
    "Methodology": "AutoLRS breaks down the LR schedule optimization into dynamically optimizing a constant LR for every 'τ' steps (a 'training stage'). It employs Bayesian Optimization (BO) to find the LR that minimizes the validation loss at the end of each stage, treating the validation loss as a black-box function. To overcome the prohibitive cost of evaluating each LR candidate for 'τ' steps, AutoLRS uses a light-weight time-series forecasting model. This model is trained by applying each candidate LR for only 'τ'' (τ' << τ) steps and using the observed validation loss to predict the loss after 'τ' steps, often an exponential model (L(t) = aexp(bt) + c). This creates a mutual-training process between BO and the loss-prediction model. Practical improvements include gradually increasing 'τ' over training, initially minimizing training loss as a proxy for validation loss, and using iterative spline smoothing to pre-process loss series for stable exponential fitting. Lower Confidence Bound (LCB) is used as the acquisition function for BO.",
    "Experimental Setup": "AutoLRS was evaluated on three representative DNNs: ResNet-50 (ImageNet classification using SGD with momentum on 32 NVIDIA Tesla V100 GPUs), Transformer (WMT 2014 English-German NMT using Adam on 8 NVIDIA Tesla V100 GPUs), and BERTBASE (pre-training on English Wikipedia and BooksCorpus with mixed precision and Adam on 32 NVIDIA Tesla V100 GPUs, then fine-tuning on MRPC, MNLI, CoLA, and SQuAD v1.1). Additional experiments for ablation studies and comparisons used VGG-16 on CIFAR-10 and CIFAR-100 (on one NVIDIA Titan RTX GPU). Baselines included original hand-tuned LR schedules, Cyclical Learning Rate (CLR), Stochastic Gradient Descent with Warm Restarts (SGDR), Hypergradient Descent (HD), MARTHE, and Hyperband. AutoLRS default hyperparameters were: k=10, τ'=τ/10, initial τ=1000, τ'=100, doubling τ and τ' after each stage until τmax (8000 for ResNet-50/Transformer, 32000 for BERT), and κ=1000 for LCB. LR search intervals were (10^-3, 1) for ResNet-50, (10^-4, 10^-2) for Transformer, and (10^-6, 10^-3) for BERT, determined by an LR range test.",
    "Limitations": "The current AutoLRS does not search for learning rates during warmup steps, as warmup phases lack an explicit validation loss objective. While AutoLRS significantly reduces hyperparameter tuning, it still requires setting a learning-rate search interval (ηmin, ηmax), though this can be broad and found via an LR range test. The default settings for AutoLRS's internal hyperparameters (k, τ', τ, κ) generally work well, but further improvements might be possible through fine-tuning. The exponential forecasting model can struggle to fully capture non-monotone loss changes in very early training steps, although iterative spline smoothing helps mitigate this. Compared to other automated methods, Hypergradient Descent (HD) is highly sensitive to its hyperparameters, prone to overfitting, and failed to match state-of-the-art performance. Similarly, CLR and SGDR are sensitive to hyperparameters and can lead to divergence with suboptimal settings. Hyperband incurred high computational overhead and could not achieve state-of-the-art accuracy.",
    "Future Research Directions": "Not explicitly mentioned as a dedicated section. However, based on the discussion, potential future research directions include: automating the determination of the LR search interval (ηmin, ηmax) to eliminate the need for a preliminary LR range test; developing mechanisms to incorporate and optimize learning rates for warmup phases within AutoLRS; exploring more advanced yet efficient time-series forecasting models for loss prediction; further fine-tuning AutoLRS's own internal hyperparameters (k, τ', τ, κ, τmax) for even greater robustness and performance; and extending AutoLRS to a wider array of novel DNN architectures, tasks, and optimizers to further validate its generality and robustness across new domains.",
    "Experiment Code": "import torch\nimport os\nimport logging\nimport socket\nimport string\nimport random\nimport time\n\nclass AutoLRS():\n    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):\n        self._net = model\n        self._optimizer = optimizer\n        self._val_fn = val_fn \n        self._lr = 0.000001\n        self._warmup_steps = warmup_steps\n        self._warmup_lr = warmup_lr \n        self._global_step = 0\n        self._socket = socket.socket()\n        self._started = False\n        self._summary_steps = summary_steps\n        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'\n        self._listening_host = listening_host\n        self._listening_port = listening_port\n        self._best_acc = 0\n\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n\n        self.connect_server()\n    \n    def connect_server(self):\n        self._socket.connect((self._listening_host, self._listening_port))\n\n    def _verbose_operation(self, _op):\n        if self._global_step % self._summary_steps == 0:\n            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))\n\n    def save_variables(self):\n        \"\"\"Save model parameters and optimizer states.\"\"\"\n        _start_time = time.time()\n        torch.save({\n            'model_state_dict': self._net.state_dict(),\n            'optimizer_state_dict': self._optimizer.state_dict()\n            }, self._checkpoint_path)\n        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def restore_variables(self):\n        _start_time = time.time()\n        checkpoint = torch.load(self._checkpoint_path)\n        self._net.load_state_dict(checkpoint['model_state_dict'])\n        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def on_train_batch_end(self, loss):\n        if self._global_step < self._warmup_steps:\n            # linear warmup\n            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n            self._global_step += 1\n\n        elif not self._started:\n            self.save_variables()\n            print(\"backup trainable variables to CPU\") \n            self._started = True\n            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))\n            self._verbose_operation(\"Start Bayesian Optimization(BO)\")\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            self._lr = (float(data.split(\",\")[-1]))\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n        else:\n            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            if data.startswith(\"restore\"):\n                self.restore_variables()\n                self._verbose_operation(\"restore trainable variables\")\n            elif data.startswith(\"ckpt\"):\n                self.save_variables()\n                self._verbose_operation(\"backup trainable variables\")\n            elif data.startswith('evaluate'):\n                val_loss = self._val_fn()\n                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))\n                data = self._socket.recv(1024).decode(\"utf-8\")\n            elif data.startswith('save'):\n                pass\n            else:\n                self._lr = (float(data.split(\",\")[-1]))\n                for param_group in self._optimizer.param_groups:\n                    param_group['lr'] = self._lr\n                self._global_step += 1\n\nimport argparse\nimport socket                \nimport random\nimport numpy as np \nimport threading\nimport time\nimport math\nimport os\nfrom skopt import Optimizer\nfrom skopt.space import Real\nfrom scipy.interpolate import UnivariateSpline\nfrom scipy import optimize \nimport sys\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef f(b, x, y):\n    A = np.vstack([np.exp(-np.exp(b) * x), np.ones(len(x))]).T\n    res = np.linalg.lstsq(A, y, rcond=None)[1]\n    return res\n\ndef spline_iter(xs, ys, is_training, spline_deg=2, filter_ratio=0.03, num_of_iter=10, bound=0.5):\n    \"\"\" Use iterative spline to eliminate noise and outliers in the loss series.\n        is_training specifies whether the loss series in use is training loss or validation loss.\n    \"\"\"\n    bound = xs[int((len(xs) - 1) * bound)]\n    if is_training:\n        num_of_iter = 10\n    else:\n        num_of_iter = 1\n\n    for _ in range(num_of_iter):\n        spline_ys = UnivariateSpline(xs, ys, k=spline_deg)(xs)\n        dys = np.abs(ys - spline_ys)\n\n        if is_training:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[int(round(-len(dys) * filter_ratio)):])\n        else:\n            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[-1:])\n        outliers = [i for i in outliers if i < bound]\n\n        xs2 = np.zeros(len(xs) - len(outliers))\n        ys2 = np.zeros(len(xs) - len(outliers))\n        i1 = 0\n        for i2 in range(len(xs)):\n            if i2 not in outliers:\n                xs2[i1], ys2[i1] = xs[i2], ys[i2]\n                i1 += 1\n        xs, ys = xs2, ys2\n    return xs, ys\n\ndef exp_forecast(loss_series, end_step, is_training, spline_order=2):\n    \"\"\" Do exponential forecasting on a loss series.\"\"\"\n    xs = np.arange(end_step - len(loss_series), end_step)\n    xs2, ys2 = spline_iter(xs, loss_series, is_training)\n    ys = UnivariateSpline(xs2, ys2, k=spline_order)(xs)\n    logging.debug('ys after spline iter: {}'.format(ys))\n    b = optimize.fmin(f, 0, args=(xs, ys), xtol=1e-5, ftol=1e-5, disp=False)[0]\n    b = -np.exp(b)\n    A = np.vstack([np.exp(b * xs), np.ones(len(xs))]).T\n    a, c = np.linalg.lstsq(A, ys, rcond=None)[0]\n    return a, b, c\n\nclass RingBuffer:\n    \"\"\" A class for storing and manipulating loss series and do exponential forecasting. \"\"\"\n\n    def __init__(self, size):\n        self.data = [None for i in range(size)]\n\n    def reset(self):\n        self.data = [None for i in self.data]\n\n    def append(self, x):\n        self.data.pop(0)\n        self.data.append(x)\n\n    def get(self):\n        return self.data\n\n    def average(self):\n        return sum(self.data)/len(self.data)\n\n    def exponential_forcast(self, pred_index, is_training):\n        loss_series = self.data[:]\n        end_epoch = len(loss_series)\n        x = np.arange(end_epoch - len(loss_series), end_epoch)\n        y = np.array(loss_series)\n        a3, b3, c3 = exp_forecast(y, len(y), is_training)\n        forcast_y = a3 * np.exp(b3 * pred_index) + c3\n        logging.debug(\"Exponential fit: {}, {}, {}\".format(a3, b3, c3))\n        return forcast_y\n\nclass Controller(object):\n    def __init__(self, host, port, min_lr, max_lr):\n        # Constants\n        EXPLOITATION_STEP = 1000\n        LR_STEPS = 100\n        RING_BUFFER_LEN = 100\n        LR_TO_EXPLORE = 10\n        TAU_MAX = 8000\n\n        self.min_lr = float(min_lr)\n        self.max_lr = float(max_lr)\n        self.host = host\n        self.port = port\n        self.threads = []\n        self.num_threads = 1\n        self.event = threading.Event()\n        self.sock = socket.socket()\n        self.sock.bind((self.host, self.port))\n        self.global_step = 0\n        self.last_total_loss = 0.0\n        self.average_loss = 0.0\n        self.loss_vector = []\n        self.lr = 0\n        self.lr_steps = LR_STEPS \n        self.tau_max = TAU_MAX\n        self.val_freq = int(self.lr_steps/16)\n        self.lr_counter = 0\n        self.lr_to_explore = LR_TO_EXPLORE\n        self.BO_stage = True\n        self.val_stage = False\n        self.message = ''\n        self.loss_after_exploitation = None\n        self.ring_buffer_len = RING_BUFFER_LEN\n        if self.val_stage:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n        else:\n            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n        self.exploitation_step = EXPLOITATION_STEP\n        self.exploitation_flag = False \n        self.exploitation_counter = 0\n        \n        self.opt = None\n        self.x_func_dict = dict()\n        self.x_iters = []\n        self.func_val_iters = []\n\n        self.num_ranks = 0\n        self.finished_minions = 0\n        self.lock1 = threading.Lock()\n        self.lock2 = threading.Lock()\n\n    def listen(self):\n        self.sock.listen(20)\n        while True:\n            client, address = self.sock.accept()\n            logging.info('[Server]: Got connection from {}'.format(address))\n            self.threads.append(threading.Thread(target = self.run, args = (client, address, self.event)))\n            if len(self.threads) == self.num_threads:\n                self.num_ranks = len(self.threads)\n                self.num_minions = self.num_ranks - 1\n                logging.info('[Server]: num_ranks: {}'.format(self.num_ranks))\n                for thread in self.threads:\n                    thread.start()\n                logging.info('[Server]: threads started')\n                for thread in self.threads:\n                    thread.join()\n                    self.threads = []\n                logging.info('[Server]: join finished')\n                sys.exit()\n\n    def run(self, c, address, event):\n        size = 1024\n        while True:\n            data = c.recv(size).decode()\n            if not data:\n                sys.exit()\n            logging.debug(data.split(','))\n            total_loss = float(data.split(',')[-1])\n            self.lock2.acquire()\n            self.loss_vector.append(total_loss)\n\n            # compute average loss across ranks\n            if len(self.loss_vector) == self.num_ranks:\n                self.average_loss = sum(self.loss_vector) / len(self.loss_vector) \n                logging.info('[Server]: average loss = {}, step = {}'.format(self.average_loss, self.global_step))\n                if self.val_stage:\n                    if 'val' in data:\n                        self.ring_loss_buffer.append(self.average_loss)\n                    else:\n                        self.global_step += 1\n                else:\n                    self.ring_loss_buffer.append(self.average_loss)\n                    self.global_step += 1\n                self.loss_vector = []\n            self.lock2.release()\n\n            if 'val' in data:\n                c.send(str(self.lr).encode('utf-8')) \n                continue\n\n            if 'minion' in data:\n                # blocking\n                event_is_set = event.wait()\n                c.send(self.message.encode('utf-8')) \n                logging.debug('[Server] message: %s', self.message)\n                self.lock1.acquire()\n                self.finished_minions += 1\n                self.lock1.release()\n                if self.finished_minions == self.num_minions:\n                    event.clear()\n                    self.finished_minions = 0\n                continue\n\n            else:\n                if data.startswith('startBO'):\n                    self.last_total_loss = self.average_loss\n                    self.init_loss = self.average_loss\n                    self.loss_after_exploitation = self.average_loss\n\n                # exploitation stage -- actual training stage using the best-found LR\n                if self.exploitation_flag:\n                    logging.debug('[Server exploitation]: average loss ' + str(self.average_loss) + ' step=' + str(self.global_step))\n                    if self.exploitation_counter == self.exploitation_step:\n                        self.BO_stage = True\n                        self.exploitation_flag = False\n                        self.exploitation_counter = 0\n                        logging.info('[Server]: exploitation stage done')\n                        logging.info('[Server]: reconfigure...')\n                        if self.lr_steps < self.tau_max / 10:\n                            self.lr_steps = self.lr_steps * 2\n                            self.val_freq = int(self.lr_steps/16)\n                            self.ring_buffer_len = self.lr_steps \n                            self.exploitation_step = self.exploitation_step * 2\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n                        else:\n                            self.val_stage = True\n                        if self.val_stage:\n                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)\n                        self.loss_after_exploitation = self.average_loss\n\n                        self.message = 'save'\n                        c.send(self.message.encode('utf-8'))\n                        event.set()\n                        continue\n\n                    else:\n                        self.exploitation_counter += 1\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                        event.set()\n                        continue\n\n                # BO stage -- LR search stage\n                if self.BO_stage:\n                    self.opt = Optimizer([Real(self.min_lr, self.max_lr, 'log-uniform')], \"GP\", n_initial_points=1, acq_func='LCB', acq_func_kwargs={'kappa':1e6})\n                    self.BO_stage = False\n                    self.lr = self.opt.ask()[0]\n\n                    # prevent BO in scikit-optimize from searching for the same LR explored before\n                    while True:\n                        if self.lr in self.x_func_dict:\n                            self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                            self.lr = self.opt.ask()[0]\n                        else:\n                            break\n\n                    self.message = str(','.join(('ckpt', str(self.lr))))\n                    c.send(','.join(('ckpt', str(self.lr))).encode('utf-8'))\n                    event.set()\n                    logging.debug('[Server]: checkpoint command sent')\n                    continue\n\n                # ask BO to suggest the next LR \n                if self.lr_counter == self.lr_steps:\n                    logging.debug('ring_buffer: {}'.format(self.ring_loss_buffer.get()))\n                    if any([math.isnan(x) for x in self.ring_loss_buffer.get()]):\n                        predicted_loss = \"nan\"\n                    elif self.val_stage:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=int(self.exploitation_step/self.val_freq), is_training=False)\n                        current_loss = sum(self.ring_loss_buffer.get()[-1:])/1.0\n                    else:\n                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=self.exploitation_step, is_training=True)\n                        current_loss = sum(self.ring_loss_buffer.get()[-10:])/10.0\n\n                    logging.info('[Server]: predicted loss: {} due to LR {}'.format(predicted_loss, self.lr))\n\n                    # Huge loss jump can make the exponential prediction inaccurate, so set a threshold here. \n                    #if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) > 10 * self.loss_after_exploitation:\n                    #    predicted_loss = current_loss \n                    #    logging.info('New predicted_loss: ' + str(predicted_loss))\n\n                    if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) >= 1.0 * self.init_loss and self.val_stage:\n                        predicted_loss = current_loss \n                        logging.info('[Server]: New predicted_loss: ' + str(predicted_loss))\n\n                    if self.val_stage:\n                        self.ring_loss_buffer = RingBuffer(int(math.floor(self.ring_buffer_len)/self.val_freq))\n                    else:\n                        self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)\n\n                    # feed a (LR, predicted loss in tau steps) instance to BO.\n                    if str(predicted_loss) == 'nan':\n                        self.opt.tell([float(self.lr)], 1e6)\n                    else:\n                        self.opt.tell([float(self.lr)], predicted_loss)\n                    self.x_iters.append(float(self.lr))\n                    self.func_val_iters.append(predicted_loss)\n                    self.x_func_dict[self.lr] = predicted_loss\n                    self.lr_counter = 1\n\n                    if len(self.func_val_iters) == self.lr_to_explore:\n                        min_index = self.func_val_iters.index(min(self.func_val_iters))\n\n                        # log the best lr found for the next stage.\n                        logging.info('[Server]: best LR: {}, min loss: {}'.format(self.x_iters[min_index], self.func_val_iters[min_index]))\n\n                        self.lr = self.x_iters[min_index]\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                        self.exploitation_flag = True\n\n                        self.func_val_iters = []\n                        self.x_iters = []\n                        self.x_func_dict = dict()\n                    else:\n                        # Ask BO for the next LR to explore\n                        self.lr = self.opt.ask()[0]\n                        while True:\n                            if self.lr in self.x_func_dict:\n                                self.opt.tell([self.lr], self.x_func_dict[self.lr])\n                                self.lr = self.opt.ask()[0]\n                            else:\n                                break\n                        self.message = str(','.join(('restore', str(self.lr))))\n                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))\n                        event.set()\n                        logging.debug('[Server]: restore command sent')\n                else:\n                    self.lr_counter += 1\n                    if self.val_stage and self.lr_counter % self.val_freq == 0:\n                        self.message = \"evaluate\"\n                        c.send(self.message.encode('utf-8')) \n                    else:\n                        self.message = str(self.lr)\n                        c.send(str(self.lr).encode('utf-8')) \n                    event.set()\n\n# Training\ndef train(autolrs_callback):\n    for epoch in range(350 * 2): # multiply 2 to your original training epochs because the search steps of AutoLRS is the same as the actual training steps\n        print('\\nEpoch: %d' % (epoch+1))\n        net.train()\n        for batch_idx, (inputs, targets) in enumerate(trainloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            autolrs_callback.on_train_batch_end(loss.item())\n        test()\n\n# Test\ndef test():\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    print(\"Test acc: \", correct/total)\n    if correct/total > best_acc:\n        best_acc = correct/total\n        print('Saving..')\n        state = {\n            'net': net.state_dict(),\n            'acc': best_acc\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, './checkpoint/best_ckpt.pth')\n    print(\"Best acc: \", best_acc)\n    net.train()\n    \ndef val_fn():\n    net.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(valloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n    net.train()\n    return val_loss\n\nautolrs_callback = AutoLRS(net, optimizer, val_fn)\ntrain(autolrs_callback)",
    "Experiment Result": "AutoLRS Algorithm Parameters:\n- Initial learning rate (in AutoLRS client): 0.000001\n- Warmup steps: 0\n- Warmup LR: 0\n- Listening host/port: localhost:12315 (default)\n- Logging frequency (summary_steps): 1\n- Training stage constants (from Controller):\n  - EXPLOITATION_STEP (initial τ): 1000\n  - LR_STEPS (LR candidates per BO stage τ'): 100\n  - RING_BUFFER_LEN (for loss series): 100\n  - LR_TO_EXPLORE (number of LRs to try in BO): 10\n  - TAU_MAX (maximum τ): 8000\n  - val_freq: int(lr_steps/16) (initially 6)\n- Dynamic adjustment: lr_steps and exploitation_step are doubled if lr_steps < tau_max / 10.\n- Validation stage transition: val_stage becomes True when lr_steps approaches TAU_MAX (specifically, when lr_steps is not less than tau_max / 10).\n- Loss forecasting model: Exponential model L(t) = aexp(bt) + c (implemented in exp_forecast function).\n  - Spline smoothing parameters (in spline_iter):\n    - spline_deg: 2\n    - filter_ratio: 0.03\n    - num_of_iter: 10 (for training loss), 1 (for validation loss)\n    - bound: 0.5\n- Bayesian Optimization (BO) parameters:\n  - Optimizer: scikit-optimize Optimizer with Gaussian Process (\"GP\").\n  - Acquisition function: Lower Confidence Bound (LCB) with kappa=1e6.\n  - Initial points for BO: 1 (n_initial_points=1).\n  - Search space: Real(min_lr, max_lr, 'log-uniform').\n- Predicted loss adjustment:\n  - If predicted_loss is NaN, it's set to 1e6.\n  - If val_stage is true and max(ring_loss_buffer.get()) >= 1.0 * init_loss, predicted_loss is set to current_loss.\n\nCIFAR-10 Example Specifics:\n- Dataset: CIFAR-10\n- Data augmentation: RandomCrop(32, padding=4), RandomHorizontalFlip(), ToTensor(), Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)).\n- Model: VGG('VGG16') (other models like ResNet18, MobileNetV2, etc., are commented out).\n- Device: CUDA if available, otherwise CPU. Uses torch.nn.DataParallel for multiple GPUs.\n- Loss function: nn.CrossEntropyLoss().\n- Optimizer: optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4) (initial LR 0.1 is overridden by AutoLRS).\n- Batch sizes: trainloader=128, testloader=100, valloader=128.\n- Validation subset length (VAL_LEN): 10 mini-batches.\n- Total epochs for training loop: 700 (350 * 2, as search steps are the same as actual training steps)."
}{
    "Title": "Reverse engineering learned optimizers reveals known and novel mechanisms",
    "Main Contributions": "The paper investigates the opaque internal workings of learned optimizers, addressing how they outperform baselines and whether they learn known or novel optimization behaviors. The main contributions include developing tools for analyzing these high-dimensional nonlinear algorithms and discovering that learned optimizers implement interpretable mechanisms such as momentum, gradient clipping, learning rate schedules, and novel forms of learning rate adaptation across four diverse tasks. This work establishes a framework for interpreting learned optimizers.",
    "Methodology": "The learned optimizers are parameterized by a Gated Recurrent Unit (GRU) network, taking only the gradient as input. They are meta-trained by minimizing the average training loss of target problems using backpropagation through fully unrolled computational graphs and Adam as the meta-optimizer. Key analysis tools include: 1) \"Update Functions\" to visualize parameter changes as a function of gradient and infer effective learning rates. 2) A \"Dynamical Systems Perspective\" using linearized approximations near fixed points to understand optimizer state evolution. This approach helps identify momentum timescales from Jacobian eigenvalues. 3) Analyzing \"Autonomous Dynamics\" to reveal learning rate schedules. 4) Studying \"Input-Dependent Fixed Points\" (forming S-curves) to understand learning rate adaptation to gradient magnitudes. 5) Investigating parameter type-specific tuning by averaging optimizer state trajectories.",
    "Experimental Setup": "Learned optimizers were trained and analyzed on four tasks: linear regression, the Rosenbrock function, training a fully connected neural network on the Two Moons dataset, and training a convolutional neural network on the MNIST dataset (with stochastic gradients). For comparison, baseline optimizers (momentum, RMSProp, Adam) were hyperparameter-tuned individually for each task via 2500 random samples. Meta-training of the GRU-based optimizers involved an unroll length of 200 iterations, a meta-gradient batch size of 32, Adam meta-optimizer with tuned learning rate, gradient clipping, exponential learning rate decay, and L2 regularization over 5000 steps on a single TPUv2 core. Performance was evaluated using the meta-objective (average training loss) across 64 random test problems.",
    "Limitations": "The current analysis provides an isolated understanding of specific mechanisms but lacks a holistic picture of how they are integrated. Some observed behaviors, such as the non-monotonic update function for the MNIST task, remain unexplained. Furthermore, some findings, like parameter type-specific tuning, are correlative rather than causal, suggesting that the network might be responding to other factors correlated with depth or parameter type. The reliance on linearized approximations near fixed points is a methodological constraint, and the choice of tasks (fast convergence) implicitly suggests limitations in generalization to very long optimization horizons.",
    "Future Research Directions": "Future work includes distilling compressed optimizers from the identified mechanisms using data-driven or symbolic regression techniques. The developed analysis methods can be extended to study the generalization capabilities of learned optimizers. This involves mapping specific mechanisms to quantitative properties of loss surfaces (e.g., curvature, convexity) to predict when and how learned optimizers trained in one setting can be applied to new problems. The paper also suggests using the developed tools for extracting insights from other meta-learned algorithms.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Mechanic: A Learning Rate Tuner",
    "Main Contributions": "Introduces 'mechanic', a novel technique for automatically tuning the learning rate scale factor of any base optimization algorithm and schedule, eliminating the need for manual hyperparameter tuning. It provides a practical realization of recent theoretical reductions from online convex optimization (OCO) and demonstrates that it consistently matches or improves upon manually tuned learning rates across various large-scale deep learning tasks.",
    "Methodology": "The core methodology involves wrapping a base optimization algorithm with a 'meta' OCO algorithm, called the tuner. The tuner automatically adjusts a scalar 's' to scale the updates of the base algorithm, where the wrapped algorithm's iterate `xt` is defined as `xbase_1 + st * (xbase_t - xbase_1)`. The tuner receives an approximate gradient `ht = <gt, Δt>` (where `Δt = xbase_t - xbase_1`). The tuner algorithm (Algorithm 1) is a practical, simplified variant of a theoretically tractable parameter-free OCO algorithm (Algorithm 2). It incorporates exponentially weighted moving averages using multiple 'β' values (n=6) to introduce recency bias, combining their outputs for robustness. A peculiar weight-decay-esque term `λ * (sum(st,i)) * ||gt|| * xt / ||xt||` is also added to the tuner's input gradient `ht`, which empirically improves performance, particularly on vision tasks and in reducing result variance.",
    "Experimental Setup": "The method was rigorously evaluated across a range of large-scale deep learning tasks. These include Masked Language Modeling (MLM) for BERT pre-training on the Wikibooks dataset and subsequent finetuning on the 5 largest datasets from the GLUE suite (MNLI, QNLI, SST-2, QQP). For image classification, experiments covered training CNNs from scratch (ResNet-18, WRN-40-10 on CIFAR-10/100) and transfer learning (ViT models pre-trained on JFT-300M and finetuned on ImageNet, CIFAR-10, and CIFAR-100). Base optimization algorithms tested include AdamW, Lion, and SGD with Momentum. The evaluation considered varying batch sizes and model sizes, with results reported as peak validation scores averaged over 3 runs. Comparisons were made against manually tuned baselines and the D-adaptation algorithm, including a suite of 12 logistic regression problems.",
    "Limitations": "The 'mechanic' algorithm does not perform well in combination with dropout regularization. It optimizes for the train loss because it repurposes gradients from the training set, which contrasts with typical manual tuning based on validation set performance. The empirically effective weight-decay-esque term lacks a formal analytical explanation. Furthermore, the simplified tuner algorithm used in practice (Algorithm 1) deviates from the theoretically tractable version (Algorithm 2), making its rigorous theoretical analysis challenging.",
    "Future Research Directions": "Future work includes investigating a theoretical motivation for the empirically beneficial weight-decay term. Another promising direction is to leverage similar techniques to learn a per-layer scale factor, which could significantly outperform baselines by allowing more degrees of freedom without substantial computational overhead, an otherwise infeasible manual tuning task. Developing a principled way to handle the discrepancy between optimizing for train loss and evaluating based on validation set performance is also a key area for future research.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "MoMo: Momentum Models for Adaptive Learning Rates",
    "Main Contributions": "The paper introduces MoMo (Momentum Model) and MoMo-Adam, novel Polyak-type adaptive learning rates for momentum-based optimization methods like SGD-M and Adam. The core contribution is a new interpretation of momentum as a model of the objective function, constructed by averaging sampled loss functions and gradients. This model incorporates known lower bounds of the loss (e.g., zero) via truncation, leading to more robust performance. For cases with unknown non-zero lower bounds, the paper develops on-the-fly estimation techniques. MoMo demonstrates an O(1/root(K)) convergence rate for convex problems with interpolation, requiring minimal problem-specific knowledge. Empirically, MoMo and MoMo-Adam significantly reduce sensitivity to learning rate tuning and automatically exhibit warm-up and decay behaviors during training.",
    "Methodology": "MoMo methods leverage a model-based stochastic optimization approach, replacing the instantaneous loss function `f(x, sk)` with a simpler, truncated model `mk(x)` of the full loss `f(x)`. This `mk(x)` is built by exponentially weighted averages of past loss function evaluations and their linear approximations (gradients). A lower bound `fk*` is imposed on `mk(x)`, typically zero, or estimated online. The next iterate is computed by approximately minimizing `mk(x)` plus a proximal term. MoMo uses constant exponential weighting for its internal averages, while MoMo-Adam integrates Adam's diagonal preconditioner and bias-corrected exponential weighting. For unknown lower bounds, an online estimation method (MoMo* and MoMo-Adam*) is introduced based on a lower bound for an unbiased estimate of the optimal value `f*`, derived from convexity assumptions and past iterates.",
    "Experimental Setup": "The methods were evaluated against SGD-M and Adam/AdamW across various machine learning tasks. For tasks where `f*` is near zero (fixed `fk* = 0`), experiments included image classifiers (ResNet110 on CIFAR100, ResNet20/VGG16/ViT on CIFAR10, MLP on MNIST) and a recommender system (DLRM on Criteo). For tasks with non-zero `f*` (using online `fk*` estimation), experiments involved image classification (ResNet18 on Imagenet32/Imagenet-1k), German-to-English translation (transformer on IWSLT14), and a diffusion model (UNet on Smithsonian Butterflies). Hyperparameter sweeps, particularly for the base learning rate, were conducted, using both constant learning rates and schedules (warm-up + cosine decay or inverse square-root decay). Performance was measured by training loss, validation accuracy, and BLEU score (for translation), with results averaged over multiple seeds. Comparisons to AdaBelief, AdaBound, and Lion were also included.",
    "Limitations": "The theoretical convergence analysis for MoMo, while establishing an O(1/root(K)) rate, does not demonstrate a theoretical improvement due to momentum, with the best constant achieved when momentum parameter beta=0. This suggests a gap between empirical effectiveness and theoretical understanding of momentum in this context. The convergence proof relies on an interpolation assumption (f(x*, s) = f* for all s), which implies zero loss for every data point and may not hold in all practical scenarios. For problems where the optimal loss `f*` is significantly above zero, MoMo methods with a default `fk* = 0` (non-adaptive lower bound) show limited or no improvement over baselines, necessitating the more complex online lower bound estimation. The online lower bound estimation uses a bootstrapping approximation and an initial dependency on `D1` (distance to solution) is circumvented by setting `f1* = 0` which might not be optimal.",
    "Future Research Directions": "Future research could extend the application of the developed online lower bound estimation to other stochastic momentum-based methods beyond MoMo, given its independent interest. The general framework of Lemma 3.1 allows for the straightforward derivation of MoMo-versions of any preconditioned momentum method (e.g., Adabelief, Adabound), suggesting further exploration of these variants. Theoretical work could focus on providing a convergence analysis that formally demonstrates the benefits of momentum within the MoMo framework, addressing the current gap where empirical observations surpass theoretical guarantees. Additionally, relaxing or generalizing the interpolation assumption used in the current convergence proof could broaden the theoretical applicability of MoMo. Further refinement and optimization of the online lower bound estimation, especially for non-convex problems, is another potential direction.",
    "Experiment Code": "",
    "Experiment Result": "The system configures optimization parameters via Fairseq's command-line arguments. For an Adam-like optimizer, learning rate (`lr`) and Adam's beta parameters (`adam_betas`) are passed to the `fairseq-train` command. The values for these are loaded from a configuration object (e.g., `config.fairseq[field][key]`).\n\nExample parameters configured:\n- `lr`: learning rate\n- `adam_betas`: beta values for the Adam optimizer (e.g., `'\"(0.9, 0.999)\"'`)"
}{
    "Title": "Where Do Large Learning Rates Lead Us?",
    "Main Contributions": "This paper empirically studies the impact of initial learning rates (LRs) on neural network generalization, loss landscape, and feature learning. It identifies a narrow range of initial LRs (subregime 2A) slightly above the convergence threshold that consistently yields optimal generalization after fine-tuning or weight averaging. The study reveals that these optimal LRs lead to the location of convex basins containing high-quality minima and promote the learning of a sparse set of the most relevant features. The findings, initially observed in a controlled scale-invariant setting, are validated in conventional neural network training.",
    "Methodology": "The study employs a controlled setting by training scale-invariant (SI) neural networks with fixed parameter norms using projected SGD on a sphere, ensuring a fixed effective learning rate. Training is divided into two stages: pre-training with different fixed initial LRs (PLRs) and subsequent fine-tuning with a small LR (FLR) or Stochastic Weight Averaging (SWA). The impact is analyzed across three LR regimes (convergence, chaotic equilibrium, divergence) as defined by prior work. Local geometry of minima is assessed using angular distance and linear-path error barriers. Feature learning is investigated using a synthetic binary classification example with interpretable 'tick' features and by analyzing frequency bands (2D Discrete Fourier Transform) in real-world image datasets. The key findings are then verified in a practical setting using standard ResNet-18 models with SGD, momentum, weight decay, and data augmentation without the sphere constraint.",
    "Experimental Setup": "The primary experiments use a scale-invariant ResNet-18 model trained on the CIFAR-10 dataset. Additional experiments include SI ConvNet on CIFAR-10/CIFAR-100, SI ResNet-18 on CIFAR-100, and a plain ConvNet, a Vision Transformer (ViT) architecture on CIFAR-10/CIFAR-100, and ResNet-18 on Tiny ImageNet for practical validation. All networks are trained with SGD, batch size 128 (512 for ViT), for 200 epochs each for pre-training and fine-tuning. The synthetic example uses a 3-layer MLP with ReLU and Layer Normalization on 32-dimensional data, with 512 training and 2000 test samples, using SGD with batch size 32, and involved 50 training runs across different seeds. Practical setups utilize weight decay (5e-4), momentum (0.9), and standard augmentations (random crops, horizontal flips, per-channel normalization). Compute resources include NVIDIA TESLA V100 and A100 GPUs, totaling 1500-2000 GPU hours.",
    "Limitations": "The study is primarily empirical, lacking direct theoretical support, though it builds on related work. The findings are specific to the datasets (image or synthetic) and neural network architectures (convolutional, MLP, Vision Transformer) used, and their generalization to other settings is not guaranteed. While scale invariance is addressed, other neural network invariances, such as rescale invariance of homogeneous activations like ReLU, may have been overlooked.",
    "Future Research Directions": "Future work could explore a more precise definition and efficient practical methods for determining the 'convergence threshold' for initial LRs. A significant direction involves understanding the exact relationship between the properties of learned features (e.g., sparsity) and the characteristics of the found loss landscape basins (e.g., sharpness or convexity). This includes investigating whether feature sparsity acts as a regularizer, leading to simpler models. Further research should also investigate the complex interplay between LRs and generalization in practical scenarios with spurious features or benign memorization, aiming for a broader understanding of LR impact on trained models.",
    "Experiment Code": "File Path: nets/convnet_si.py\nContent:\ndef block(input, output):\n    module_list = [\n        nn.Conv2d(input, output, kernel_size=3, stride=1, padding=1, bias=False),\n        nn.BatchNorm2d(output, affine=False),\n        nn.ReLU(),\n        nn.MaxPool2d(2)\n    ]\n    return module_list\n\nclass ConvNetDepth(nn.Module):\n    def __init__(self, init_channels=64, num_classes=10, max_depth=3, init_scale=-1,\n                 su_init=False):\n        super(ConvNetDepth, self).__init__()\n        # ... (rest of the __init__ method for layer construction) ...\n        self.conv_layers = nn.Sequential(*module_list)\n        self.linear_layers = nn.Sequential(*linear_layer)\n        \n        if su_init:\n            self._su_init()\n\n    def _su_init(self):\n        params = [p for n, p in self.named_parameters() if check_si_name(n, 'ConvNetSI')]  # BN-ed params\n        N = sum(p.numel() for p in params)\n        pnorm = sum((p ** 2).sum() for p in params) ** 0.5\n\n        vec = torch.randn(N, device=params[0].device)\n        vec /= torch.norm(vec)\n        vec *= pnorm\n\n        tensors = unflatten_like(vec, params)\n        for param, tensor in zip(params, tensors):\n            param.data = tensor\n\nFile Path: nets/resnet_si.py\nContent:\nclass PreActBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, **kwargs):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes, affine=False)\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, affine=False)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.BatchNorm2d(in_planes, affine=False),\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n            )\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10, init_channels=64, linear_norm=10.0, linear_bias=False):\n        super(PreActResNet, self).__init__()\n        # ... (rest of the __init__ method for layer construction) ...\n        self.bn = nn.BatchNorm2d(8*c*block.expansion, affine=False)\n        # ...\n        if linear_norm > 0:\n            alpha = linear_norm\n            W = self.linear.weight.data\n            self.linear.weight.data = alpha * W / W.norm()\n\nFile Path: nets/convnet_si_ci3_wn.py\nContent:\nfrom torch.nn.utils import weight_norm\n\ndef block(input, output):\n    module_list = [\n        weight_norm(nn.Conv2d(input, output, kernel_size=3, stride=1, padding=1, bias=False)),\n        nn.ReLU(),\n        nn.MaxPool2d(2)\n    ]\n    return module_list\n\nclass ConvNetDepth(nn.Module):\n    def __init__(self, init_channels=64, num_classes=10, max_depth=3, init_scale=50.0, learn_bias=False, zero_bias=False):\n        super(ConvNetDepth, self).__init__()\n        # ... (layer definitions) ...\n        for n, param in linear_layer[-1].named_parameters():\n            if learn_bias and \"bias\" in n:\n                continue\n            param.requires_grad = False\n\n        for layer in module_list:\n            for n, param in layer.named_parameters():\n                if \"weight_g\" in n:\n                    param.requires_grad = False\n\nFile Path: training_utils.py\nContent:\ndef fix_si_pnorm(model, si_pnorm_0, model_name=\"ResNet18\"):\n    si_pnorm = np.sqrt(sum((p ** 2).sum().item() for n, p in model.named_parameters() if check_si_name(n, model_name)))\n    p_coef = si_pnorm_0 / si_pnorm\n    for n, p in model.named_parameters():\n        if check_si_name(n, model_name):\n            p.data *= p_coef\n\ndef fix_si_pnorms(model, si_pnorm_0, model_name=\"ResNet18\"):\n    for n, p in model.named_parameters():\n        if check_si_name(n, model_name):\n            si_pnorm = torch.norm_except_dim(p.data, 2, 0)\n            p_coef = si_pnorm_0 / si_pnorm\n            p.data *= p_coef\n\nFile Path: train.py\nContent:\ndef main():\n    args = parser()\n    # ... (data loading and model initialization) ...\n\n    if args.fix_elr:\n        print(\"Training with fixed ELR\")\n        # ... (parameter group definition using check_si_name) ...\n        with torch.no_grad():\n            si_pnorm_0 = np.sqrt(sum((p ** 2).sum().item() for p in param_groups[0][\"params\"]))\n            lr = args.elr * si_pnorm_0 ** 2\n    elif args.fix_all_elr:\n        print(\"Training with all fixed ELRs\")\n        # ... (parameter group definition) ...\n        with torch.no_grad():\n            si_pnorm_0 = np.sqrt(sum((p ** 2).sum().item() for p in param_groups[0][\"params\"]) /\n                                 sum(p.shape[0] for p in param_groups[0][\"params\"]))\n            training_utils.fix_si_pnorms(model, si_pnorm_0, args.model)\n            pnorm_0_sqr_total = sum((p ** 2).sum().item() for p in param_groups[0][\"params\"])\n            lr = args.elr * pnorm_0_sqr_total\n    else:\n        param_groups = model.parameters()\n        si_pnorm_0 = None\n        lr = args.lr_init\n        elr_coefs = None\n\n    optimizer = torch.optim.SGD(param_groups,\n                                lr=lr,\n                                momentum=args.momentum,\n                                weight_decay=args.wd)\n\n    # ... (learning rate scheduler setup) ...\n\n    for epoch in range(epoch_from, epoch_to+1):\n        train_epoch(model, loaders, squared_loss if args.use_squared_loss else cross_entropy, optimizer,\n                    epoch=epoch,\n                    end_epoch=epoch_to+1,\n                    eval_freq=args.eval_freq,\n                    save_freq=args.save_freq,\n                    save_freq_int=args.save_freq_int,\n                    fix_elr = args.fix_elr,\n                    fix_all_elr=args.fix_all_elr,\n                    si_pnorm_0=si_pnorm_0,\n                    output_dir=output_dir,\n                    lr_init=lr,\n                    lr_schedule=not args.no_schedule,\n                    noninvlr=args.noninvlr,\n                    c_schedule=args.c_schedule,\n                    d_schedule=args.d_schedule,\n                    fbgd=args.fbgd,\n                    cosan_schedule = args.cosan_schedule,\n                    model_name = args.model)\n\ndef train_epoch(\n    loader,\n    model,\n    criterion,\n    optimizer,\n    # ... (other parameters) ...\n):\n    # ... (training loop) ...\n    for i, (input, target) in enumerate(loader):\n        # ... (forward pass and loss calculation) ...\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if fix_elr and si_pnorm_0 is not None:\n            fix_si_pnorm(model, si_pnorm_0, model_name)\n\n        if fix_all_elr and si_pnorm_0 is not None:\n            fix_si_pnorms(model, si_pnorm_0, model_name)\n\nFile Path: get_info_funcs.py\nContent:\ndef Hvp(vec, params, outputs, data_size, targets, criterion=cross_entropy, **kwargs):\n    loss = criterion(None, outputs, targets, reduction='sum') / data_size\n    grad = torch.autograd.grad(loss, params, create_graph=True)\n    prod = 0.\n    for (g, v) in zip(grad, vec):\n        prod += (g * v).sum()\n    Hv = torch.autograd.grad(prod, params)\n    return Hv\n\ndef Fvp(vec, params, outputs, data_size, **kwargs):\n    Jv, = Rop(outputs, params, vec)\n    batch, dims = outputs.size(0), outputs.size(1)\n    probs = torch.softmax(outputs, dim=1)\n    M = torch.zeros(batch, dims, dims, device=probs.device)\n    M.view(batch, -1)[:, ::dims + 1] = probs\n    H = M - torch.einsum('bi,bj->bij', (probs, probs))\n    HJv = torch.squeeze(H @ torch.unsqueeze(Jv, -1), -1) / data_size\n    JHJv = Lop(outputs, params, HJv)\n    return JHJv\n\ndef eval_eigs(model, dataloader, fisher=True, train_mode=False, nsteps=10, return_evecs=False, pnames=None,\n              criterion=cross_entropy):\n    # ... (Lanczos algorithm to compute eigenvalues) ...\n\ndef eval_trace(model, dataloader, fisher=True, train_mode=False, n_vecs=5, pnames=None,\n               criterion=cross_entropy):\n    # ... (Computes trace of Fisher or Hessian) ...\n\ndef calc_grads(model, dataloader, criterion, train_mode=False, return_numpy=False, pnames=None):\n    # ... (Calculates batch gradients of the model) ...\n\ndef get_batch_outliers_gnorms(model, dataloader, criterion, train_mode=False, pnames=None):\n    # ... (Calculates outliers and gradient norms) ...\n\ndef calc_probs_stats(model, dataloader, train_mode=False):\n    # ... (Calculates probability statistics per each correct class) ...\n\nFile Path: fake.py\nContent:\nclass FakeData(VisionDataset):\n    def __init__(self, size=1000, image_size=(3, 224, 224), num_classes=10,\n                 transform=None, target_transform=None, random_offset=0):\n        super(FakeData, self).__init__(None, transform=transform,\n                                       target_transform=target_transform)\n        self.size = size\n        self.num_classes = num_classes\n        self.image_size = image_size\n        self.random_offset = random_offset\n        torch.manual_seed(self.random_offset)\n        self.data = torch.rand(self.size, *self.image_size)\n        self.targets = torch.randint(self.num_classes, size=(self.size,), dtype=torch.long)\n",
    "Experiment Result": "### Model Architectures and Scale-Invariance:\n- **Scale-Invariant (SI) Networks:** `ConvNetSI` and `ResNet18SI` models are designed with `nn.BatchNorm2d(..., affine=False)` to enforce scale-invariance in their Batch Normalization layers. This is explicitly noted as 'affine is False for clarity of SI experiments'.\n- **SI with Affine BN:** `ConvNetSIAf` and `ResNet18SIAf` use `nn.BatchNorm2d(..., affine=True)`, providing a comparison to fully SI models.\n- **Weight Normalization for SI:** `ConvNetSICI3WN` uses `torch.nn.utils.weight_norm` on convolutional layers, and the `weight_g` components of these layers are frozen (`param.requires_grad = False`) to maintain fixed norms and enforce scale invariance. `ConvNetSICI3WN2` uses a custom `WeightNorm` wrapper for similar functionality.\n- **Network Width and Depth:** Configurable via `num_channels` (e.g., 32) and `depth` (e.g., 3) for `ResNet18` and `ConvNet` models.\n\n### Parameter Norms and Projected SGD:\n- **Fixed Parameter Norms:** The training process supports fixing the parameter norms, especially for SI layers. This is enabled by `--fix_si_pnorm` or `--fix_all_elr` flags, which trigger `training_utils.fix_si_pnorm` or `fix_si_pnorms` to re-normalize the SI parameters after each optimizer step.\n- **Sphere Initialization:** Models can be initialized uniformly on a sphere using the `--su_init` flag, implemented in `ConvNetDepth._su_init` for `ConvNetSI` models.\n- **Last Layer Initialization Scale:** The `--init_scale` parameter (e.g., `10.0`, `50.0`) allows setting the initial norm of the last layer's weights, with an option to freeze its parameters for SI of the objective.\n\n### Learning Rate Regimes and Schedules:\n- **Fixed Effective Learning Rate (ELR):** Training can be configured with a fixed effective learning rate for SI parameters using the `--fix_elr` flag and specifying `elr` (e.g., `1e-3`). The actual learning rate for these parameters is dynamically adjusted based on the current parameter norm and the desired ELR.\n- **Separate LR for Non-SI Parameters:** The `--noninvlr` parameter allows setting a distinct learning rate for non-scale-invariant parameters (e.g., `0.0` to fix them, `-1` to train normally).\n- **Learning Rate Schedules:** Supported schedules include `no_schedule`, `c_schedule` (continuous linear decay after 1/4 epochs), `d_schedule` (discrete decay at 1/4 epoch intervals), and `cosan_schedule` (cosine annealing).\n- **Training Stages:** The method describes "
}{
    "Title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic",
    "Main Contributions": "The paper rigorously investigates the extent of dataset contamination in Large Language Models (LLMs) by evaluating their mathematical reasoning abilities. It introduces Grade School Math 1000 (GSM1k), a novel, human-generated benchmark mirroring GSM8k but guaranteed to be free of training data contamination. The study reveals that many LLMs exhibit significant overfitting on GSM8k, with accuracy drops of up to 8% on GSM1k, and demonstrates a positive correlation between a model's likelihood of generating GSM8k examples and its performance gap. However, it also finds that frontier models show minimal overfitting, and all models, even overfit ones, retain some generalizable reasoning ability.",
    "Methodology": "The core methodology involved creating GSM1k, a new dataset of 1205 grade school math problems, using only human annotators to prevent data contamination. Extensive quality checks and efforts were made to match GSM1k's difficulty distribution (human solve rates, number of steps, answer magnitude) to that of GSM8k. LLMs (a wide range of open- and closed-source models) were benchmarked on both GSM8k and GSM1k using a fork of EleutherAI's LM Evaluation Harness with 5-shot prompts from GSM8k train set. Data contamination was further investigated by correlating models' per-character log-likelihood of generating GSM8k test set examples with their performance differences between GSM8k and GSM1k.",
    "Experimental Setup": "The primary datasets used were the newly constructed GSM1k (1205 problems) and the established GSM8k benchmark (8.5k problems). A diverse set of leading open- and closed-source LLMs were evaluated, including GPT-4, Gemini, Claude, Mistral, Llama, Phi, Yi, Xwin, Gemma, CodeLlama, DBRX, Pythia, and older models like GPT-NeoX-20B and GPT-2. The evaluation was conducted using a modified LM Evaluation Harness, employing 5-shot prompting with examples from the GSM8k training set and an increased maximum token generation limit (1000). Open-source models were run at temperature 0, utilizing vLLM for accelerated inference where compatible, otherwise HuggingFace libraries. Closed-source models were queried via LiteLLM APIs. Ablations were performed to test different prompting formats and the number of few-shot examples.",
    "Limitations": "GSM1k is an approximation of GSM8k, and despite efforts, may not be identically distributed. The correlation between data contamination (measured by log-likelihood) and overfitting is positive but relatively low (Spearman's r2 = 0.36), suggesting that data contamination is not the sole explanation for overfitting, and other indirect factors might be at play. The dataset GSM1k is not publicly released yet, limiting external validation and immediate use by the broader research community. The method for estimating problem difficulty based on the number of operations is described as a 'rough estimate' and may not always correlate directly with true difficulty. Evaluation of proprietary models involved sending the dataset via API, with an acknowledgement that this theoretically carries a risk of data leakage for model training.",
    "Future Research Directions": "The paper commits to publicly releasing the GSM1k dataset once specific conditions are met (e.g., top open-source models score >95% or June 2025), which will enable further independent research on LLM reasoning and contamination. The authors plan to conduct recurring evaluations of major LLM releases and continuously update their results, along with open-sourcing their evaluation code for transparency. They suggest that future benchmark development could adopt a similar strategy of initial non-release with a pre-committed future release to prevent 'gaming'. Further research could investigate the 'indirect means' of overfitting beyond direct data contamination, such as training on similar data or benchmark-driven checkpoint selection, and delve deeper into why frontier models demonstrate stronger generalization.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Language models are multilingual chain-of-thought reasoners",
    "Main Contributions": "This research evaluates the multilingual reasoning abilities of large language models (LLMs). It introduces the Multilingual Grade School Math (MGSM) benchmark, consisting of 250 grade-school math problems manually translated into ten typologically diverse languages from the GSM8K dataset. The study finds that multilingual chain-of-thought (CoT) prompting significantly improves reasoning performance, with abilities emerging with increasing model scale. Crucially, models exhibit strikingly strong multilingual reasoning even in underrepresented languages. The research also demonstrates that these multilingual reasoning abilities extend to other tasks like commonsense reasoning (XCOPA) and word-in-context semantic judgment (XL-WiC), with PaLM-540B achieving a new state-of-the-art on XCOPA (89.9%) using few-shot CoT prompting.",
    "Methodology": "The study employs various chain-of-thought (CoT) prompting strategies, including: direct answer prediction (DIRECT), native-language CoT (NATIVE-COT), English-language CoT regardless of problem language (EN-COT), and translating the problem to English and solving with English CoT (TRANSLATE-EN). Different few-shot exemplar selection methods are explored: native question exemplars, English question exemplars, and generic multilingual question exemplars. The Google Translate API is used for the TRANSLATE-EN strategy. Evaluations are conducted using state-of-the-art large language models, specifically GPT-3 (text-davinci-002), PaLM-540B, and Codex (code-davinci-002), using greedy decoding (temperature τ = 0). The MGSM benchmark was created by professional human translators, with quality verification to ensure no machine translation usage.",
    "Experimental Setup": "The primary benchmark is the newly introduced Multilingual Grade School Math (MGSM), which contains 250 arithmetic reasoning problems (requiring 2-8 steps) manually translated from the English GSM8K dataset into ten typologically diverse languages: Bengali (BN), Chinese (ZH), French (FR), German (DE), Japanese (JA), Russian (RU), Spanish (ES), Swahili (SW), Telugu (TE), and Thai (TH), in addition to English (EN). Answers are kept consistent as Arabic numerals. Complementary evaluations are performed on XCOPA for causal commonsense reasoning (11 languages) and XL-WiC for word-in-context semantic judgment (13 languages). The performance is measured by accuracy, and analyses include the effect of model scale, language frequency in training data, and the number and type of few-shot exemplars.",
    "Limitations": "The number of few-shot exemplars for GPT-3 experiments was limited in some underrepresented languages (e.g., Russian, Chinese, Japanese, Thai, Telugu, Bengali) due to the BPE tokenization increasing token count and exceeding the GPT-3 API's 2048-token input limit. While CoT generally improved performance, it did not show improvement over direct answer prediction on the XL-WiC task, suggesting that the prompts might not be optimal or that the task itself may not require the kind of multi-step reasoning that benefits from explicit intermediate steps. The MGSM benchmark restricts answers to Arabic numerals, potentially overlooking cross-lingual numeracy for scripts employing different numeral systems.",
    "Future Research Directions": "Future research could investigate cross-lingual numeracy by mapping Arabic numerals to those of corresponding scripts in datasets like MGSM. Further exploration of optimal prompt formats for tasks like XL-WiC is warranted, alongside a deeper investigation into whether the WiC task truly benefits from the depiction of intermediate reasoning steps. The findings suggest that further scaling of large language models may continue to improve their multilingual reasoning ability. English chain-of-thought prompting is proposed as a useful baseline for future multilingual reasoning research.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
    "Main Contributions": "Introduced OpenMathInstruct-1, a 1.8 million problem-solution pair math instruction tuning dataset, which is permissively licensed and 4x larger than previous state-of-the-art datasets. Developed a novel prompting strategy utilizing masked text solutions to significantly increase training set coverage for GSM8K (99.9%) and MATH (93%). Demonstrated that models fine-tuned on this dataset, specifically OpenMath-CodeLlama-70B, achieve competitive performance (84.6% on GSM8K and 50.7% on MATH) comparable to or better than GPT-distilled counterparts. Publicly released the dataset, fine-tuned models, and code to support open-source efforts in mathematical reasoning.",
    "Methodology": "The dataset was constructed by synthesizing code-interpreter style solutions for GSM8K and MATH benchmarks using the Mixtral-base LLM. The solution format interweaves natural language reasoning with Python code blocks. Few-shot prompting (K=5 examples) was employed, with a key innovation being 'masked text solution prompting'. This technique involves masking intermediate numerical computations in reference text solutions with symbols (e.g., M, N) to prevent LLMs from generating shortcut answers by copying. Solution sampling was performed with a temperature of 1.0 and top_p of 0.95, subject to token and code block limits. Post-processing steps included removing syntactically noisy solutions (e.g., multiple answer blocks, incomplete code). For fine-tuning, data selection strategies included 'fair downsampling' for balanced problem representation and 'Any-Code' filtering, which prioritizes code-based solutions over text-based ones for precision.",
    "Experimental Setup": "Models based on Mistral-7B, Llama 2 (7B, 13B, 70B), and CodeLlama (7B, 13B, 34B, 70B) were fine-tuned. The training corpus consisted of a mixture of 512K fair downsampled GSM8K instances and 512K MATH instances (with Any-Code filtering), totaling approximately 1.02 million problem-solution pairs. Training typically ran for 4 epochs for models up to 34B parameters and 2-3 epochs for 70B models, using a global batch size of 128, AdamW optimizer (weight decay 1e-2), and dropout of 0.1, implemented with the NeMo toolkit. Evaluation was conducted in a zero-shot setting on popular mathematical reasoning benchmarks: GSM8K, MATH, GSM-Hard, SV AMP, TabMWP, ASDiv, and MAWPS. A 1K-sized validation set from GSM8K and MATH training data was used for ablation studies and hyperparameter selection. Both greedy decoding and self-consistency (k=50 samples, temperature 0.7) were used for evaluation.",
    "Limitations": "The presence of 'semantically noisy solutions' (correct answers derived from flawed reasoning) that are hard to filter presents a quality control challenge for the synthetic dataset. Performance improvements on in-domain math benchmarks may not consistently translate to other related tasks, as evidenced by a performance drop on GSM-Hard compared to GSM, suggesting a lack of robustness to input perturbations. There's an inherent risk in using these imperfect datasets and models for sensitive applications like student assignment evaluation or math tutoring. Additionally, the high computational cost of fine-tuning runs limited the ability to perform multiple runs for comprehensive statistical significance analysis (e.g., error bars).",
    "Future Research Directions": "Future work could focus on developing more sophisticated semantic filters to detect and eliminate solutions generated with flawed reasoning. Another direction involves exploring methods for LLMs to recover more effectively from errors during code generation, such as when initial code blocks are incorrect. Research could also aim to improve the generalization and robustness of models to various input perturbations and to tasks beyond the scope of the current in-domain mathematical benchmarks. Further investigation into balancing solution diversity with performance gains from code-preferential strategies is also a promising avenue.",
    "Experiment Code": "File Path: nemo_skills/dataset/gsm8k/prepare.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.S\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport os\nimport re\nimport urllib.request\nfrom pathlib import Path\n\nURL = \"https://raw.githubusercontent.com/openai/grade-school-math/master/grade_school_math/data/{}.jsonl\"\n\n\n# some of the training samples have incorrect answers - fixing the ones we know about here\nfixes = {\n    \"\"\"Mr. Finnegan has 3 tanks with a capacity of 7000 gallons, 5000 gallons, and 3000 gallons, respectively. If he fills the first tank up to 3/4 full, the second tank with water up to 4/5 of its capacity, and the third tank up to half of its capacity, how many gallons in total are in the tanks?\"\"\": 10750,\n    \"\"\"If you double a number and add 5 to the result, then that's 20 more than half of the original number. What's the original number?\"\"\": 10,\n    \"\"\"John has 2 hives of bees.  One of the hives has 1000 bees and produces 500 liters of honey.  The second has 20% fewer bees but each bee produces 40% more honey.  How much honey does he produce?\"\"\": 2740,\n    \"\"\"Three blue chips are in a jar which is 10% of the entire chips. If 50% of the chips are white and the rest are green, how many green chips are there?\"\"\": 15,\n    \"\"\"Janet filmed a new movie that is 60% longer than her previous 2-hour long movie.  Her previous movie cost $50 per minute to film, and the newest movie cost twice as much per minute to film as the previous movie.  What was the total amount of money required to film Janet's entire newest film?\"\"\": 19200,\n    \"\"\"Students at Highridge High earn 2 points for each correct answer during a quiz bowl If a student correctly answers all the questions in a round, the student is awarded an additional 4 point bonus. They played a total of five rounds each consisting of five questions. If James only missed one question, how many points did he get?\"\"\": 64,\n    \"\"\"Robert and Teddy are planning to buy snacks for their friends.  Robert orders five boxes of pizza at $10 each box and ten cans of soft drinks at $2 each. Teddy buys six hamburgers at $3 each and an additional ten cans of soft drinks. How much do they spend in all?\"\"\": 108,\n    \"\"\"James invests $2000 a week into his bank account.  He had $250,000 in his account when the year started.  At the end of the year, he gets a windfall that is worth 50% more than what he has in his bank account.   How much money does he have?\"\"\": 531000,\n    \"\"\"James does chores around the class.  There are 3 bedrooms, 1 living room, and 2 bathrooms to clean.  The bedrooms each take 20 minutes to clean.  The living room takes as long as the 3 bedrooms combined.  The bathroom takes twice as long as the living room.  He also cleans the outside which takes twice as long as cleaning the house.  He splits the chores with his 2 siblings who are just as fast as him.  How long, in hours, does he work?\"\"\": 6,\n    \"\"\"During one game, a total of 50 people attended a baseball team’s games. Forty percent and thirty-four percent of the audiences are supporters of the first and second teams, respectively. How many people attended the game did not support either of the teams?\"\"\": 13,\n    \"\"\"Pete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\"\"\": \"07:30\",\n}\n\n\ndef save_data(split):\n    actual_split = \"test\" if split == \"test\" else \"train\"\n    data_dir = Path(__file__).absolute().parent\n    original_file = str(data_dir / f\"original_{actual_split}.jsonl\")\n    data_dir.mkdir(exist_ok=True)\n    output_file = str(data_dir / f\"{split}.jsonl\")\n\n    urllib.request.urlretrieve(URL.format(actual_split), original_file)\n\n    data = []\n    with open(original_file, \"rt\", encoding=\"utf-8\") as fin:\n        for line in fin:\n            original_entry = json.loads(line)\n            new_entry = {}\n            # mapping to the required naming format\n            new_entry[\"problem\"] = original_entry[\"question\"]\n            solution, expected_answer = original_entry[\"answer\"].split(\"####\")\n            new_entry[\"expected_answer\"] = float(expected_answer.replace(\",\", \"\"))\n            # converting to int if able to for cleaner text-only representation\n            if int(new_entry[\"expected_answer\"]) == new_entry[\"expected_answer\"]:\n                new_entry[\"expected_answer\"] = int(new_entry[\"expected_answer\"])\n            # removing redundant computations\n            new_entry[\"reference_solution\"] = re.sub(r\"<<.*?>>\", \"\", solution)\n            # fixing some of the errors in the training set\n            if original_entry[\"question\"] in fixes:\n                new_entry[\"expected_answer\"] = fixes[original_entry[\"question\"]]\n            data.append(new_entry)\n\n    with open(output_file, \"wt\", encoding=\"utf-8\") as fout:\n        for entry in data:\n            fout.write(json.dumps(entry) + \"\\n\")\n\n    # cleaning up original data file\n    os.remove(original_file)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--split\",\n        default=\"all\",\n        choices=(\"all\", \"test\", \"train\"),\n    )\n    args = parser.parse_args()\n\n    if args.split == \"all\":\n        for split in [\"test\", \"train\"]:\n            save_data(split)\n    else:\n        save_data(args.split)\n\nFile Path: nemo_skills/dataset/hendrycks_math/prepare.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\n\nfrom nemo_skills.dataset.utils import save_data_from_qwen\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--split\",\n        default=\"all\",\n        choices=(\"all\", \"test\", \"train\"),\n    )\n    args = parser.parse_args()\n\n    if args.split == \"all\":\n        for split in [\"test\", \"train\"]:\n            save_data_from_qwen(\"math\", split)\n    else:\n        save_data_from_qwen(\"math\", args.split)\n\nFile Path: nemo_skills/dataset/hendrycks_math/fix_ref_solns.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\n\ndef _post_fix(problem_id, soln_string):\n    \"\"\"Post fixing some answer strings\"\"\"\n    if problem_id == \"test/intermediate_algebra/78.json\":\n        soln_string = re.sub(r\"\\\\(\\d+)\", r\"\\\\1\", soln_string)\n\n    if problem_id == \"train/number_theory/7115.json\":\n        return \"A\"\n\n    if problem_id == \"train/number_theory/1012.json\":\n        return \"E\"\n\n    if problem_id == \"train/prealgebra/666.json\":\n        return \"125\"\n\n    if problem_id == \"train/intermediate_algebra/172.json\":\n        return \"two lines\"\n\n    if problem_id == \"train/prealgebra/1691.json\":\n        return \"1.85\"\n\n    if problem_id == \"train/geometry/6177.json\":\n        return \"C\"\n\n    if problem_id == \"train/number_theory/7117.json\":\n        return \"A\"\n\n    if problem_id == \"train/geometry/6202.json\":\n        return \"D\"\n\n    if problem_id == \"train/precalculus/268.json\":\n        return \"A\"\n\n    return soln_string\n\n\ndef _post_fix_multi_answer(problem_id, results):\n    \"\"\"Fixing cases where there are multiple boxed entries.\"\"\"\n\n    if problem_id == \"train/prealgebra/452.json\":\n        # Two ptions are mathematically equivalent\n        return results[0]\n\n    if problem_id == \"train/algebra/1771.json\":\n        return \";\".join(results)\n\n    if problem_id == \"train/algebra/152.json\":\n        return \";\".join(results)\n\n    if problem_id == \"train/algebra/2156.json\":\n        # Same fraction but without antlr-4.11 can't verify if the two are equal\n        return results[-1]\n\n    if problem_id == \"train/intermediate_algebra/1609.json\":\n        # Same fraction but without antlr-4.11 can't verify if the two are equal\n        return results[-1]\n\n    if problem_id == \"train/precalculus/865.json\":\n        # The question has 2 answers, we just choose the last answer for now\n        # TODO - Fix handling of OR questions\n        return results[-1]\n\n    if problem_id == \"train/precalculus/982.json\":\n        # Question has many answers since it's a phase shift question.\n        # Choosing the rightmost answer for now.\n        # TODO - Fix handling of OR questions\n        return results[-1]\n\n    if problem_id == \"train/precalculus/1149.json\":\n        # This question has two solutions, each being 30,150. We can pick any of the results entries\n        return results[-1]\n\n    if problem_id == \"train/number_theory/837.json\":\n        # The two answers are for 24 hr clock vs 12 hr clock. Choosing the 24 version\n        return results[0]\n\n    if problem_id == \"train/intermediate_algebra/396.json\":\n        # Or question, picking the rightmost answer\n        return results[-1]\n\n    if problem_id == \"train/counting_and_probability/955.json\":\n        # The first boxed entry is an intermediate step\n        return results[-1]\n\n    # Test set fixes\n    if problem_id == \"test/prealgebra/1088.json\":\n        # Two solutions are mathematically equivalent\n        return results[0]\n\n    if problem_id == \"test/algebra/1197.json\":\n        # The first entry is an intermediate result\n        return results[-1]\n\n    if problem_id == \"test/geometry/66.json\":\n        # The two entries are same, choosing the first one expressed in frac\n        return results[0]\n\n    if problem_id == \"test/geometry/1125.json\":\n        # Both are 0.25, choosing the first one\n        return results[0]\n\n    if problem_id == \"test/prealgebra/1407.json\":\n        # There are intermediate values which are not answers\n        return results[-1]\n\n    if problem_id == \"test/prealgebra/224.json\":\n        # Two answers are same, choosing rightmost\n        return results[-1]\n\n    if problem_id == \"test/prealgebra/177.json\":\n        # The answer is 12 the last entry\n        return results[-1]\n\n    if problem_id == \"test/number_theory/459.json\":\n        # Two answers are same, choosing rightmost\n        return results[-1]\n\n    if problem_id == \"test/intermediate_algebra/702.json\":\n        # OR question. Choosing the rightmost answer\n        return results[-1]\n\n    if problem_id == \"test/intermediate_algebra/25.json\":\n        # OR question. Choosing the rightmost answer\n        return results[-1]\n\n    if problem_id == \"test/intermediate_algebra/747.json\":\n        # OR question. Choosing the rightmost answer\n        return results[-1]\n\n    return \",\".join(results)\n\n\ndef _fix_solution(problem_id, ref_soln):\n    if problem_id == \"train/algebra/24014.json\":\n        return ref_soln.replace(\"\\\\boxed 2\", \"\\\\boxed{2}\")\n\n    if problem_id == \"train/algebra/25040.json\":\n        return ref_soln.replace(\"\\\\boxed 9$\", \"\\\\boxed{9}$\")\n\n    if problem_id == \"train/algebra/535.json\":\n        # Original soln\n        # $\\\\log_24=\\\\boxed{2}$, so $\\\\log_2(4^2) = \\\\log_2((2^2)^2) = \\\\log_2 (2^4) = \\\\boxed{4}$\n        return \"$\\\\log_24=2$, so $\\\\log_2(4^2) = \\\\log_2((2^2)^2) = \\\\log_2 (2^4) = \\\\boxed{4}$\"\n\n    if problem_id == \"train/geometry/892.json\":\n        return ref_soln.replace(\"\\\\boxed{144}\", \"144\")\n\n    if problem_id == \"train/number_theory/7041.json\":\n        return ref_soln.replace(\"\\\\boxed{j-i \\\\\\equiv 0 \\\\pmod{6}}\", \"j-i \\\\\\equiv 0 \\\\pmod{6}\")\n\n    if problem_id == \"train/intermediate_algebra/1266.json\":\n        return ref_soln.replace(\n            \"(x^2-\\\\boxed{\\\\phantom{09}})(x^2-\\\\boxed{\\\\phantom{25}})\", \"(x^2-\\\\phantom{09})(x^2-\\\\phantom{25})\"\n        )\n\n    return ref_soln\n\nFile Path: nemo_skills/dataset/utils.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport contextlib\nimport importlib\nimport json\nimport os\nimport sys\nimport tempfile\nimport time\nimport urllib.request\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Dict\nfrom urllib.error import URLError\n\nfrom nemo_skills.evaluation.math_grader import extract_answer\nfrom nemo_skills.pipeline.utils import cluster_download_file, get_unmounted_path\n\n\n@contextlib.contextmanager\ndef add_to_path(p):\n    old_path = sys.path\n    sys.path = sys.path[:]\n    sys.path.insert(0, str(p))\n    try:\n        yield\n    finally:\n        sys.path = old_path\n\n\ndef add_rounding_instruction(data: Dict) -> Dict:\n    try:\n        float(data[\"expected_answer\"])\n        number_of_values = 0\n        if \".\" in str(data[\"expected_answer\"]):\n            number_of_values = len(str(data[\"expected_answer\"]).split(\".\")[1])\n        if number_of_values == 0:\n            data[\"problem\"] += \" Express the answer as an integer.\"\n        elif number_of_values == 1:\n            data[\"problem\"] += \" Round the answer to one decimal place.\"\n        else:\n            data[\"problem\"] += f\" Round the answer to {number_of_values} decimal places.\"\n    except ValueError:\n        pass\n    return data\n\n\ndef import_from_path(file_path, module_name=None):\n    if module_name is None:  # unique random name\n        module_name = f\"dynamic_module_{int(time.time() * 1000)}\"\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n    return module\n\n\nclass ExtraDatasetType(str, Enum):\n    local = \"local\"\n    cluster = \"cluster\"\n\n\ndef _get_dataset_module_from_cluster(cluster_config, mounted_path):\n    # getting tmp path to download init.py\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmp_path = str(Path(tmpdir) / \"init.py\")\n        cluster_dataset_path = get_unmounted_path(cluster_config, mounted_path)\n        try:\n            cluster_download_file(cluster_config, cluster_dataset_path, tmp_path)\n        except FileNotFoundError:\n            raise RuntimeError(\n                f\"Init file {mounted_path} not found on the cluster. \"\n                f\"Please check the dataset name you're using. Did you forget to run prepare data commands?\"\n            )\n        return import_from_path(tmp_path)\n\n\ndef get_default_dataset_module(dataset, data_dir=None, cluster_config=None):\n    is_on_cluster = False\n    if data_dir is None:\n        data_path = \"/nemo_run/code/nemo_skills/dataset\"\n        dataset_module = importlib.import_module(f\"nemo_skills.dataset.{dataset}\")\n    else:\n        data_path = data_dir\n        if cluster_config is None or cluster_config[\"executor\"] == \"none\":\n            with add_to_path(data_dir):\n                dataset_module = importlib.import_module(dataset)\n        else:\n            if cluster_config[\"executor\"] == \"local\":\n                with add_to_path(get_unmounted_path(cluster_config, data_dir)):\n                    dataset_module = importlib.import_module(dataset)\n            else:\n                dataset = dataset.replace(\".\", \"/\")\n                dataset_module = _get_dataset_module_from_cluster(cluster_config, f\"{data_dir}/{dataset}/__init__.py\")\n                is_on_cluster = True\n    return dataset_module, data_path, is_on_cluster\n\n\ndef get_dataset_module(dataset, data_dir=None, cluster_config=None, extra_datasets=None, extra_datasets_type=None):\n    \"\"\"\n    Get dataset module either in default folder or in extra datasets folder.\n\n    If cluster_config is provided, the data_dir will be resolved as a mounted\n    path and appropriately downloaded from the cluster.\n\n    Same will be done for extra_datasets if extra_datasets_type is cluster.\n\n    Search priority:\n    1. data_dir (or `nemo_skills.dataset` if None) folder\n    3. extra_datasets parameter if defined\n    4. `NEMO_SKILLS_EXTRA_DATASETS` environment variable\n    \"\"\"\n    try:\n        dataset_module, data_path, is_on_cluster = get_default_dataset_module(dataset, data_dir, cluster_config)\n    except ModuleNotFoundError:\n        try:\n            dataset = dataset.replace(\".\", \"/\")\n            extra_datasets = extra_datasets or os.environ.get(\"NEMO_SKILLS_EXTRA_DATASETS\")\n            is_on_cluster = False\n            data_path = extra_datasets\n            if extra_datasets is None:\n                raise RuntimeError(f\"Dataset {dataset} not found in {data_dir if data_dir else 'nemo_skills.dataset'}\")\n            if extra_datasets_type == ExtraDatasetType.local:\n                with add_to_path(extra_datasets):\n                    dataset_module = importlib.import_module(dataset)\n            else:\n                dataset_module = _get_dataset_module_from_cluster(\n                    cluster_config, f'{extra_datasets}/{dataset}/\"__init__.py\"'\n                )\n                is_on_cluster = True\n        except ModuleNotFoundError:\n            raise RuntimeError(\n                f\"Dataset {dataset} not found in any of the searched locations: \"\n                f\"{data_dir if data_dir else 'nemo_skills.dataset'}, {extra_datasets}\"\n            )\n    return dataset_module, data_path, is_on_cluster\n\n\ndef get_lean4_header():\n    LEAN4_HEADER = \"import Mathlib\\n\\nimport Aesop\\n\\nset_option maxHeartbeats 0\\n\\nopen Topology Filter Real Complex TopologicalSpace Finset Function Metric Nat Rat\\nopen scoped BigOperators Matrix\\n\\n\"\n    return LEAN4_HEADER\n\n\ndef add_header_to_jsonl_inplace(jsonl_path, header):\n    \"\"\"\n    Adds or updates the header field for all entries in a JSONL file.\n\n    Args:\n        jsonl_path (str): Path to the JSONL file.\n        header (str): The header string to add or update in each entry.\n    \"\"\"\n    with open(jsonl_path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n\n    with open(jsonl_path, \"w\", encoding=\"utf-8\") as file:\n        for line in lines:\n            data = json.loads(line)\n            data[\"header\"] = header\n            file.write(json.dumps(data) + \"\\n\")\n\n\ndef download_with_retries(url, output_file, max_retries=3, retry_delay=1):\n    \"\"\"Download a file with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            urllib.request.urlretrieve(url, output_file)\n            return True\n        except URLError as e:\n            if attempt == max_retries - 1:\n                raise RuntimeError(f\"Failed to download after {max_retries} attempts: {e}\")\n            time.sleep(retry_delay * (attempt + 1))\n    return False\n\n\ndef save_data_from_qwen(dataset, split=\"test\"):\n    url = (\n        \"https://raw.githubusercontent.com/QwenLM/Qwen2.5-Math/refs/heads/main/evaluation/data/{dataset}/{split}.jsonl\"\n    )\n\n    data_dir = Path(__file__).absolute().parent\n    ns_dataset = dataset if dataset != \"math\" else \"hendrycks_math\"\n    original_file = str(data_dir / ns_dataset / f\"original_{split}.json\")\n    data_dir.mkdir(exist_ok=True)\n    output_file = str(data_dir / ns_dataset / f\"{split}.jsonl\")\n    data = []\n    if not os.path.exists(original_file):\n        formatted_url = url.format(split=split, dataset=dataset)\n        download_with_retries(formatted_url, original_file)\n\n    with open(original_file, \"rt\", encoding=\"utf-8\") as fin:\n        for line in fin:\n            entry = json.loads(line)\n\n            if \"answer\" in entry:\n                entry[\"expected_answer\"] = entry.pop(\"answer\")\n\n            if \"problem\" not in entry:\n                entry[\"problem\"] = entry.pop(\"question\")\n\n            if dataset == \"olympiadbench\":\n                entry[\"expected_answer\"] = entry.pop(\"final_answer\")[0].strip(\"$\")\n\n            if dataset == \"minerva_math\":\n                entry[\"expected_answer\"] = extract_answer(entry[\"solution\"])\n\n            data.append(entry)\n\n    with open(output_file, \"wt\", encoding=\"utf-8\") as fout:\n        for entry in data:\n            fout.write(json.dumps(entry) + \"\\n\")\n\n    # cleaning up original data file\n    os.remove(original_file)\n\n    return output_file\n\n\ndef get_mcq_fields(question, choices):\n    options_dict = {chr(ord(\"A\") + i): option for i, option in enumerate(choices)}\n    options_text = \"\\n\".join(f\"{letter}) {option}\" for letter, option in options_dict.items())\n    question = question.strip(\"\\n\")\n    return {\n        \"problem\": f\"{question}\\n\\n{options_text}\",\n        \"options\": options_text,\n        **options_dict,\n    }\n\nFile Path: nemo_skills/inference/generate.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport json\nimport logging\nimport random\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom copy import deepcopy\nfrom dataclasses import asdict, field, is_dataclass\nfrom pathlib import Path\nfrom typing import Any\n\nimport hydra\nimport litellm\nfrom omegaconf import ListConfig\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nfrom nemo_skills.code_execution.sandbox import get_sandbox, sandbox_params\nfrom nemo_skills.evaluation.evaluator import (\n    evaluate,\n    get_evaluator_class,\n    supports_single_eval,\n)\nfrom nemo_skills.inference.model import (\n    ParallelThinkingConfig,\n    get_code_execution_model,\n    get_model,\n    get_parallel_thinking_model,\n    get_tool_calling_model,\n    server_params,\n)\nfrom nemo_skills.inference.model.base import EndpointType\nfrom nemo_skills.prompt.utils import get_prompt, get_token_count\nfrom nemo_skills.utils import (\n    chunk_data,\n    get_help_message,\n    get_logger_name,\n    get_server_wait_cmd,\n    nested_dataclass,\n    parse_reasoning,\n    setup_logging,\n)\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\n@nested_dataclass(kw_only=True)\nclass InferenceConfig:\n    # Type of completion to generate when using OpenAI\n    # \"chat\": used by default\n    # \"text\": for text completions, in this case we will\n    # take the tokenizer from the model and apply it to the prompt before sending it.\n    # You can override tokenizer with tokenizer parameter.\n    # \"responses\": for responses api format.\n    endpoint_type: EndpointType = EndpointType.chat\n    temperature: float = 0.0  # Temperature of 0 means greedy decoding\n    top_k: int = -1\n    top_p: float = 0.95\n    min_p: float = 0.0\n    random_seed: int = 0\n    tokens_to_generate: int | None = None\n    repetition_penalty: float = 1.0\n    top_logprobs: int | None = None\n    timeout: int | None = 14400  # Timeout for each individual LLM call in seconds\n\n    extra_body: dict = field(default_factory=dict)  # Any other extra params passed with extra_body argument\n\n\n@nested_dataclass(kw_only=True)\nclass GenerateSolutionsConfig:\n    \"\"\"Generation parameters.\"\"\"\n\n    input_file: str  # Path to the input file with data\n    output_file: str  # Where to save the generations\n    prompt_config: str | None = None  # How to format the data into prompts\n\n    # Deprecated, please use endpoint_type in the InferenceConfig instead\n    use_completions_api: bool = False\n\n    # path or name of the tokenizer to use for completions API. By default uses server.model\n    tokenizer: str | None = None\n    # extra parameters to pass to the tokenizer's apply_chat_template method\n    chat_template_kwargs: dict = field(default_factory=dict)\n    # to specify the format of the prompt, \"ns\" for Nemo-Skills format or \"openai\" for OpenAI chat format\n    prompt_format: str = \"ns\"\n    prompt_suffix: str = \"\"  # suffix to add to the prompt, e.g. \" /no_think\"\n    system_message: str | None = None  # can override the default system message in the config\n    code_tags: str | None = None  # required when using code execution\n    examples_type: str | None = None  # to be able to customize few-shot examples\n\n    # Inference server configuration {server_params}\n    server: dict = field(default_factory=dict)\n    # Sandbox configuration {sandbox_params}\n    sandbox: dict = field(default_factory=dict)\n    wait_for_sandbox: bool = False  # whether we need to wait for sandbox\n    # Prompt configuration - path to yaml files\n    start_assistant_response_key: str | None = None  # whether to start assistant response with this key\n\n    inference: InferenceConfig = field(default_factory=InferenceConfig)  # LLM call parameters\n\n    max_samples: int = -1  # If > 0, will stop after generating this many samples. Useful for debugging\n    skip_filled: bool = False  # If True, will skip the generations that are already in the output file\n\n    # maximum number of concurrent requests to the server for the async loop\n    # if sync loop is used, this is the batch size\n    max_concurrent_requests: int = 512\n    # chunk the dataset into equal sized parts and index into them\n    num_chunks: int | None = None  # if specified, will split the data into chunks and only generate for one chunk\n    chunk_id: int | None = None  # if specified, will index the specified chunk only\n\n    # if False, will not add num_generated_tokens and generation_time values.\n    # Useful when running judge jobs to keep the original generation statistics\n    add_generation_stats: bool = True\n\n    # Count the number of tokens in the prompt\n    count_prompt_tokens: bool = False\n\n    generation_key: str = \"generation\"\n\n    async_position_key: str = \"_async_position\"  # key to use for preserving position in async loop in data dict\n\n    # can add this flag to just print the first prompt instead of running generation\n    # useful to double check that your data can be loaded and prompt has what you expect\n    dry_run: bool = False\n\n    # set to True if code execution needs to be supported\n    code_execution: bool = False\n    # Controls how many code executions are allowed in prompt (useful for models that support dynamically setting this)\n    # if total_code_executions placeholder is not in the prompt, this parameter has no effect\n    # Can be int, (min,max) tuple, or None\n    # If (min,max) tuple, will be randomly sampled from random.randint(min_val, max_val) for each sample in a batch\n    # useful to generate data with variable number of total_code_executions_in_prompt\n    total_code_executions_in_prompt: Any = None\n    # When True, total_code_executions_in_prompt override model defaults\n    override_max_code_executions: bool = False\n\n    # stop phrase for llms\n    stop_phrase: str | None = None  # if None, will not add any extra stop phrase\n\n    # parallel_thinking config\n    parallel_thinking: ParallelThinkingConfig = field(default_factory=ParallelThinkingConfig)\n\n    # Module-based tool configuration\n    #   List of tool provider locators using double-colon syntax for the tool class.\n    #   Each item should be of the form:\n    #     - Module class:  module.path.to.provider::ClassName\n    #     - File class:    /abs/or/rel/path/to/provider.py::ClassName\n    #\n    #   Examples:\n    #     - ++tool_modules=[\"nemo_skills.mcp.servers.python_tool::PythonTool\"]\n    #     - ++tool_modules=[\"/nemo_run/code/mcp/example_tool.py::ExampleTool\",\"nemo_skills.mcp.servers.exa_tool::ExaTool\"]\n    tool_modules: list[str] | None = None\n    #\n    #   Per-tool overrides keyed by the Tool class name (the same ClassName used above).\n    #   Use dotted keys to set nested values (e.g., client_params.base_url).\n    #\n    #   Common patterns:\n    #     - Set PythonTool timeout knob:\n    #         ++tool_overrides.PythonTool.exec_timeout_s=30\n    #     - Set an ExampleTool server-only arg:\n    #         ++tool_overrides.ExampleTool.foo_argument='[TEST] '\n    tool_overrides: dict | None = field(default_factory=dict)\n\n    # if True, will move full generation to _full_generation key and keep cfg.generation_key without thinking tokens\n    # IMPORTANT: do not set this for non-reasoning models as it will make the generations empty!\n    parse_reasoning: bool = False\n    end_reasoning_string: str = \"</think>\"\n\n    # If True, will enable litellm disk cache (useful for keeping intermediate results in case of job timelimit failures)\n    enable_litellm_cache: bool = False\n\n    # Evaluation setup if requested. If eval_type is set to None, evaluation is skipped\n    eval_type: str | None = None  # \"lean4-proof\", \"math\", etc.\n    eval_config: dict = field(default_factory=dict)  # Config for the evaluator\n\n    def __post_init__(self):\n        self._post_init_validate_data()\n        self._post_init_validate_server()\n        self._post_init_validate_params()\n        self._post_init_deprecated_params()\n\n    def _post_init_validate_data(self):\n        if isinstance(self.total_code_executions_in_prompt, ListConfig):\n            self.total_code_executions_in_prompt = list(self.total_code_executions_in_prompt)\n\n        if self.total_code_executions_in_prompt is not None and not isinstance(\n            self.total_code_executions_in_prompt, (int, list, tuple)\n        ):\n            raise ValueError(\n                \"`total_code_executions_in_prompt` must be either int, list, tuple, or None, \"\n                f\"got {type(self.total_code_executions_in_prompt)}\"\n            )\n\n    def _post_init_validate_server(self):\n        if self.server[\"server_type\"] == \"megatron\":\n            LOG.warning(\"Megatron inference is extremely slow. It's highly recommended to use other server types!\")\n\n    def _post_init_validate_params(self):\n        \"\"\"Validate that certain parameters are restricted to certain values\"\"\"\n        if self.prompt_format not in [\"ns\", \"openai\"]:\n            raise ValueError(f\"prompt_format must be either 'ns' or 'openai', got '{self.prompt_format}'\")\n\n        if self.prompt_format == \"openai\":\n            assert self.prompt_config is None, \"prompt_config is not supported for prompt_format == 'openai'\"\n        else:\n            assert self.prompt_config is not None, \"prompt_config is required when prompt_format == 'ns'\"\n        for param, default_value in self._get_disallowed_params():\n            if getattr(self, param) != default_value:\n                raise ValueError(f\"{param} must be {default_value}\")\n\n    def _post_init_deprecated_params(self):\n        if self.use_completions_api:\n            raise ValueError(\"use_completions_api is deprecated, please use ++inference.endpoint_type=text instead.\")\n\n    def _get_disallowed_params(self):\n        \"\"\"Returns a list of parameters with their default values to check that they are not changed from the defaults\"\"\"\n        return []\n\n\ncs = hydra.core.config_store.ConfigStore.instance()\ncs.store(name=\"base_generation_config\", node=GenerateSolutionsConfig)\n\n\nclass GenerationTask:\n    @classmethod\n    def get_generation_default_args(cls) -> str:\n        \"\"\"\n        Returns the default arguments for the generation task.\n        Override this method to customize the default arguments.\n\n        Returns:\n            Dict: Default arguments for the generation task.\n        \"\"\"\n        return \"\"\n\n    @classmethod\n    def get_server_command_fn(cls) -> callable:\n        \"\"\"\n        Returns the function to get the server command for the generation task.\n        Override this method to customize the server command function.\n\n        Returns:\n            callable: Function that returns the server command.\n        \"\"\"\n        from nemo_skills.pipeline.utils import get_server_command\n\n        return get_server_command\n\n    def __init__(self, cfg: GenerateSolutionsConfig):\n        \"\"\"\n        Class that represents a generation task. It implements a template of steps to generate solutions using LLMs.\n        Individual functions can be overriden to customize the behavior of the generation task.\n\n        Args:\n            cfg: GenerateSolutionsConfig object with the configuration parameters or subclass.\n        \"\"\"\n        self.cfg = cfg\n        self.cfg.inference.extra_body = dict(self.cfg.inference.extra_body)\n\n        # chat template kwargs goes either into extra body of inference or as a prompt parameter\n        if self.cfg.chat_template_kwargs:\n            if self.cfg.inference.endpoint_type != EndpointType.text:\n                if \"chat_template_kwargs\" in self.cfg.inference.extra_body:\n                    raise ValueError(\n                        \"chat_template_kwargs is provided in both inference.extra_body and as a separate argument. \"\n                        \"You can only use one of them!\"\n                    )\n\n                self.cfg.inference.extra_body[\"chat_template_kwargs\"] = dict(self.cfg.chat_template_kwargs)\n                self.cfg.chat_template_kwargs = None\n\n        if self.cfg.inference.extra_body.get(\"chat_template_kwargs\"):\n            if self.cfg.chat_template_kwargs:\n                raise ValueError(\n                    \"chat_template_kwargs is provided in both inference.extra_body and as a separate argument. \"\n                    \"You can only use one of them!\"\n                )\n            if self.cfg.inference.endpoint_type == EndpointType.text:\n                self.cfg.chat_template_kwargs = self.cfg.inference.extra_body.pop(\"chat_template_kwargs\")\n\n        # Setup tokenizer\n        if (\n            self.cfg.inference.endpoint_type == EndpointType.text\n            or self.cfg.server.get(\"enable_soft_fail\", False)\n            or self.cfg.count_prompt_tokens\n        ):\n            # These are the only cases where we need a tokenizer\n            self.tokenizer = self.cfg.tokenizer or self.cfg.server[\"model\"]\n        else:\n            self.tokenizer = None\n\n        # Setup litellm cache\n        self.setup_litellm_cache()\n\n        if self.cfg.inference.endpoint_type == EndpointType.text and self.cfg.inference.tokens_to_generate is None:\n            raise ValueError(\"When using completions API, tokens_to_generate must be specified!\")\n\n        # Setup prompt formatter and LLM\n        self.prompt = self.setup_prompt()\n        self.llm = self.setup_llm()\n\n        # Setup hf_tokenizer for counting prompt tokens\n        self.hf_tokenizer = None\n        if self.cfg.count_prompt_tokens:\n            self.hf_tokenizer = AutoTokenizer.from_pretrained(self.tokenizer)\n\n            if self.hf_tokenizer is None:\n                raise ValueError(\"Tokenizer could not be initialized. Needed for counting prompt tokens.\")\n\n        if self.cfg.code_execution:\n            self.extra_generate_params = self.prompt.get_code_execution_args()\n        else:\n            self.extra_generate_params = {}\n\n        # Setup evaluator if specified\n        self.should_run_evaluation = self.cfg.eval_type is not None\n        self.evaluator = None\n        if self.should_run_evaluation:\n            self.cfg.eval_config = dict(self.cfg.eval_config)\n            if supports_single_eval(self.cfg.eval_type, self.cfg.eval_config):\n                LOG.info(\"Evaluator supports per-datapoint evals, will interleave evaluation with generation.\")\n                self.evaluator = get_evaluator_class(self.cfg.eval_type, self.cfg.eval_config)\n\n        LOG.info(\n            \"Async loop is maintaining %d generations in parallel. \"\n            \"Use max_concurrent_requests to control the number of concurrent requests.\",\n            self.cfg.max_concurrent_requests,\n        )\n\n        # Initialize semaphore for controlling concurrent requests\n        if self.cfg.parallel_thinking.mode is not None:\n            # Each request will generate multiple solutions, so we need to divide the semaphore by the parallel requests\n            # Some models (like NIM speech models) don't have cfg attribute\n            if hasattr(self.llm, \"cfg\"):\n                divisor = self.llm.cfg.max_concurrent_requests\n            else:\n                divisor = 1\n            self.semaphore = asyncio.Semaphore(self.cfg.max_concurrent_requests // divisor)\n        else:\n            self.semaphore = asyncio.Semaphore(self.cfg.max_concurrent_requests)\n\n        # output_lock will be initialized when async_loop is called\n        self.output_lock = None\n\n    def setup_prompt(self):\n        if self.cfg.prompt_format == \"openai\":\n            return None\n\n        prompt = get_prompt(\n            prompt_config=self.cfg.prompt_config,\n            tokenizer=self.tokenizer,\n            code_tags=self.cfg.code_tags,\n            examples_type=self.cfg.examples_type,\n            system_message=self.cfg.system_message,\n        )\n\n        LOG.info(\"Prompt used: %s\", prompt)\n        return prompt\n\n    def setup_llm(self):\n        self.sandbox = get_sandbox(**self.cfg.sandbox) if self.cfg.sandbox is not None else None\n\n        if self.cfg.code_execution:\n            llm = get_code_execution_model(**self.cfg.server, tokenizer=self.tokenizer, sandbox=self.sandbox)\n        elif self.cfg.tool_modules is not None:\n            llm = get_tool_calling_model(\n                **self.cfg.server,\n                tool_modules=self.cfg.tool_modules,\n                tool_overrides=self.cfg.tool_overrides,\n                tokenizer=self.tokenizer,\n                additional_config={\"sandbox\": self.cfg.sandbox},\n            )\n        else:\n            llm = get_model(**self.cfg.server, tokenizer=self.tokenizer)\n\n        if self.cfg.parallel_thinking.mode is not None:\n            # We don't want to override these key variables which overlap with self.cfg\n            inference_override_config = {\n                \"endpoint_type\": self.cfg.parallel_thinking.endpoint_type,\n                # The following are specific to parallel thinking and we want\n                # to defend against any future key overlaps with the main generation config\n                \"mode\": self.cfg.parallel_thinking.mode,\n                \"window_size\": self.cfg.parallel_thinking.window_size,\n                \"solution_key\": self.cfg.parallel_thinking.solution_key,\n                \"filter_incomplete_solutions\": self.cfg.parallel_thinking.filter_incomplete_solutions,\n            }\n\n            llm = get_parallel_thinking_model(\n                model=llm,\n                orig_prompt_filler=self.fill_prompt,  # Needed for prompt fillling\n                parallel_thinking=self.cfg.parallel_thinking,\n                main_config=self.cfg,\n                tokenizer=self.tokenizer,\n                inference_override_config=inference_override_config,\n            )\n\n        return llm\n\n    def log_example_prompt(self, data):\n        data_point = deepcopy(data[0])\n\n        LOG.info(\"Example prompt:\\nData dictionary: %s\\nPrompt: %s\", data_point, self.fill_prompt(data_point, data))\n\n    def load_data(self):\n        data = []\n        with open(self.cfg.input_file, \"rt\", encoding=\"utf-8\") as fin:\n            for line in fin:\n                data.append(json.loads(line))\n\n        # chunk the dataset if required\n        if self.cfg.num_chunks is not None and self.cfg.chunk_id is not None:\n            data, self.cfg.output_file = chunk_data(data, self.cfg.output_file, self.cfg.chunk_id, self.cfg.num_chunks)\n            LOG.info(\n                f\"Chunking the data into {self.cfg.num_chunks} chunks and processing chunk {self.cfg.chunk_id}.\\n\"\n                f\"Number of samples in the chunk: {len(data)}\"\n            )\n\n        if self.cfg.max_samples > 0:\n            data = data[: self.cfg.max_samples]\n\n        return data\n\n    def preprocess_data(self, data):\n        \"\"\"A placeholder for any data preprocessing that needs to be done before generation.\"\"\"\n        return data\n\n    def postprocess(self):\n        \"\"\"A placeholder for any postprocessing that needs to be done after generation.\n\n        Data is already saved to self.cfg.output_file, so it can be read and re-saved from there.\n        \"\"\"\n        pass\n\n    def run_batch_evaluation(self):\n        \"\"\"Run final evaluation consuming all data together if configured.\"\"\"\n        self.cfg.eval_config[\"input_file\"] = self.cfg.output_file\n        evaluate(self.cfg.eval_type, self.cfg.eval_config)\n\n    def skip_completed_samples(self, data):\n        # if non-async file exists and we are asked to skip filled, then there is no more data to process\n        if self.cfg.skip_filled and Path(self.cfg.output_file).exists():\n            return []\n\n        filled_positions = set()\n        if self.cfg.skip_filled:\n            if self.cfg.num_chunks:\n                chunk_index = self.cfg.output_file.rfind(\"_chunk\")\n                base_output_file = self.cfg.output_file[:chunk_index] + \".jsonl\"\n                if Path(base_output_file).exists():\n                    LOG.warning(f\"File `{base_output_file}` exists, skipping generation\")\n                    return []\n            try:\n                with open(self.cfg.output_file + \"-async\", \"rt\", encoding=\"utf-8\") as fin:\n                    for line in fin:\n                        filled_positions.add(int(json.loads(line)[self.cfg.async_position_key]))\n            except FileNotFoundError:\n                LOG.warning(f\"File `{self.cfg.output_file}-async` not found, starting from scratch\")\n\n        remaining_data = []\n        for idx, dp in enumerate(data):\n            if idx in filled_positions:\n                continue\n            if self.cfg.prompt_format == \"openai\" and isinstance(dp, list):\n                # openai format allows for a list to be top-level key, if that's the case, wrapping it in a messages key\n                dp = {\"messages\": dp}\n            dp[self.cfg.async_position_key] = idx\n            remaining_data.append(dp)\n\n        return remaining_data\n\n    # TODO: data will not include any samples skipped after restart\n    def fill_prompt(self, data_point, data):\n        \"\"\"Passing in full data in case it's needed to fill the prompt in subclasses.\"\"\"\n        if self.cfg.prompt_format == \"openai\":\n            if self.cfg.prompt_suffix:\n                data_point[\"messages\"][-1][\"content\"] += self.cfg.prompt_suffix\n            if self.cfg.system_message:\n                if data_point[\"messages\"][0][\"role\"] != \"system\":\n                    data_point[\"messages\"].insert(0, {\"role\": \"system\", \"content\": self.cfg.system_message})\n                else:\n                    data_point[\"messages\"][0][\"content\"] = self.cfg.system_message\n            return data_point[\"messages\"]\n\n        total_code_executions_in_prompt = self.cfg.total_code_executions_in_prompt\n        if total_code_executions_in_prompt is not None:\n            if isinstance(total_code_executions_in_prompt, (list, tuple)):\n                min_val, max_val = total_code_executions_in_prompt\n                total_code_executions_in_prompt = random.randint(min_val, max_val)\n            data_point[\"total_code_executions\"] = total_code_executions_in_prompt\n        data_point = deepcopy(data_point)\n        filled_prompt = self.prompt.fill(\n            data_point,\n            start_assistant_response_key=self.cfg.start_assistant_response_key,\n            chat_template_kwargs=self.cfg.chat_template_kwargs,\n            format_as_string=(self.cfg.inference.endpoint_type == EndpointType.text),\n        )\n        if self.cfg.prompt_suffix:\n            if isinstance(filled_prompt, list):\n                filled_prompt[-1][\"content\"] += self.cfg.prompt_suffix\n            else:\n                filled_prompt += self.cfg.prompt_suffix\n        return filled_prompt\n\n    def dump_outputs(self, outputs, data_points, fout):\n        for output in outputs:\n            fout.write(json.dumps(output) + \"\\n\")\n\n    async def postprocess_single_output(self, output, original_data_point):\n        # to make it easier to follow up with other generations and limit accidental errors, we are adding\n        # all of the original data to the output file alongside the new generations\n        output[self.cfg.generation_key] = output.pop(\"generation\")\n\n        if not self.cfg.add_generation_stats:\n            output.pop(\"generation_start_time\", None)\n            output.pop(\"generation_end_time\", None)\n            output.pop(\"generation_time\", None)\n            output.pop(\"num_generated_tokens\", None)\n            output.pop(\"num_input_tokens\", None)\n\n        for key in output:\n            original_data_point.pop(key, None)\n        output.update(original_data_point)\n        if self.cfg.parse_reasoning:\n            parse_reasoning(\n                output,\n                self.cfg.generation_key,\n                self.cfg.end_reasoning_string,\n            )\n\n    def prefill_generation(self, data_point) -> dict | None:\n        \"\"\"Prefill generation in case LLM is not required.\"\"\"\n        # Override this method to customize the prefilling behavior.\n        return None\n\n    async def process_single_datapoint(self, data_point, all_data):\n        # Handle inference config - check if it's a dataclass or already a dict\n        if is_dataclass(self.cfg.inference):\n            inference_params = asdict(self.cfg.inference)\n        else:\n            # Already a dict from Hydra\n            inference_params = dict(self.cfg.inference)\n\n        generation_params = {\n            **inference_params,\n            **self.extra_generate_params,\n            \"prompt\": self.fill_prompt(data_point, all_data),\n            \"stop_phrases\": [self.cfg.stop_phrase] if self.cfg.stop_phrase else None,\n        }\n\n        if self.cfg.code_execution:\n            if self.cfg.override_max_code_executions and self.cfg.total_code_executions_in_prompt is not None:\n                generation_params[\"max_code_executions\"] = data_point[\"total_code_executions\"]\n\n        result = await self.generate_with_semaphore(**generation_params)\n\n        if self.cfg.count_prompt_tokens:\n            num_input_tokens = get_token_count(self.hf_tokenizer, generation_params[\"prompt\"])\n            result[\"num_input_tokens\"] = num_input_tokens\n\n        return result\n\n    async def generate_with_semaphore(self, **generation_params):\n        \"\"\"Generate with semaphore control.\n\n        Ensures no more than max_concurrent_requests LLM calls can be run at the same time.\n        Should work even if process_single_datapoint is doing multiple requests in parallel\n        as long as those requests also use this function.\n        \"\"\"\n        async with self.semaphore:\n            return await self.llm.generate_async(**generation_params)\n\n    async def evaluate_single_datapoint(self, data_point):\n        eval_start_time = time.time()\n        eval_results = await self.evaluator.eval_single(data_point)\n        eval_end_time = time.time()\n        data_point[\"interleaved_eval_single_time_s\"] = eval_end_time - eval_start_time\n        data_point.update(eval_results)\n        return data_point\n\n    async def _generate_and_save_datapoint(self, data_point, all_data, fout, pbar):\n        \"\"\"Starts generation, evaluation and saves the output for a single data point.\"\"\"\n        # Generate output for this single data point\n        start_time = time.time()\n        output = await self.process_single_datapoint(data_point, all_data)\n        end_time = time.time()\n\n        if self.cfg.add_generation_stats:\n            output[\"generation_start_time\"] = start_time\n            output[\"generation_end_time\"] = end_time\n            output[\"generation_time\"] = end_time - start_time\n\n        await self.postprocess_single_output(output, data_point)\n\n        # evaluate single-data point if requested and evaluator supports that\n        if self.should_run_evaluation and self.evaluator:\n            output = await self.evaluate_single_datapoint({**data_point, **output})\n\n        # Thread-safe output writing\n        async with self.output_lock:\n            self.dump_outputs([output], [data_point], fout)\n            pbar.update(1)\n\n    async def async_loop(self, data):\n        \"\"\"Async loop to generate generations using asyncio.\"\"\"\n\n        # Initialize output lock for thread-safe writing\n        if self.output_lock is None:\n            self.output_lock = asyncio.Lock()\n\n        # We first segregate the data into prefilled and non-prefilled data points\n        prefilled_data_points, prefilled_outputs = [], []\n        remaining_data_points = []\n\n        for data_point in data:\n            prefill_output = self.prefill_generation(data_point)\n            if prefill_output is not None:\n                prefilled_outputs.append(prefill_output)\n                prefilled_data_points.append(data_point)\n            else:\n                remaining_data_points.append(data_point)\n\n        pbar = tqdm(total=len(remaining_data_points), desc=\"Remaining generations\")\n\n        with open(self.cfg.output_file + \"-async\", \"at\", encoding=\"utf-8\", buffering=1) as fout:\n            # Dump prefilled data first\n            if len(prefilled_data_points) > 0:\n                for output, data_point in zip(prefilled_outputs, prefilled_data_points):\n                    await self.postprocess_single_output(output, data_point)\n\n                    # evaluate single-data point if requested and evaluator supports that\n                    if self.should_run_evaluation and self.evaluator:\n                        output = await self.evaluate_single_datapoint({**data_point, **output})\n                async with self.output_lock:\n                    self.dump_outputs(prefilled_outputs, prefilled_data_points, fout)\n\n            # Create tasks for all remaining data points\n            tasks = []\n            for data_point in remaining_data_points:\n                task = asyncio.create_task(self._generate_and_save_datapoint(data_point, data, fout, pbar))\n                tasks.append(task)\n\n            # Wait for all tasks to complete\n            if tasks:\n                await asyncio.gather(*tasks)\n\n            pbar.close()\n\n        self.restore_async_order()\n\n    def restore_async_order(self):\n        # After we are done, need to restore the order and resave without position ids\n        with open(self.cfg.output_file + \"-async\", \"rt\", encoding=\"utf-8\") as fin:\n            generations = [json.loads(line) for line in fin]\n\n        ordered_generations = [None] * len(generations)\n        for gen_dict in generations:\n            async_pos = gen_dict.pop(self.cfg.async_position_key)\n            ordered_generations[async_pos] = gen_dict\n\n        with open(self.cfg.output_file, \"wt\", encoding=\"utf-8\") as fout:\n            for gen_dict in ordered_generations:\n                fout.write(json.dumps(gen_dict) + \"\\n\")\n\n        Path(self.cfg.output_file + \"-async\").unlink()\n        self.cleanup_litellm_cache()\n\n    def wait_for_server(self):\n        if not self.cfg.server.get(\"base_url\") and not self.cfg.server.get(\"host\") and not self.cfg.server.get(\"port\"):\n            LOG.info(\"Skipping server wait as no server address is provided.\")\n            return\n        server_address = self.cfg.server.get(\"base_url\") or f\"{self.cfg.server['host']}:{self.cfg.server['port']}\"\n        # Hydra sets None parameters to \"None\" string\n        if server_address == \"None\":\n            LOG.info(\"Skipping server wait as no server address is provided.\")\n            return\n        server_start_cmd = get_server_wait_cmd(server_address)\n        subprocess.run(server_start_cmd, shell=True, check=True)\n\n    def wait_for_sandbox(self):\n        if self.cfg.wait_for_sandbox:\n            self.sandbox.wait_for_sandbox()\n\n    def setup_litellm_cache(self):\n        if self.cfg.enable_litellm_cache:\n            # One cache per (output_file_name, chunk_id) pair\n            output_file_name = Path(self.cfg.output_file).name\n            self.litellm_cache_dir = (\n                Path(self.cfg.output_file).parent / \"litellm_cache\" / f\"{output_file_name}_{self.cfg.chunk_id or 0}\"\n            )\n            litellm.cache = litellm.Cache(type=\"disk\", disk_cache_dir=self.litellm_cache_dir)\n\n    def cleanup_litellm_cache(self):\n        if self.cfg.enable_litellm_cache:\n            shutil.rmtree(self.litellm_cache_dir)\n\n    def generate(self):\n        Path(self.cfg.output_file).absolute().parent.mkdir(parents=True, exist_ok=True)\n\n        data = self.load_data()\n\n        data = self.skip_completed_samples(data)\n\n        if len(data) == 0:\n            LOG.info(\"No data to process, skipping generation\")\n        else:\n            data = self.preprocess_data(data)\n\n            self.log_example_prompt(data)\n\n            if self.cfg.dry_run:\n                LOG.info(\"Exiting without running generation as dry_run flag is set.\")\n                return\n\n            if not self.cfg.skip_filled:\n                for output_path in [Path(self.cfg.output_file), Path(self.cfg.output_file + \"-async\")]:\n                    if output_path.exists():\n                        output_path.unlink()\n\n            self.wait_for_server()\n            self.wait_for_sandbox()\n            asyncio.run(self.async_loop(data))\n\n        if self.should_run_evaluation and self.evaluator is None:\n            self.run_batch_evaluation()\n        self.postprocess()\n\n\nGENERATION_TASK_CLASS = GenerationTask\n\n\n# Update the hydra main to use the class method\n@hydra.main(version_base=None, config_name=\"base_generation_config\")\ndef generate(cfg: GenerateSolutionsConfig):\n    cfg = GenerateSolutionsConfig(_init_nested=True, **cfg)\n    LOG.info(\"Config used: %s\", cfg)\n\n    task = GenerationTask(cfg)\n    task.generate()\n\n\nHELP_MESSAGE = get_help_message(\n    GenerateSolutionsConfig,\n    server_params=server_params(),\n    sandbox_params=sandbox_params(),\n)\n\n\nif __name__ == \"__main__\":\n    if \"--help\" in sys.argv or \"-h\" in sys.argv:\n        print(HELP_MESSAGE)\n    else:\n        setup_logging()\n        generate()\n\nFile Path: nemo_skills/inference/llm_math_judge.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport sys\nfrom dataclasses import field\n\nimport hydra\n\nfrom nemo_skills.evaluation.math_grader import extract_answer\nfrom nemo_skills.inference.generate import GenerateSolutionsConfig, GenerationTask, InferenceConfig\nfrom nemo_skills.inference.model import server_params\nfrom nemo_skills.utils import get_help_message, get_logger_name, nested_dataclass, prefill_judgement, setup_logging\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\n@nested_dataclass(kw_only=True)\nclass LlmMathJudgeConfig(GenerateSolutionsConfig):\n    \"\"\"LLM math judge parameters.\n    For the full list of supported parameters, use 'python -m nemo_skills.inference.generate --help'\n    \"\"\"\n\n    # Inheritance was converting these dataclasses to dicts, so to be on the safe side we override them\n    inference: InferenceConfig = field(default_factory=InferenceConfig)  # LLM call parameters\n    # Inference server configuration {server_params}\n    server: dict = field(default_factory=dict)\n\n    # Override the default Generation config here\n    prompt_config: str = \"judge/math\"\n    generation_key: str = \"judgement\"\n\n    add_generation_stats: bool = False\n\n\ncs = hydra.core.config_store.ConfigStore.instance()\ncs.store(name=\"base_llm_math_judge_config\", node=LlmMathJudgeConfig)\n\n\nclass LLMMathJudgeTask(GenerationTask):\n    def __init__(self, cfg: LlmMathJudgeConfig):\n        super().__init__(cfg)\n\n    def preprocess_data(self, data):\n        \"\"\"Extract the predicted answer from the generation.\"\"\"\n        for data_point in data:\n            if \"predicted_answer\" not in data_point:\n                data_point[\"predicted_answer\"] = extract_answer(data_point[\"generation\"])\n\n        return data\n\n    def prefill_generation(self, data_point):\n        \"\"\"Prefill judgement\"\"\"\n        judgement = prefill_judgement(data_point)\n        if judgement is None:\n            return None\n        else:\n            return {\"generation\": judgement}\n\n\nGENERATION_TASK_CLASS = LLMMathJudgeTask\n\n\n# Update the hydra main to use the class method\n@hydra.main(version_base=None, config_name=\"base_llm_math_judge_config\")\ndef generate(cfg: LlmMathJudgeConfig):\n    cfg = LlmMathJudgeConfig(_init_nested=True, **cfg)\n    LOG.info(\"Config used: %s\", cfg)\n\n    task = LLMMathJudgeTask(cfg)\n    task.generate()\n\n\nHELP_MESSAGE = get_help_message(\n    LlmMathJudgeConfig,\n    server_params=server_params(),\n)\n\n\nif __name__ == \"__main__\":\n    if \"--help\" in sys.argv or \"-h\" in sys.argv:\n        print(HELP_MESSAGE)\n    else:\n        setup_logging()\n        generate()\n\nFile Path: nemo_skills/inference/model/code_execution.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport copy\nimport logging\nimport time\nfrom dataclasses import field\n\nfrom nemo_skills.code_execution import extract_code_to_execute, format_code_output\nfrom nemo_skills.code_execution.sandbox import Sandbox\nfrom nemo_skills.utils import get_logger_name, nested_dataclass\n\nfrom .base import BaseModel, EndpointType\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\n@nested_dataclass(kw_only=True)\nclass CodeExecutionConfig:\n    max_code_output_characters: int = 1000\n    code_execution_timeout: float = 10.0\n    code_execution_language: str = \"ipython\"\n    code_execution_headers: list[str] = field(default_factory=lambda: [])\n    max_code_executions: int = 8\n    sandbox_traceback_verbosity: str = \"plain\"  # could be plain, context, verbose, or minimal\n    add_remaining_code_executions: bool = False\n\n\nclass CodeExecutionWrapper:\n    def __init__(self, model: BaseModel, sandbox: Sandbox, config: CodeExecutionConfig):\n        self.model = model\n        self.sandbox = sandbox\n        self.config = config\n\n    async def _generate_single(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int,\n        temperature: float,\n        top_p: float,\n        top_k: int,\n        min_p: float,\n        repetition_penalty: float,\n        random_seed: int,\n        stop_phrases: list[str] | None = None,\n        top_logprobs: int | None = None,\n        timeout: float | int | None = 14400,  # None is 10min\n        max_code_executions: int | None = None,  # if not None, will override self.config.max_code_executions\n        stream: bool = False,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ):\n        # Handle OpenAI-style dictionary prompts\n        is_openai_format = not isinstance(prompt, str)\n\n        if top_logprobs is not None:  # TODO: add this\n            raise NotImplementedError(\"top_logprobs is not supported yet.\")\n\n        if stream:\n            return self._stream_single(\n                prompt=prompt,\n                endpoint_type=endpoint_type,\n                code_begin=code_begin,\n                code_end=code_end,\n                code_output_begin=code_output_begin,\n                code_output_end=code_output_end,\n                code_output_format=code_output_format,\n                tokens_to_generate=tokens_to_generate,\n                temperature=temperature,\n                top_p=top_p,\n                top_k=top_k,\n                min_p=min_p,\n                repetition_penalty=repetition_penalty,\n                random_seed=random_seed,\n                stop_phrases=stop_phrases,\n                timeout=timeout,\n                max_code_executions=max_code_executions,\n                extra_body=extra_body,\n            )\n\n        effective_max_code_executions = self.config.max_code_executions\n        if max_code_executions is not None:\n            effective_max_code_executions = max_code_executions\n\n        # making a copy of prompts to not corrupt original data\n        if is_openai_format:\n            new_prompt = copy.deepcopy(prompt)\n        else:\n            new_prompt = copy.deepcopy(prompt)\n\n        start_time = int(time.time())\n\n        stop_phrases = stop_phrases or []\n\n        request = {\n            \"endpoint_type\": endpoint_type,\n            \"prompt\": new_prompt,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            \"top_p\": top_p,\n            \"min_p\": min_p,\n            \"random_seed\": random_seed,\n            \"repetition_penalty\": repetition_penalty,\n            \"stop_phrases\": stop_phrases + [code_end],\n            \"timeout\": timeout,\n            \"extra_body\": extra_body,\n        }\n        session_id = None\n        code_rounds_executed = 0\n        total_num_generated_tokens = 0\n        generation_time = 0\n        code_execution_time = 0\n        stopped_on_repetition = False\n        num_code_timeouts = 0\n        # adding plus one to make sure there is always some completion after the last requested code block\n        try:\n            for generation_index in range(effective_max_code_executions + 1):\n                generation_time_start = time.time()\n                if timeout is not None:\n                    # updating timeout to account for the time already spent\n                    new_timeout = int(timeout - (time.time() - start_time))\n                    request[\"timeout\"] = new_timeout\n                    if request[\"timeout\"] <= 0:\n                        break\n\n                output_dict = await self.model.generate_async(**request, remove_stop_phrases=False)\n\n                output, num_generated_tokens = output_dict[\"generation\"], output_dict.get(\"num_generated_tokens\", 0)\n                # no need to do anything with this as the code below should just exit, so that's only for logging\n                stopped_on_repetition = output_dict.get(\"stopped_on_repetition\", False)\n\n                # openai and trtllm don't show what stop word was triggered, so we assume that it was `code_end`\n                # if there's an unfinished code block\n                if output.count(code_end) + 1 == output.count(code_begin):\n                    output += code_end\n\n                # Update the prompt based on format\n                if is_openai_format:\n                    request[\"prompt\"].append({\"role\": \"assistant\", \"content\": output})\n                    request[\"prompt\"].append({\"role\": \"user\", \"content\": \"continue\"})\n                else:\n                    request[\"prompt\"] += output\n\n                # if it's the extra iteration, we don't execute the code block and just finish\n\n                if generation_index == effective_max_code_executions:\n                    break\n                # adjusting requested tokens to account for what has been generated already\n                request[\"tokens_to_generate\"] -= num_generated_tokens\n                total_num_generated_tokens += num_generated_tokens\n                generation_time += int(time.time() - generation_time_start)\n                # TODO: currently we don't account for tokens in the code output that we add to the prompt\n                #       in most cases the output should be small though\n                if request[\"tokens_to_generate\"] <= 0:\n                    break\n                # .rfind(code_end, 0, -1) searches for the second-to-last occurrence of code_end and checks\n                # that the last code_begin is not closed to ensure that we are inside the code block\n                if output.endswith(code_end) and output.rfind(code_begin) > output.rfind(code_end, 0, -1):\n                    code_execution_time_start, execution_dict, session_id = await self.execute_generated_code(\n                        prompt, code_begin, code_end, output, session_id\n                    )\n                    remaining_code_executions = None\n                    if self.config.add_remaining_code_executions:\n                        remaining_code_executions = effective_max_code_executions - generation_index - 1\n                    # adding code output to the prompt\n                    code_output = format_code_output(\n                        execution_dict,\n                        code_output_begin,\n                        code_output_end,\n                        code_output_format,\n                        remaining_code_executions,\n                    )\n\n                    if \"process_status\" in execution_dict and execution_dict[\"process_status\"] == \"timeout\":\n                        num_code_timeouts += 1\n\n                    if is_openai_format:\n                        request[\"prompt\"][-2][\"content\"] += code_output\n                    else:\n                        request[\"prompt\"] += code_output\n\n                    code_execution_time += int(time.time() - code_execution_time_start)\n                    code_rounds_executed += 1\n                else:  # if no code was generated, we need to finish\n                    break\n\n            # removing original prompt and returning the generation\n            if is_openai_format:\n                generation = \"\\n\".join(msg[\"content\"] for msg in request[\"prompt\"] if msg[\"role\"] == \"assistant\")\n            else:\n                generation = request[\"prompt\"][len(prompt) :]\n\n            return {\n                \"generation\": generation,\n                \"code_rounds_executed\": code_rounds_executed,\n                \"num_generated_tokens\": total_num_generated_tokens,\n                \"generation_time\": generation_time,\n                \"code_execution_time\": code_execution_time,\n                \"stopped_on_repetition\": stopped_on_repetition,\n                \"num_code_timeouts\": num_code_timeouts,\n            }\n        finally:\n            # Clean up session if we created one and configured to do so\n            if session_id is not None and self.config.code_execution_language == \"ipython\":\n                await self.sandbox.delete_session(str(session_id))\n\n    async def execute_generated_code(self, input_prompt, code_begin, code_end, output, session_id):\n        code_execution_time_start = time.time()\n        header = \"\\n\".join(self.config.code_execution_headers)\n        code_block = extract_code_to_execute(output, code_begin, code_end)\n        extracted_code = f\"{header}{code_block}\"\n        execution_dict, session_id = await self.sandbox.execute_code(\n            generated_code=extracted_code,\n            language=self.config.code_execution_language,\n            timeout=self.config.code_execution_timeout,\n            max_output_characters=self.config.max_code_output_characters,\n            session_id=session_id,\n            traceback_verbosity=self.config.sandbox_traceback_verbosity,\n        )\n\n        return code_execution_time_start, execution_dict, session_id\n\n    async def generate_async(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int | None = None,\n        temperature: float = 0.0,\n        top_p: float = 0.95,\n        top_k: int = -1,\n        min_p: float = 0.0,\n        repetition_penalty: float = 1.0,\n        random_seed: int = 0,\n        stop_phrases: list[str] | None = None,\n        remove_stop_phrases: bool = True,\n        top_logprobs: int | None = None,\n        timeout: float | int | None = 14400,  # None is 10min\n        max_code_executions: int | None = None,\n        stream: bool = False,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ) -> list[dict]:\n        \"\"\"For any generation parameter you can specify a list of values that needs to match the number of prompts.\n\n        Not every server supports that, so make sure to override this method directly if that's not the case.\n        \"\"\"\n        if top_logprobs is not None:  # TODO: add this\n            raise NotImplementedError(\"top_logprobs is not supported yet.\")\n\n        kwargs = {\n            \"endpoint_type\": endpoint_type,\n            \"code_begin\": code_begin,\n            \"code_end\": code_end,\n            \"code_output_begin\": code_output_begin,\n            \"code_output_end\": code_output_end,\n            \"code_output_format\": code_output_format,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"top_k\": top_k,\n            \"min_p\": min_p,\n            \"repetition_penalty\": repetition_penalty,\n            \"random_seed\": random_seed,\n            \"stop_phrases\": stop_phrases,\n            \"timeout\": timeout,\n            \"max_code_executions\": max_code_executions,\n            \"stream\": stream,\n            \"extra_body\": extra_body,\n        }\n\n        request = {key: value for key, value in kwargs.items()}\n        request[\"prompt\"] = prompt\n\n        output = await self._generate_single(**request)\n        if not stream:\n            # Some models (like NIM speech models) don't have this method\n            if hasattr(self.model, \"_maybe_apply_stop_phrase_removal\"):\n                self.model._maybe_apply_stop_phrase_removal(output, remove_stop_phrases, stop_phrases)\n\n        return output\n\n    async def _stream_single(\n        self,\n        prompt: str | list[dict],\n        code_begin: str,\n        code_end: str,\n        code_output_begin: str,\n        code_output_end: str,\n        code_output_format: str,\n        tokens_to_generate: int = 512,\n        temperature: float = 0.0,\n        top_p: float = 0.95,\n        top_k: int = -1,\n        min_p: float = 0.0,\n        repetition_penalty: float = 1.0,\n        random_seed: int = 0,\n        stop_phrases: list[str] | None = None,\n        timeout: float | int | None = 14400,  # None is 10min,\n        max_code_executions: int | None = None,\n        extra_body: dict = None,\n        endpoint_type: EndpointType = None,\n    ):\n        \"\"\"\n        Helper method, that implements streaming generation.\n        \"\"\"\n        # Handle OpenAI-style dictionary prompts\n        is_openai_format = not isinstance(prompt, str)\n\n        effective_max_code_executions = self.config.max_code_executions\n        if max_code_executions is not None:\n            effective_max_code_executions = max_code_executions\n\n        stop_phrases = stop_phrases or []\n\n        request = {\n            \"endpoint_type\": endpoint_type,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"top_k\": top_k,\n            \"min_p\": min_p,\n            \"repetition_penalty\": repetition_penalty,\n            \"random_seed\": random_seed,\n            \"stop_phrases\": stop_phrases + [code_end],\n            \"timeout\": timeout,\n            \"tokens_to_generate\": tokens_to_generate,\n            \"stream\": True,\n            \"extra_body\": extra_body,\n        }\n\n        current_full_prompt = copy.deepcopy(prompt)\n        session_id = None  # For sandbox state continuity\n        try:\n            for generation_index in range(effective_max_code_executions + 1):\n                model_token_iterator = await self.model.generate_async(prompt=current_full_prompt, **request)\n\n                current_output_segment = \"\"\n                num_generated_tokens = 0\n                async for chunk in model_token_iterator:\n                    yield chunk\n                    current_output_segment += chunk[\"generation\"]\n                    num_generated_tokens += 1\n\n                request[\"tokens_to_generate\"] -= num_generated_tokens\n                if request[\"tokens_to_generate\"] <= 0:\n                    break\n                if not current_output_segment:\n                    break\n\n                # openai and trtllm don't show what stop word was triggered, so we assume that it was `code_end`\n                # if there's an unfinished code block\n                if current_output_segment.count(code_end) + 1 == current_output_segment.count(code_begin):\n                    current_output_segment += code_end\n                    yield {\"generation\": code_end}\n\n                # Update the prompt based on format\n                if is_openai_format:\n                    current_full_prompt.append({\"role\": \"assistant\", \"content\": current_output_segment})\n                    current_full_prompt.append({\"role\": \"user\", \"content\": \"continue\"})\n                else:\n                    current_full_prompt += current_output_segment\n\n                if generation_index == effective_max_code_executions:\n                    # This was the last iteration, intended for final text generation after all code executions.\n                    break\n\n                if current_output_segment.endswith(code_end) and current_output_segment.rfind(\n                    code_begin\n                ) > current_output_segment.rfind(code_end, 0, -1):\n                    execution_dict, session_id = await self.sandbox.execute_code(\n                        generated_code=extract_code_to_execute(current_output_segment, code_begin, code_end),\n                        language=self.config.code_execution_language,\n                        timeout=self.config.code_execution_timeout,\n                        max_output_characters=self.config.max_code_output_characters,\n                        session_id=session_id,\n                        traceback_verbosity=self.config.sandbox_traceback_verbosity,\n                    )\n\n                    remaining_code_executions = None\n                    if self.config.add_remaining_code_executions:\n                        remaining_code_executions = effective_max_code_executions - generation_index - 1\n\n                    formatted_code_output = format_code_output(\n                        execution_dict,\n                        code_output_begin,\n                        code_output_end,\n                        code_output_format,\n                        remaining_code_executions,\n                    )\n                    yield {\"generation\": formatted_code_output}  # Yield the entire formatted code output as one chunk\n\n                    # Append executed code's output to the prompt\n                    if is_openai_format:\n                        current_full_prompt[-2][\"content\"] += formatted_code_output\n                    else:\n                        current_full_prompt += formatted_code_output\n                else:  # if no code was generated, we need to finish\n                    break\n        finally:\n            if session_id is not None and self.config.code_execution_language == \"ipython\":\n                await self.sandbox.delete_session(str(session_id))\n\nFile Path: nemo_skills/evaluation/math_grader.py\nContent:\n# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport re\n\nfrom latex2sympy2_extended import NormalizationConfig, normalize_latex\nfrom math_verify import LatexExtractionConfig, StringExtractionConfig, parse, verify\n\nfrom nemo_skills.utils import get_logger_name\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\ndef _additional_normalization(expr):\n    # Remove % and \\\\% from the number\n    percentage_pattern = r\"^(\\\\d+\\\\.?\\\\d*)(?:\\\\%|%)$\"\n    match_gt = re.fullmatch(percentage_pattern, expr)\n    if match_gt:\n        expr = match_gt.group(1)\n    # Remove . corresponding to the end of sentence\n    expr = expr.rstrip(\".\\\\\")\n    return expr\n\n\ndef math_equal(gt_answer, predicted_answer, take_modulo: int | None = None, **kwargs):\n    if predicted_answer is None:\n        return False\n\n    gt_answer = str(gt_answer)\n    predicted_answer = str(predicted_answer)\n\n    # if we are sure that gt is always integer\n    if take_modulo is not None:\n        gt_answer = int(gt_answer) % take_modulo\n        try:\n            predicted_answer = int(predicted_answer) % take_modulo\n        except Exception:\n            predicted_answer = None\n        # no need to simpy call in this case\n        return predicted_answer == gt_answer\n\n    # Try to compare as MCQ options\n    mcq_options = \"ABCDEFGHIJ\"\n    norm_gt_mcq = gt_answer.strip()\n\n    is_mcq = re.fullmatch(\"|\".join(mcq_options), norm_gt_mcq)\n    parsed_gt = parse(gt_answer, [StringExtractionConfig(strings=tuple(mcq_options))])\n    parsed_pred = parse(predicted_answer, [StringExtractionConfig(strings=tuple(mcq_options))])\n    if is_mcq and verify(parsed_gt, parsed_pred):\n        return verify(parsed_gt, parsed_pred)\n\n    # Additional normalization step\n    gt_answer = _additional_normalization(gt_answer)\n    predicted_answer = _additional_normalization(predicted_answer)\n\n    # Try literal comparison\n    literal_pattern = r\"[a-zA-Z ,]+|[0-9 ]+\"\n    normalized_gt = normalize_latex(gt_answer, NormalizationConfig)\n    normalized_pred = normalize_latex(predicted_answer, NormalizationConfig)\n    is_literal = re.fullmatch(literal_pattern, normalized_gt) and re.fullmatch(literal_pattern, normalized_pred)\n    is_normalized_equal = normalized_gt.replace(\" \", \"\") == normalized_pred.replace(\" \", \"\")\n\n    if is_literal or is_normalized_equal:\n        return is_normalized_equal\n\n    # Fallback to symbolic comparison\n    current_gt_answer = gt_answer\n    current_predicted_answer = predicted_answer\n\n    # math_verify.parse expects input to be in latex environment, e.g. $...$\n    latex_env_search_pattern = r\"\\\\$.*\\\\$|\\\\\\\\(.*\\\\\\\\)|\\\\\\\\[.*\\\\\\\\]|\\\\boxed{\"\n    if not re.search(latex_env_search_pattern, current_gt_answer, re.DOTALL):\n        current_gt_answer = f\"${current_gt_answer}$\"\n    if not re.search(latex_env_search_pattern, current_predicted_answer, re.DOTALL):\n        current_predicted_answer = f\"${current_predicted_answer}$\"\n\n    parsed_gt = parse(current_gt_answer, [LatexExtractionConfig()])\n    parsed_pred = parse(current_predicted_answer, [LatexExtractionConfig()])\n\n    return verify(parsed_gt, parsed_pred, **kwargs)\n\n\ndef extract_answer(\n    string: str, extract_from_boxed: bool = True, extract_regex: str = r\"The final answer is (.+)$\", relaxed=False\n):\n    \"\"\"Extract Answer String from \\\\boxed expression or based on regex\n    If relaxed=True: try both methods, boxed first.\n    If relaxed=False: use only one method based on extract_from_boxed flag.\n    \"\"\"\n    if relaxed:\n        return search_boxed(string) or search_regex(string, extract_regex)\n\n    if extract_from_boxed:\n        return search_boxed(string)\n    return search_regex(string, extract_regex)\n\n\ndef search_regex(string: str, regex: str):\n    match = re.findall(regex, string)\n    if match:\n        return match[-1]\n    return None\n\n\ndef search_boxed(string: str):\n    if \"\\\\boxed\" not in string:\n        return None\n\n    idx = string.rfind(\"\\\\boxed\")\n    if idx < 0:\n        idx = string.rfind(\"\\\\fbox\")\n        if idx < 0:\n            return None\n\n    i = idx\n    right_brace_idx = None\n    num_left_braces_open = 0\n    while i < len(string):\n        if string[i] == \"{\":\n            num_left_braces_open += 1\n        if string[i] == \"}\":\n            num_left_braces_open -= 1\n            if num_left_braces_open == 0:\n                right_brace_idx = i\n                break\n        i += 1\n\n    if right_brace_idx is None:\n        retval = None\n    else:\n        retval = string[idx : right_brace_idx + 1]\n\n    if retval:\n        left = \"\\\\boxed{\"\n        try:\n            assert retval[: len(left)] == left\n            assert retval[-1] == \"}\"\n            return retval[len(left) : -1]\n        except AssertionError:\n            return None\n\n    return None\n\nFile Path: nemo_skills/code_execution/utils.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport re\nfrom typing import Tuple\n\nfrom nemo_skills.utils import get_logger_name\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\ndef format_code_output(\n    execution_dict,\n    code_output_begin: str,\n    code_output_end: str,\n    code_output_format: str = \"llama\",\n    remaining_code_executions: int | None = None,\n):\n    \"\"\"Formatting code output to be displayed as an llm expects it.\"\"\"\n    remaining_ce_string = \"\"\n    if remaining_code_executions is not None:\n        if remaining_code_executions > 0:\n            remaining_ce_string = (\n                f\"```system\\n\"\n                f\"Remaining code executions: {remaining_code_executions}. \"\n                f\"You will not be able to call code when you run out of executions, so use it wisely. \"\n                f\"Note that you can still continue solving the problem without code after that.\\n\"\n                f\"```\\n\"\n            )\n        else:\n            remaining_ce_string = (\n                \"```system\\n\"\n                \"You have run out of code executions! You can no longer write or execute code. \"\n                \"Now you should continue solving the problem by relying on your mathematical reasoning and analytical skills.\\n\"\n                \"```\\n\"\n            )\n    if code_output_format == \"llama\":\n        output = execution_dict[\"process_status\"]\n        if execution_dict[\"stdout\"]:\n            output += f\"\\n[stdout]\\n{execution_dict['stdout']}[/stdout]\"\n        if execution_dict[\"stderr\"]:\n            output += f\"\\n[stderr]\\n{execution_dict['stderr']}[/stderr]\"\n        output = f\"{code_output_begin}\\n\\n{output}{remaining_ce_string}{code_output_end}\\n\\n\"\n    elif code_output_format == \"qwen\":\n        output = \"\"\n        if execution_dict[\"stdout\"]:\n            output += f\"{execution_dict['stdout']}\"\n        if execution_dict[\"stderr\"]:\n            output += f\"{execution_dict['stderr']}\"\n        output = f\"{code_output_begin}{output}{code_output_end}{remaining_ce_string}\"\n    else:\n        raise ValueError(f\"Unknown code_output_format: {code_output_format}\")\n\n    # wrapping with code output separators\n    return output\n\n\ndef _extract_between_separators(generation: str, separators: Tuple[str, str], extract_all: bool = False):\n    \"\"\"Extracting all text between last occurrence of separators[0] and [1].\n\n    If extract_all is True, returning a list with all occurrences of text between separators.\n    \"\"\"\n    if extract_all:\n        separators = [re.escape(sp) for sp in separators]\n        pattern = f\"{separators[0]}(.*?){separators[1]}\"\n        return re.findall(pattern, generation, re.DOTALL)\n    return generation.split(separators[0])[-1].split(separators[1])[0]\n\n\ndef extract_code_to_execute(generation: str, code_begin: str, code_end: str, extract_all: bool = False):\n    return _extract_between_separators(generation, [code_begin, code_end], extract_all)\n\n\ndef extract_code_output(generation: str, code_output_begin: str, code_output_end: str, extract_all: bool = False):\n    return _extract_between_separators(generation, [code_output_begin, code_output_end], extract_all)\n\n\ndef extract_code_block(text: str, languages=None, extract_code_mode: str = \"last\") -> str:\n    if languages is None:\n        languages = [\"\"]\n    for language in languages:\n        matches = re.findall(rf\"```{{language}}\\\\s*\\\\n?(.*?)\\\\n?```\", text, re.DOTALL)\n        if matches:\n            idx = 0 if extract_code_mode == \"first\" else -1\n            return matches[idx].strip()\n    return \"\"\n\n\ndef clean_formal_generation(\n    generation: str,\n    final_answer_key: str | None = None,\n    extract_code_mode: str = \"last\",\n) -> str:\n    # Extract part after final_answer_key if present and configured\n    if final_answer_key and final_answer_key in generation:\n        generation = generation.split(final_answer_key, 1)[1].strip()\n\n    languages = [\"lean4\", \"lean3\", \"lean\", \"\"]\n    extracted_code = extract_code_block(generation, languages, extract_code_mode=extract_code_mode)\n    if extracted_code:\n        return extracted_code\n\n    # If no explicit code block, remove any surrounding triple backticks\n    return re.sub(r\"^\\\\s*```(?:lean4|lean3|lean)?\\\\s*|\\\\s*```[\\\\s]*$\", \"\", generation).strip()\n\nFile Path: nemo_skills/prompt/utils.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport json\nimport logging\nimport random\nimport re\nfrom dataclasses import asdict, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport yaml\nfrom transformers import AutoTokenizer\n\nfrom nemo_skills.code_execution.utils import format_code_output\nfrom nemo_skills.prompt.few_shot_examples import examples_map\nfrom nemo_skills.utils import get_logger_name, nested_dataclass\n\nLOG = logging.getLogger(get_logger_name(__file__))\n\n\nclass BM25Retriever:\n    def __init__(self, data_path: str, field: str):\n        from rank_bm25 import BM25Okapi\n\n        with open(data_path, \"rt\", encoding=\"utf-8\") as fin:\n            self.entries = [json.loads(x) for x in fin]\n\n        corpus = [entry[field] for entry in self.entries]\n        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n        self.bm25 = BM25Okapi(tokenized_corpus)\n\n    def retrieve(self, query: str, top_k: int = 1):\n        tokenized_query = query.split(\" \")\n        return self.bm25.get_top_n(tokenized_query, self.entries, n=top_k)\n\n\n@nested_dataclass(kw_only=True)\nclass FewShotExamplesConfig:\n    prefix: str = \"\"\n    template: str = \"\"\n    suffix: str = \"\"\n\n    examples_type: Optional[str] = None\n\n    retrieval_field: Optional[str] = None  # e.g. question, reference_solution, etc.\n    retrieval_file: Optional[str] = None  # needs to be provided if retrieval_field is not None\n    retrieved_entries: int = 10  # need to set higher than few_shots to filter out exact matches\n    retrieved_few_shots: int = 5\n    randomize_retrieved_entries: bool = False\n    max_retrieved_chars: int = 100000000  # no limit by default\n    max_retrieved_chars_field: str = \"reference_solution\"\n    retriever: Optional[Any] = None\n\n    def __post_init__(self):\n        \"\"\"Error checks + building example_dicts and retriever if needed.\"\"\"\n        if self.examples_type is not None and self.retriever is not None:\n            raise ValueError(\"examples_type and retriever cannot be used together\")\n\n        if self.retriever is not None:\n            return\n\n        if self.retrieval_field is not None:  # building retriever\n            if self.retrieval_file is None:\n                raise ValueError(\"retrieval_file must be provided if retrieval_field is not None\")\n            self.retriever = BM25Retriever(self.retrieval_file, field=self.retrieval_field)\n        else:\n            if self.retrieval_file is not None:\n                raise ValueError(\"retrieval_field must be provided if retrieval_file is not None\")\n\n\n@nested_dataclass(kw_only=True)\nclass CodeTags:\n    # used to execute code within these tags\n    code_begin: str = \"```python\\n\"\n    code_end: str = \"```\\n\"\n\n    # used to extract the code output\n    code_output_begin: str = \"```output\\n\"\n    code_output_end: str = \"```\\n\"\n\n    # used to post-process code output\n    code_output_format: str = \"qwen\"\n\n\n@nested_dataclass(kw_only=True)\nclass PromptConfig:\n    user: str\n    system: str | None = None\n    code_tags: CodeTags = None\n    few_shot_examples: FewShotExamplesConfig = field(default_factory=FewShotExamplesConfig)\n\n\nclass Prompt:\n    def __init__(self, config, tokenizer):\n        # rebuilding prompt config to make sure post init is called again in\n        # case some parameters were manually changed after the config was created\n        self.config = PromptConfig(_init_nested=True, **asdict(config))\n        self.tokenizer = tokenizer\n        if self.tokenizer:\n            # assuming it's the object already if not str\n            if isinstance(self.tokenizer, str):\n                self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer)\n\n    def build_filled_example(self, example_dict: Dict[str, Any]) -> str:\n        \"\"\"Builds a filled example string based on the example dictionary.\"\"\"\n\n        # replacing code/code-output separators in the examples if present\n        example_dict = example_dict.copy()\n        if \"solution\" in example_dict and self.config.code_tags:\n\n            def replace_code_output(match):\n                code_output = match.group(2)\n                formatted_output = format_code_output(\n                    execution_dict={\"process_status\": \"completed\", \"stdout\": code_output, \"stderr\": \"\"},\n                    code_output_begin=self.config.code_tags.code_output_begin,\n                    code_output_end=self.config.code_tags.code_output_end,\n                    code_output_format=self.config.code_tags.code_output_format,\n                )\n                return formatted_output\n\n            pattern = r\"({code_output_begin}\\n)(.*?)({\\s*code_output_end})\"\n            example_dict[\"solution\"] = re.sub(pattern, replace_code_output, example_dict[\"solution\"], flags=re.DOTALL)\n\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\n                \"{code_begin}\", self.config.code_tags.code_begin\n            )\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_end}\", self.config.code_tags.code_end)\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_output_begin}\", \"\")\n            example_dict[\"solution\"] = example_dict[\"solution\"].replace(\"{code_output_end}\", \"\")\n\n        return self.config.few_shot_examples.template.format(**example_dict)\n\n    def build_examples_dict(self, input_dict):\n        if self.config.few_shot_examples.examples_type:\n            return examples_map[self.config.few_shot_examples.examples_type.format(**input_dict)]\n\n        if self.config.few_shot_examples.retriever is None:\n            return []\n\n        example_dicts = self.config.few_shot_examples.retriever.retrieve(\n            query=input_dict[self.config.few_shot_examples.retrieval_field],\n            top_k=self.config.few_shot_examples.retrieved_entries,\n        )\n        reference = input_dict[self.config.few_shot_examples.retrieval_field]\n        # filtering exact match if it's there\n        while example_dicts and example_dicts[0][self.config.few_shot_examples.retrieval_field] == reference:\n            example_dicts = example_dicts[1:]\n\n        # removing too long solutions\n        example_dicts = [\n            example_dict\n            for example_dict in example_dicts\n            if len(example_dict[self.config.few_shot_examples.max_retrieved_chars_field])\n            < self.config.few_shot_examples.max_retrieved_chars\n        ]\n\n        if len(example_dicts) < self.config.few_shot_examples.retrieved_few_shots:\n            LOG.warning(\n                'Too little examples (%d) found for the query \"%s\"',\n                len(example_dicts),\n                input_dict[self.config.few_shot_examples.retrieval_field],\n            )\n\n        # let's reverse the order to show the most relevant last\n        examples = example_dicts[: self.config.few_shot_examples.retrieved_few_shots][::-1]\n        if self.config.few_shot_examples.randomize_retrieved_entries:\n            random.shuffle(examples)\n\n        return examples\n\n    def build_user_message(self, input_dict: Dict[str, str]) -> str:\n        \"\"\"Builds all examples string concatenated by delimiter.\"\"\"\n        example_dicts = self.build_examples_dict(input_dict)\n\n        filled_examples = \"\".join([self.build_filled_example(example) for example in example_dicts])\n        if not filled_examples:\n            examples = \"\"\n        else:\n            examples = f\"{self.config.few_shot_examples.prefix}{filled_examples}{self.config.few_shot_examples.suffix}\"\n        user_message = self.config.user.format(examples=examples, **input_dict)\n        return user_message\n\n    def get_code_execution_args(self):\n        \"\"\"Returns the code execution arguments.\"\"\"\n        if self.config.code_tags is None:\n            raise ValueError(\n                \"Please provide 'code_tags' in your prompt configuration before calling get_code_execution_args().\"\n            )\n        return {\n            \"code_begin\": self.config.code_tags.code_begin,\n            \"code_end\": self.config.code_tags.code_end,\n            \"code_output_begin\": self.config.code_tags.code_output_begin,\n            \"code_output_end\": self.config.code_tags.code_output_end,\n            \"code_output_format\": self.config.code_tags.code_output_format,\n        }\n\n    def format_assistant_response(\n        self, content: str, thinking: str | None = None, chat_template_kwargs: dict | None = None\n    ) -> str:\n        \"\"\"Adds special tokens to the end of assistant response and formats thinking if provided\"\"\"\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer is not set.\")\n\n        messages = [{\"role\": \"user\", \"content\": \"\"}]\n\n        chat_template_kwargs = chat_template_kwargs or {}\n        user_string = self.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True, **chat_template_kwargs\n        )\n\n        messages.append({\"role\": \"assistant\", \"content\": content})\n        if thinking is not None:\n            messages[-1][\"thinking\"] = thinking\n        assistant_string = self.tokenizer.apply_chat_template(messages, tokenize=False, **chat_template_kwargs)\n\n        assert assistant_string.startswith(user_string), f\"Something is wrong\\n{user_string}\\n||\\n{assistant_string}\"\n\n        formatted_response = assistant_string[len(user_string) :]\n        if thinking is not None:\n            # Check that thinking is part of the assistant string\n            # If not, the thinking string should be added to the \"content\" field during preprocessing\n            # Tokenizers for models like Qwen3-4B don't add thinking to the assistant string by themselves\n            assert thinking in assistant_string, (\n                f\"The thinking content is not part of the assistant string. We suggest you add the thinking string to the 'content' field during preprocessing.\\n\\nThinking string:{thinking}\\n\\nAssistant string:{formatted_response}\"\n            )\n\n        return formatted_response\n\n    def fill(\n        self,\n        input_dict: Dict[str, str],\n        start_assistant_response_key: str | None = None,\n        chat_template_kwargs: dict | None = None,\n        format_as_string=False,\n    ) -> str | List[dict]:\n        \"\"\"\n        Fills the prompt with the input_dict.\n        Operates in two modes:\n        - If `self.tokenizer` is set, it will use it to format the prompt, returning a string.\n        - If `self.tokenizer` is not set, it will assume chat format and return a list of dictionaries.\n\n        Args:\n            input_dict: The input dictionary to fill the prompt with.\n            start_assistant_response_key: Whether to append the value of this key to the beginning of assistant response.\n            chat_template_kwargs: Any extra parameters to pass to the tokenizer's apply_chat_template method.\n            format_as_string: When False (default) we will just return a list of messages, when format_as_string is True, we will return a string using the tokenizer's apply_chat_template method (and fail if the tokenizer is not set).\n\n        Returns:\n            The filled prompt - either a string or a list of dictionaries.\n        \"\"\"\n\n        if self.config.system is not None:\n            messages = [\n                {\"role\": \"system\", \"content\": self.config.system},\n            ]\n        else:\n            messages = []\n        messages.append({\"role\": \"user\", \"content\": self.build_user_message(input_dict)})\n\n        if not format_as_string:\n            if start_assistant_response_key:\n                raise ValueError(\"start_assistant_response_key is not supported for chat template format.\")\n\n            if chat_template_kwargs:\n                raise ValueError(\"chat_template_kwargs can only be used when format_as_string=True\")\n\n            return messages\n        else:\n            if self.tokenizer is None:\n                raise ValueError(\"tokenizer is not set, can't format messages as a string\")\n\n            chat_template_kwargs = chat_template_kwargs or {}\n            try:\n                messages_string = self.tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True,\n                    **chat_template_kwargs,\n                )\n            except ValueError as e:\n                if \"Cannot use chat template functions because tokenizer.chat_template is not set\" in str(e):\n                    # assuming that's a base model and we just need to add bos\n                    if len(messages) != 1 or messages[0][\"role\"] != \"user\":\n                        raise ValueError(\n                            \"The model doesn't support chat template, can't format messages which contain non-user values\"\n                        )\n                    if hasattr(self.tokenizer, \"bos_token\"):\n                        messages_string = self.tokenizer.bos_token + messages[0][\"content\"]\n                    else:\n                        messages_string = messages[0][\"content\"]\n                else:\n                    raise e\n            if start_assistant_response_key:\n                messages_string += input_dict[start_assistant_response_key]\n            return messages_string\n\n    def __str__(self):\n        return str(self.config)\n\nFile Path: nemo_skills/prompt/few_shot_examples/gsm8k.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Source https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yaml\n# Derived from the CoT paper - https://arxiv.org/pdf/2201.11903 - Table 20\ngsm8k_standard_few_shot = [\n    {\n        \"problem\": \"There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\",\n        \"solution\": \"There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is \\\\boxed{6}.\",\n    },\n    {\n        \"problem\": \"If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\",\n        \"solution\": \"There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is \\\\boxed{5}.\",\n    },\n    {\n        \"problem\": \"Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\",\n        \"solution\": \"Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is \\\\boxed{39}.\",\n    },\n    {\n        \"problem\": \"Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\",\n        \"solution\": \"Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is \\\\boxed{8}.\",\n    },\n    {\n        \"problem\": \"Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\",\n        \"solution\": \"Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is \\\\boxed{9}.\",\n    },\n    {\n        \"problem\": \"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\",\n        \"solution\": \"There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is \\\\boxed{29}.\",\n    },\n    {\n        \"problem\": \"Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\",\n        \"solution\": \"Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is \\\\boxed{33}.\",\n    },\n    {\n        \"problem\": \"Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\",\n        \"solution\": \"Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is \\\\boxed{8}.\",\n    },\n]\n\n\ngsm8k_text_with_code = [\n    {\n        \"problem\": \"Missy had a giant piggy bank in her bedroom.  Every day she would search the house for change to put in her bank.  After 4 years, the bank was opened and it contained $450 in change.  If the second, third, and fourth-year she doubled the amount of money she put in the bank from the amount she had put in in the previous year, how much money, in dollars, did she put in the bank the first year?\",\n        \"solution\": \"\"\"\nMissy started with some amount of money in the first year.\nShe was then doubling the amount of money every year for 4 years and ended up with $450.\nLet's write down an equation for this problem and solve it using sympy.\n{code_begin}from sympy import solve, symbols\nfirst_year_money = symbols('first_year_money')\nsecond_year_money = 2 * first_year_money\nthird_year_money = 2 * second_year_money\nfourth_year_money = 2 * third_year_money\n# Solve for first year money given that the combined saving is 450\nresult = solve(first_year_money + second_year_money + third_year_money + fourth_year_money - 450, first_year_money)[0]\nresult\n{code_end}{code_output_begin}\n30\n{code_output_end}\nThus Missy put \\\\boxed{30} dollars in the bank the first year.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Pete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\",\n        \"solution\": \"\"\"\nSince Pete needs to take a 10 minutes walk and then a 1 hour 20 minutes train ride, he will spend a total of 1 hour and 30 minutes.\nThis means that he needs to leave 1 hour and 30 minutes earlier than 09:00 hours.\nSubtracting 1 hour and 30 minutes from 09:00 hours we get \\\\boxed{07:30} hours.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Mark deposited $88 in a bank. Bryan deposited $40 less than five times as much as Mark. How much did Bryan deposit in the bank?\",\n        \"solution\": \"\"\"\nLet's solve this problem using Python code.\n{code_begin}mark_deposit = 88\nfive_times_mark_deposit = 5 * mark_deposit\nbryan_deposit = five_times_mark_deposit - 40\nbryan_deposit\n{code_end}{code_output_begin}\n400\n{code_output_end}\nSo Bryan deposited \\\\boxed{400} dollars in the bank.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\",\n        \"solution\": \"\"\"\nLet's solve this problem using Python code.\n{code_begin}discount_percent = 20\nprice_before_discount = 120\ndiscount_portion = discount_percent / 100\ndiscount_amount = price_before_discount * discount_portion\nprice_after_discount = price_before_discount - discount_amount\nprice_after_discount\n{code_end}{code_output_begin}\n96.0\n{code_output_end}\nSo the new price of the lamp is \\\\boxed{96} dollars.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"James opens up a flower shop.  He needs to pay rent of $1200 a week with an additional 20% of rent to pay for utilities and he has 2 employees per shift with the store open 16 hours a day for 5 days a week.  If he pays each employee $12.50 an hour, what are his weekly expenses to run the store?\",\n        \"solution\": \"\"\"\nThe cost consists of rent, utilities, and employee salaries. Let's compute each of them separately and then add them up.\n{code_begin}# rent cost\nrent_per_week = 1200\n# utility cost\nutility_per_week = rent_per_week * 20 / 100\n# employee cost\nemployee_work_hours = 16\nwork_days_per_week = 5\nemployee_work_hours_per_week = work_days_per_week * employee_work_hours\nnumber_of_employees = 2\nemployee_cost_per_hour = 12.5\nemployees_cost_per_week = number_of_employees * employee_work_hours_per_week * employee_cost_per_hour\n# add the three to get total cost\ncost_per_week = rent_per_week + utility_per_week + employees_cost_per_week\ncost_per_week\n{code_end}{code_output_begin}\n3440.0\n{code_output_end}\nThus James's weekly expenses add up to \\\\boxed{3440} dollars.\n\"\"\".strip(),\n    },\n]\n\n\n# Solutions are only in text\ngsm8k_text_detailed = [\n    {\n        \"problem\": \"Missy had a giant piggy bank in her bedroom.  Every day she would search the house for change to put in her bank.  After 4 years, the bank was opened and it contained $450 in change.  If the second, third, and fourth-year she doubled the amount of money she put in the bank from the amount she had put in the previous year, how much money, in dollars, did she put in the bank the first year?\",\n        \"solution\": \"\"\"\nLet $x$ be the amount of money Missy put in the bank the first year.\nShe was then doubling the amount of money every year for 4 years.\nSo in the second year she put $2*x$ amount in her bank.\nIn the third year she put $2*2*x = 4*x$ amount in her bank.\nAnd in the fourth year she put $2*4*x = 8*x$ amount in her bank.\nSo the total amount she put in the bank is $x + (2*x) + (4*x) + (8*x) = 15*x$.\nAs the problem states, this total amounts to $450.\nSo $450 = 15*x$ which implies $x = 30$.\nThus, Missy put \\\\boxed{30} dollars in the bank the first year.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Pete has to take a 10-minute walk down to the train station and then board a 1hr 20-minute train to LA. When should he leave if he cannot get to LA later than 0900 hours? (24-hr time)\",\n        \"solution\": \"\"\"\nSince Pete needs to take a 10 minutes walk and then a 1 hour 20 minutes train ride, he will spend a total of 1 hour and 30 minutes.\nThis means that he needs to leave 1 hour and 30 minutes earlier than 09:00 hours.\nSubtracting 1 hour and 30 minutes from 0900 hours we get \\\\boxed{07:30} hours.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Mark deposited $88 in a bank. Bryan deposited $40 less than five times as much as Mark. How much did Bryan deposit in the bank?\",\n        \"solution\": \"\"\"\nFive times of what Mark deposited is $88 * 5 = 440$.\nBryan deposited $440 - 40 = 400$.\nSo Bryan deposited \\\\boxed{400} dollars in the bank.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A department store displays a 20% discount on all fixtures. What will be the new price of a 25 cm high bedside lamp that was worth $120?\",\n        \"solution\": \"\"\"\nThe lamp was originally priced at $120.\nA 20% discount amounts to $120 * 20 / 100 = 24$.\nSo the discount reduces the price of the lamp to $120 - 24 = 96$.\nSo the new price of the lamp is \\\\boxed{96} dollars.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"James opens up a flower shop.  He needs to pay rent of $1200 a week with an additional 20% of rent to pay for utilities and he has 2 employees per shift with the store open 16 hours a day for 5 days a week.  If he pays each employee $12.50 an hour, what are his weekly expenses to run the store?\",\n        \"solution\": \"\"\"\nThe store expense consist of rent, utilities, and employee salaries.\nLet's compute each of these expenses separately at a week timescale and then add them up.\nThe expense due to rent is $1200.\nThe expense due to utilities is 20% of rent expense. Thus, it is $1200 * 20 / 100 = 240$.\nNow we calculate the expense due to employee salaries.\nEach employee works 16*5=80 hours per week.\nFor each employee the cost per week based on hourly wage of $12.5/hr is $12.5 * 80 = 1000$ per week.\nFor two employees, this amounts to $2 * 1000 = 2000$.\nAdding the cost of rent, utilities, and employee salaries amounts to $1200 + 240 + 2000 = 3440$.\nThus James's weekly expenses to run the store add up to \\\\boxed{3440} dollars.\n\"\"\".strip(),\n    },\n]\n\n\nexamples_map = {\n    \"gsm8k_standard_few_shot\": gsm8k_standard_few_shot,\n    \"gsm8k_text_with_code\": gsm8k_text_with_code,\n    \"gsm8k_problem_augmentation\": gsm8k_problem_augmentation,\n    \"gsm8k_text_detailed\": gsm8k_text_detailed,\n}\n\nFile Path: nemo_skills/prompt/few_shot_examples/math.py\nContent:\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,S\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.N\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Source https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/minerva_math/utils.py#L42\nmath_standard_few_shot = [\n    {\n        \"problem\": \"Find the domain of the expression  $\\\\frac{\\\\sqrt{x-2}}{\\\\sqrt{5-x}}$.}\",\n        \"solution\": \"The expressions inside each square root must be non-negative. Therefore, $x-2 \\\\ge 0$, so $x\\\\ge2$, and $5 - x \\\\ge 0$, so $x \\\\le 5$. Also, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$. Therefore, the domain of the expression is \\\\boxed{[2,5)}$.\",\n    },\n    {\n        \"problem\": \"If $\\\\det \\\\mathbf{A} = 2$ and $\\\\det \\\\mathbf{B} = 12,$ then find $\\\\det (\\\\mathbf{A} \\\\mathbf{B}).$\",\n        \"solution\": \"We have that $\\\\det (\\\\mathbf{A} \\\\mathbf{B}) = (\\\\det \\\\mathbf{A})(\\\\det \\\\mathbf{B}) = (2)(12) = \\\\boxed{24}.$\",\n    },\n    {\n        \"problem\": \"Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\",\n        \"solution\": \"If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\\\cdot 12\\\\cdot20=480$ pounds of weight.  If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\\\cdot15\\\\cdot n=30n$ pounds of weight.  Equating this to 480 pounds, we can solve for $n$:\\n\\\\begin{align*}\\n30n&=480\\\\\\n\\\\Rightarrow\\\\qquad n&=480/30=\\\\boxed{16}\\\\n\\\\end{align*}\",\n    },\n    {\n        \"problem\": \"If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero,\\nfind $\\\\frac{a}{b},$ assuming $b$ is nonzero.\",\n        \"solution\": \"If we multiply the first equation by $-\\\\frac{3}{2}$, we obtain\\n\\n$$6y-9x=-\\\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have\\n\\n$$-\\\\frac{3}{2}a=b\\\\Rightarrow\\\\frac{a}{b}=\\\\boxed{-\\\\frac{2}{3}}.$$\",\n    },\n]\n\n\nmath_text_detailed = [\n    {\n        \"problem\": \"A parabola with equation $y=x^2+bx+c$ passes through the points $(-1,-11)$ and $(3,17)$. What is $c$?\",\n        \"solution\": \"\"\"\nFrom the question we know that points $(-1, -11)$ and $(3, 17)$ lie on the parabola. This means that when we substitute $x$ and $y$ from these points into the equation $y = x^2 + bx + c$, the equation must hold true. We substitute these two points into the given equation to solve for $c$.\n\nFor the point $(-1, -11)$:\n\nSubstitute $x = -1$ and $ y = -11 $ into the equation:\n\\[ -11 = (-1)^2 + b(-1) + c \\\\Rightarrow -11 = 1 - b + c \\\\Rightarrow -b + c = -12 \\]\n\nFor the point $(3, 17)$:\n\nSubstitute $x = 3$ and $y = 17$ into the equation:\n\\[ 17 = (3)^2 + b(3) + c \\\\Rightarrow 17 = 9 + 3b + c \\\\Rightarrow 3b + c = 8 \\]\n\nIn summary, we have the two equations\n\\\\begin{align*}\n-b + c &= -12\\\\\\n3b + c &= 8\n\\\\end{align*}\n\nTo solve for $c$ we can eliminate $b$ by multiplying the first equation by 3 and adding equations together.\nMultiplying the first equation by 3, we have $3(-b + c) = 3 (-12) \\\\Rightarrow -3b + 3c = -36$. Adding equations together gives us\n\\[ (-3b + 3c) + (3b + c) = -36 + 8 \\\\Rightarrow -3b + 3b + 3c + c = -28 \\\\Rightarrow 4c = -28 \\\\Rightarrow c = -28 : 4 \\\\Rightarrow c = \\\\boxed{-7} \\]\n\"\"\".strip(),\n    },\n    {\n        \"problem\": 'Let $f(x)$ be an odd function.  Is $f(f(x))$ even, odd, or neither?\\n\\nEnter \"odd\", \"even\", or \"neither\".',\n        \"solution\": \"\"\"\nTo determine whether $f(f(x))$ is even, odd, or neither, we need to use the property of $f(x)$ being an odd function.\n\nAn odd function is defined as:\n\\[ f(-x) = -f(x) \\\\quad \\\\text{for all } x \\]\n\nGiven that $f(x)$ is odd, let's find $f(f(-x))$ and see how it relates to $f(f(x))$.\n\n1. Substitute $-x$ into the function $f(x)$:\n\\[ f(-x) \\]\n\n2. Since $f(x)$ is odd, apply the definition of an odd function:\n\\[ f(-x) = -f(x) \\]\n\n3. Now substitute $-f(x)$ into the function $f$:\n\\[ f(f(-x)) = f(-f(x)) \\]\n\n4. Again, using the fact that $f(x)$ is odd, apply the definition:\n\\[ f(-f(x)) = -f(f(x)) \\]\n\n5. We have found that:\n\\[ f(f(-x)) = -f(f(x)) \\]\n\nThis matches the definition of an odd function.\n\nSo, the answer is:\n\\[ \\\\boxed{\\\\text{odd}} \\]\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A rectangular box $P$ is inscribed in a sphere of radius $r$. The surface area of $P$ is 384, and the sum of the lengths of its 12 edges is 112. What is $r$?\",\n        \"solution\": \"\"\"\nLet the dimensions of the rectangular box $P$ be $x$, $y$, and $z$. We know the following:\n\n1. The sum of the lengths of the edges of $P$ is\n\\[ 4(x + y + z) = 112 \\\\Rightarrow x + y + z = 112 : 4 \\\\Rightarrow x + y + z = 28 \\]\n\n2. The surface area of $P$ is\n\\[ 2xy + 2yz + 2xz = 384 \\\\Rightarrow xy + yz + xz = 384 : 2 \\\\Rightarrow xy + yz + xz = 192 \\]\n\nSince the box is inscribed in the sphere, the diagonal of the box is the diameter of the sphere. The length of the diagonal is $\\\\sqrt{x^2 + y^2 + z^2}$\n\nThe diameter of the sphere is $2r$, so:\n\\[ 2r = \\\\sqrt{x^2 + y^2 + z^2} \\\\Rightarrow (2r)^2 = x^2 + y^2 + z^2 = (x + y + z)^2 - (2xy + 2yz + 2xz) \\]\n\nSubstitute the known values:\n\\[ 4r^2 = 28^2 - 384 = 784 - 384 = 400 \\\\Rightarrow r^2 = 100 \\\\Rightarrow r = \\\\boxed{10} \\]\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"Let $\\\\mathbf{a} = \\\\begin{pmatrix} 2 \\\\\\\\ 1 \\\\\\\\ 5 \\\\end{pmatrix}.$  Find the vector $\\\\mathbf{b}$ such that $\\\\mathbf{a} \\\\cdot \\\\mathbf{b} = 11$ and\\n\\[\\\\mathbf{a} \\\\times \\\\mathbf{b} = \\\\begin{pmatrix} -13 \\\\\\\\ -9 \\\\\\\\ 7 \\\\end{pmatrix}.\\n\\]\",\n        \"solution\": \"\"\"\nLet $\\\\mathbf{b} = \\\\begin{pmatrix} x \\\\\\\\ y \\\\\\\\ z \\\\end{pmatrix}$.\n\nFirst, use the dot product condition:\n\\[ \\\\mathbf{a} \\\\cdot \\\\mathbf{b} = 11 \\\\Rightarrow 2x + y + 5z = 11 \\]\n\nNext, use the cross product condition:\n\\[ \\\\mathbf{a} \\\\times \\\\mathbf{b} = \\\\begin{pmatrix} 2 \\\\\\\\ 1 \\\\\\\\ 5 \\\\end{pmatrix} \\\\times \\\\begin{pmatrix} x \\\\\\\\ y \\\\\\\\ z \\\\end{pmatrix} = \\\\begin{pmatrix} -5y + z \\\\\\\\ 5x - 2z \\\\\\\\ -x + 2y \\\\end{pmatrix} = \\\\begin{pmatrix} -13 \\\\\\\\ -9 \\\\\\\\ 7 \\\\end{pmatrix} \\]\n\nThis gives us the system of equations:\n   \\\\begin{align*}\n   2x + y + 5z = 11 \\\\\\\\ \\\\quad &(1) \\\\\\\\\n   -5y + z = -13 \\\\\\\\ \\\\quad &(2) \\\\\\\\\n   5x - 2z = -9 \\\\\\\\ \\\\quad &(3) \\\\\\\\\n   -x + 2y = 7 \\\\\\\\ \\\\quad &(4)\n   \\\\end{align*}\n\nSolve for $x$, $y$, and $z$ step-by-step:\n\nFrom (2), $z = 5y - 13$.\nFrom (4), $x = 2y - 7$.\n\nSubstitute $z = 5y - 13$ into (1):\n\\[ 2(2y - 7) + y + 5(5y - 13) = 11 \\\\Rightarrow 4y - 14 + y + 25y - 65 = 11 \\\\Rightarrow 30y - 79 = 11 \\\\Rightarrow 30y = 90 \\\\Rightarrow y = 3 \\]\n\nNow find $x$ and $z$:\n\\[ x = 2y - 7 = 2(3) - 7 = -1 \\]\n\n\\[ z = 5y - 13 = 5(3) - 13 = 2 \\]\n\nThus, the vector $\\\\mathbf{b}$ is:\n\\[ \\\\mathbf{b} = \\\\boxed{\\\\begin{pmatrix} -1 \\\\\\\\ 3 \\\\\\\\ 2 \\\\end{pmatrix}} \\]\n\"\"\".strip(),\n    },\n]\n\nmath_text_with_code = [\n    {\n        \"problem\": \"A parabola with equation $y=x^2+bx+c$ passes through the points $(-1,-11)$ and $(3,17)$. What is $c$?\",\n        \"solution\": \"\"\"\nLet's write down an equation for the parabola and solve for $c$ using sympy.\n{code_begin}import sympy as sp\n\n# define the symbols\nx, y, b, c = sp.symbols('x y b c')\n\n# define the parabola equation\nparabola_eq = sp.Eq(y, x**2 + b*x + c)\n\n# the parabola passes through the points (-1,-11) and (3,17)\n# so we substitute these points into the parabola equation\npoint_1 = parabola_eq.subs({x: -1, y: -11})\npoint_2 = parabola_eq.subs({x: 3, y: 17})\n\n# we now have two equations and two unknowns (b and c)\n# we will solve for b and c\nsolutions = sp.solve((point_1,point_2), (b, c))\nsolutions[c]\n{code_end}{code_output_begin}\n-7\n{code_output_end}\nSo c is \\\\boxed{-7}\n\"\"\".strip(),\n    },\n    {\n        \"problem\": 'Let $f(x)$ be an odd function.  Is $f(f(x))$ even, odd, or neither?\\n\\nEnter \"odd\", \"even\", or \"neither\".',\n        \"solution\": \"\"\"\nThe definition of an odd function is that $f(-x) = -f(x)$.\nApplying this to $f(f(-x))$ we get $f(f(-x)) = f(-f(x)) = -f(f(x))$.\nThus, $f(f(x))$ is an \\\\boxed{odd} function.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"At the 2007 Math Olympics, Team Canada won $17$ out of a possible $100$ medals. Which one of the following is closest to the fraction of medals that they won? $$\\n\\\\frac{1}{4} \\\\qquad \\\\frac{1}{5} \\\\qquad \\\\frac{1}{6} \\\\qquad \\\\frac{1}{7} \\\\qquad \\\\frac{1}{8}\\n$$\",\n        \"solution\": \"\"\"\nLet's use sympy to print out the differences between the fraction of medals that Canada won and each of the options.\n{code_begin}from sympy import Rational, Abs\n\n# team Canada won 17 out of 100 medals\nmedal_frac = Rational(17, 100)\n\n# list of options\noptions = [Rational(1, 4), Rational(1, 5), Rational(1, 6), Rational(1, 7), Rational(1, 8)]\n\n# let's print out the differences\n[Abs(medal_frac - frac_option) for frac_option in options]\n{code_end}{code_output_begin}\n[2/25, 3/100, 1/300, 19/700, 9/200]\n{code_output_end}\nLet's now check which difference is the smallest.\n{code_begin}import numpy as np\n\n# Calculate the idx of the closest option\nmin_idx = np.argmin([2/25, 3/100, 1/300, 19/700, 9/200])\n\n# Print the closest option\nprint(options[min_idx])\n{code_end}{code_output_begin}\n1/6\n{code_output_end}\nSo the answer is \\\\boxed{1/6}.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A rectangular box $P$ is inscribed in a sphere of radius $r$. The surface area of $P$ is 384, and the sum of the lengths of its 12 edges is 112. What is $r$?\",\n        \"solution\": \"\"\"\nLet the dimensions of $P$ be $x$, $y$, and $z$.\nThe sum of the box's edges is $4(x + y + z)$ and the surface area is $2xy + 2yz + 2xz$.\nThe diameter of the sphere is $2r$ and it's equal to the diagonal of the box.\nLet's now write down the equations based on the above information and solve them using sympy.\n{code_begin}from sympy import symbols, Eq, solve\n\n# define the variables\nx, y, z, r = symbols('x y z r')\n\n# equations based on the given information\neq1 = Eq(2 * (x*y + x*z + y*z), 384)\neq2 = Eq(4 * (x + y + z), 112)\n\n# the diagonal of the box is the diameter of the sphere\ndiagonal_eq = Eq(x**2 + y**2 + z**2, (2*r)**2)\n\n# solve the equations\nsolutions = solve((eq1, eq2, diagonal_eq), (x, y, z, r))\n\n# let's see which values of r we get\n[solution[-1] for solution in solutions]\n{code_end}{code_output_begin}\n[-10, -10, 10, 10]\n{code_output_end}\nSince the radius of the sphere is positive, we get $r = \\\\boxed{10}$.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"A bee starts flying from point $P_0$. She flies $1$ inch due east to point $P_1$. For $j \\\\ge 1$, once the bee reaches point $P_j$, she turns $30^{\\\\circ}$ counterclockwise and then flies $j+1$ inches straight to point $P_{j+1}$. When the bee reaches $P_{2015},$ how far from $P_0$ is she, in inches?\",\n        \"solution\": \"\"\"\nWe can represent the rotation action via $\\\\omega = e^{\\\\pi i/6}$.\nLet's assume the bee starts at the origin, so $P_{2015}$ is at the point \\[z = 1 + 2 \\\\omega + 3 \\\\omega^2 + 4 \\\\omega^3 + \\\\dots + 2015 \\\\omega^{2014}.\\n\\]\nThis is an arithmetic-geometric series which we can solve by simplifying the expression.\nAlternatively, we can solve for |z| using sympy using the following code.\n{code_begin}from sympy import I, pi, exp, sqrt\n\n# rotation action of 30 degrees\nomega = exp(I * pi/6)\n\nposition = 0\n\nfor i in range(2015):\n    delta = (i + 1) * omega**(i)\n    position += delta\n\nreal, img = (position.as_real_imag())\n# Distance from origin i.e. starting point\ndist = sqrt(real**2 + img**2)\nprint(dist)\n{code_end}{code_output_begin}\nsqrt(2)*(1008 + 1008*sqrt(3))\n{code_output_end}\nSo the bee is $\\\\boxed{1008\\\\sqrt{2} + 1008\\\\sqrt{6}}$ far from the starting point.\n\"\"\".strip(),\n    },\n]\n\n\nmath_problem_augmentation = [\n    {\n        \"problem\": \"\"\"\nIn the equation\n$$5x^2-kx+1=0$$\ndetermine $k$ such that the difference of the roots be equal to unity.\n\"\"\".strip(),\n        \"augmented_problem\": \"\"\"\nThe roots $x_1$ and $x_2$ of the equation\n$$x^2-3ax+a^2=0$$\nare such that\n$x_1^2+x_2^2=1.75$.\nDetermine $a$.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"\"\"\nSolve the following equation\n$\\ds\\\\f{3+x}{3x}=\\\\sqrt {\\\\ds\\\\f{1}{9}+\\\\ds\\\\f{1}{x}\\\\sqrt {\\\\ds\\\\f{4}{9}+\\\\ds\\\\f{2}{x^2}}}$\n\"\"\".strip(),\n        \"augmented_problem\": \"\"\"\nSolve the following equation\n$\\\\sqrt {1+x\\\\sqrt {x^2+24}}=x+1$\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"\"\"\nIn an infinitely decreasing geometric progression the sum\nof all the terms occupying odd places is equal to 36, and that of all\nthe terms at even places equals 12.\nFind the progression.\n\"\"\".strip(),\n        \"augmented_problem\": \"\"\"\nThe sum of the terms of an infinitely decreasing geometric\nprogression is equal to 56, and the sum of the squared terms of the\nsame progression is 448.\nFind the first term and the common ratio.\n\"\"\".strip(),\n    },\n    {\n        \"problem\": \"\"\"\nTwo railway stations are at a distance of 96 km from each other.\nOne train covers this distance 40 minutes faster than does the other.\nThe speed of the first train is 12 km/h higher than that of the second.\nDetermine the speed of both trains.\n\"\"\".strip(),\n        \"augmented_problem\": \"\"\"\nA student was asked to multiply 78 by a two-digit number\nin which the tens digit was three times as large as the units digit;\nby mistake, he interchanged the digits in the second factor and\nthus obtained a product smaller than the true product by 2808.\nWhat was the true product?\n\"\"\".strip(),\n    },\n]\n\n\nexamples_map = {\n    \"math_standard_few_shot\": math_standard_few_shot,\n    \"math_text_with_code\": math_text_with_code,\n    \"math_text_detailed\": math_text_detailed,\n    \"math_problem_augmentation\": math_problem_augmentation,\n    # 7 subjects\n    \"math_text_with_code_algebra\": math_text_with_code_algebra,\n    \"math_text_with_code_probability\": math_text_with_code_probability,\n    \"math_text_with_code_intermediate_algebra\": math_text_with_code_intermediate_algebra,\n    \"math_text_with_code_number_theory\": math_text_with_code_number_theory,\n    \"math_text_with_code_geometry\": math_text_with_code_geometry,\n    \"math_text_with_code_precalculus\": math_text_with_code_precalculus,\n    \"math_text_with_code_prealgebra\": math_text_with_code_prealgebra,\n}\n",
    "Experiment Result": "Dataset was constructed by synthesizing code-interpreter style solutions for GSM8K and MATH benchmarks. Few-shot prompting (K=5 examples) was employed using 'masked text solution prompting' which involves masking intermediate numerical computations in reference text solutions with symbols (e.g., M, N) to prevent LLMs from generating shortcut answers by copying. Solution sampling was performed with a temperature of 1.0 and top_p of 0.95, subject to token and code block limits. Post-processing steps included removing syntactically noisy solutions (e.g., multiple answer blocks, incomplete code). For fine-tuning, data selection strategies included 'fair downsampling' for balanced problem representation and 'Any-Code' filtering, which prioritizes code-based solutions over text-based ones for precision."
}{
    "Title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
    "Main Contributions": "The paper introduces OpenMathInstruct-1, a novel, large-scale (1.8M problem-solution pairs) math instruction tuning dataset, which is publicly released under a commercially permissive license. This dataset is generated using the open-source Mixtral model, addressing the reliance of previous state-of-the-art math datasets on closed-source, commercially restrictive LLMs like GPT-4. The key technical contribution is a new prompting strategy using 'masked text solutions' that significantly boosts the quality and coverage of synthetically generated code-interpreter style solutions (99.9% for GSM8K and 93% for MATH). Models fine-tuned on OpenMathInstruct-1, such as OpenMath-CodeLlama-70B, achieve competitive performance on math reasoning benchmarks (e.g., 84.6% on GSM8K and 50.7% on MATH), often outperforming or matching GPT-distilled counterparts.",
    "Methodology": "The methodology involves synthesizing code-interpreter style solutions for GSM8K and MATH benchmarks using the Mixtral 8x7B model. Few-shot prompting is employed, with various strategies explored: a default 5-shot prompt, subject-specific prompts for MATH, and a novel 'masked text solution prompting'. The masked text solution approach replaces intermediate numerical computations in reference text solutions with symbols in the prompt to prevent LLMs from 'shortcutting' by directly copying answers. Solution generation includes specific constraints (e.g., max 4096 tokens, max 3 code blocks) and post-processing to filter syntactically noisy solutions. For fine-tuning models, data selection strategies include 'fair downsampling' (to ensure balanced problem representation) and 'code-preferential solutions' (to prioritize more precise code-based solutions). Models are trained using the NeMo toolkit with AdamW optimizer and specific learning rates for different base models.",
    "Experimental Setup": "The dataset generation primarily focuses on GSM8K and MATH benchmarks. The core foundation model used for synthetic data generation is Mixtral 8x7B. Fine-tuning experiments are conducted on various open-source base models including Mistral-7B, Llama 2 (7B, 13B, 70B), and CodeLlama (7B, 13B, 34B, 70B). The fine-tuning corpus consists of a mixture of 512K fair downsampled GSM8K instances and 512K MATH instances with 'Any-Code' filtering, totaling approximately 1.02M problem-solution pairs. Evaluation is performed on a suite of popular math reasoning benchmarks: GSM8K, MATH, GSM-Hard, SVAMP, TabMWP, ASDiv, and MAWPS. A 1K-sized validation set from GSM8K and MATH training data is used for ablation studies and hyperparameter tuning. Models are evaluated in a zero-shot setting using greedy decoding and self-consistency/majority voting (with k=50 for final results and k=4 for ablations).",
    "Limitations": "The primary limitations include the challenge of identifying and removing 'semantically noisy' solutions, where correct answers are derived from flawed reasoning, as this is beyond simple syntactic filters. The authors note a lack of robustness to input perturbations, evidenced by a performance drop on GSM-Hard compared to GSM8K. The current models show limitations in error recovery during code generation, unlike more advanced models like GPT-4. The dataset's and models' imperfections mean there are risks associated with their use in critical applications like evaluating student assignments or building math tutors. Furthermore, poor performance on geometry problems is attributed to the lack of multi-modality in the base models. Finally, the high computational cost of fine-tuning limits the ability to perform multiple runs for comprehensive statistical significance analysis.",
    "Future Research Directions": "Future research directions include the development of advanced semantic filters to detect and remove solutions with flawed reasoning from synthetically generated datasets, possibly leveraging more capable models like GPT-4 for grading. Another direction involves enhancing the fine-tuned models' ability to recover from errors during code generation, potentially by incorporating specific error recovery examples into the training data. Addressing the limitations posed by the lack of multi-modality in current base models is also a promising area for improving performance on tasks like geometry problems.",
    "Experiment Code": "The methodology involves several key implementation sections:\n\n1.  **Prompt Construction & Code Integration (`nemo_skills/prompt/utils.py`)**:\n    *   The `Prompt` class methods (`build_filled_example`, `build_examples_dict`, `build_user_message`) are responsible for dynamically constructing prompts. This includes integrating few-shot examples, which may contain embedded code blocks and their execution outputs. It utilizes `format_code_output` from `nemo_skills/code_execution/utils.py` to correctly render execution results within the prompt.\n\n2.  **Code Execution Loop (`nemo_skills/inference/model/code_execution.py`)**:\n    *   The `CodeExecutionWrapper` class, specifically its `_generate_single` method, orchestrates the iterative process of interacting with the LLM. This involves:\n        *   Sending an initial prompt.\n        *   Extracting any generated Python code using `extract_code_to_execute` from `nemo_skills/code_execution/utils.py`.\n        *   Executing this code in a sandbox environment (`execute_generated_code`).\n        *   Formatting the execution output and appending it back to the prompt for subsequent LLM turns.\n\n3.  **Data Filtering for Fine-tuning (`nemo_skills/training/data_preparation_utils/filters.py`)**:\n    *   `DropIncorrectCodeBlocks`: Filters solutions, retaining only those that contain exactly one code block.\n    *   `DropIncorrectArithmetic`: Verifies the mathematical correctness of arithmetic expressions within solutions using `solve_expression`.\n    *   `SplitArithmetic`: Expands simple arithmetic expressions into step-by-step solutions for enhanced clarity.\n    *   `CodeTextFilter`: Classifies solutions based on the presence of code blocks and can filter to prioritize code-containing solutions.\n\n4.  **Data Downsampling for Fine-tuning (`nemo_skills/training/data_preparation_utils/preprocessing.py`)**:\n    *   `ShuffleAndDownsampleData`: Implements various data downsampling techniques, including 'fair downsampling' (`sampling_method='fair'`), to ensure balanced problem representation across the dataset.\n\n5.  **Training Orchestration (`nemo_skills/training/nemo_rl/start_sft.py` and `nemo_skills/training/nemo_rl/start_grpo.py`)**:\n    *   These scripts contain the `setup_data` and `sft_preprocessor`/`ns_data_processor` functions. These prepare raw problem/solution pairs into `DatumSpec` objects, suitable for the NeMo-RL training framework, handling message logging and loss multipliers for sequence-to-sequence tasks.",
    "Experiment Result": "The experiments were conducted with a focus on synthesizing code-interpreter style solutions for GSM8K and MATH benchmarks. The following experimental settings and details were employed:\n\n1.  **Model & Benchmarks**:\n    *   The primary language model used was **Mixtral 8x7B**.\n    *   Evaluations and solution synthesis targeted **GSM8K** and **MATH** benchmarks.\n\n2.  **Few-shot Prompting Strategies**:\n    *   A **default 5-shot prompt** strategy was utilized.\n    *   **Subject-specific prompts for MATH** were employed, drawing examples from `nemo_skills/prompt/few_shot_examples/math.py` (e.g., `math_text_with_code_algebra`, `math_text_with_code_geometry`).\n    *   **'Masked text solution prompting'** was explored. This approach involves replacing intermediate numerical computations in reference text solutions with symbols (e.g., `{code_begin}`, `{code_output_begin}` placeholders in prompts) to prevent LLMs from 'shortcutting' by directly copying answers. The content of few-shot examples, such as `gsm8k_text_with_code` from `nemo_skills/prompt/few_shot_examples/gsm8k.py`, illustrates this style.\n    *   Few-shot example selection was configured via `nemo_skills/prompt/utils.py:FewShotExamplesConfig` parameters, including `examples_type` (e.g., `gsm8k_text_with_code`) and `retrieved_few_shots`.\n\n3.  **Solution Generation Constraints**:\n    *   **Maximum token generation**: Solutions were constrained by a maximum number of tokens, configured via `nemo_skills/inference/generate.py:GenerateSolutionsConfig.inference.tokens_to_generate` (e.g., a maximum of 4096 tokens).\n    *   **Maximum code blocks**: Solution generation was limited to a maximum number of code blocks per turn, configured via `nemo_skills/inference/model/code_execution.py:CodeExecutionConfig.max_code_executions` (e.g., a maximum of 3 code blocks).\n    *   **Code execution timeout**: Individual code execution steps in the sandbox were subject to a timeout, specified by `CodeExecutionConfig.code_execution_timeout`.\n    *   **Sandbox traceback verbosity**: The verbosity of tracebacks from the code execution sandbox could be set to 'Plain', 'Context', 'Verbose', or 'Minimal' via `CodeExecutionConfig.sandbox_traceback_verbosity`.\n\n4.  **Post-processing for Syntactically Noisy Solutions**:\n    *   Generated solutions underwent post-processing to filter out 'syntactically noisy solutions'. This involved checks for incorrect code blocks, mathematical inaccuracies, and formatting issues, implemented through various filters in `nemo_skills/training/data_preparation_utils/filters.py`.\n\n5.  **Fine-tuning Data Selection Strategies**:\n    *   **'Fair downsampling'**: This strategy was applied to ensure balanced problem representation, implemented via `nemo_skills/training/data_preparation_utils/preprocessing.py:ShuffleAndDownsampleData` with `sampling_method='fair'`.\n    *   **'Code-preferential solutions'**: Solutions with code were prioritized during data preparation, utilizing techniques like those found in `CodeTextFilter` (`filter_type='majority_code'` or `filter_type='any_code'`).\n    *   **Contamination checking**: Datasets were checked for contamination against benchmarks using `nemo_skills/inference/check_contamination.py:CheckContaminationConfig`, involving parameters like `prompt_config='judge/check-contamination'`, `retrieve_key='problem'`, `check_both_ways`, and `top_k`.\n\n6.  **Fine-tuning Model Training**:\n    *   Models were trained using the **NeMo toolkit**.\n    *   The **AdamW optimizer** was employed.\n    *   **Specific learning rates for different base models** were configured within the NeMo-RL training scripts (`nemo_skills/pipeline/nemo_rl/grpo.py` and `nemo_skills/pipeline/nemo_rl/sft.py`), for example, via parameters like `policy.optimizer.kwargs.lr`."
}{
    "Title": "Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads",
    "Main Contributions": "The paper addresses the absence of a systematic analysis of AI capabilities for joint vision and text reasoning by evaluating state-of-the-art Large Vision-and-Language Models (LVLMs) on mathematical and algorithmic reasoning problems from children's Olympiads. It introduces the SMART-840 dataset, comprising 840 visuo-linguistic problems from the Mathematical Kangaroo Olympiad (grades 1-12, 2020-2024). The key findings reveal that modern LVLMs demonstrate increasingly powerful reasoning skills for higher-grade problems but surprisingly struggle with foundational problems designed for younger children. Furthermore, there is no significant correlation between the reasoning capabilities of AI models and young children, suggesting LVLMs employ a different type of reasoning compared to human cumulative knowledge and logic skills. The study provides a first-of-its-kind benchmarking of LVLMs against children's problem-solving abilities across their developmental growth and analyzes performance on both text-only and vision-and-text problems.",
    "Methodology": "The study's methodology involves creating a new benchmark dataset, SMART-840, from Mathematical Kangaroo Olympiad problems (2020-2024, grades 1-12). This dataset includes 840 problems, extracted from PDF documents using OCR for text and manual cropping for images, with manual inspection for errors. Problems are categorized into geometry, logic, algebra, and numbers, and also classified as image-text or text-only. The methodology compares the zero-shot performance of seven popular and state-of-the-art LVLMs (including closed-source models like GPT-4o, Gemini-Pro, Claude-3 Opus/Sonnet, GPT-4v, Gemini-Flash, and open-source models like LLaVa-Next, XGen-MM-Phi3, InternVL-Chat-V1-2, InternLM-XComposer-2.5) against aggregated human performance data from the Olympiad. LVLMs are queried using a standardized prompt requesting explanations and specific answer option formats, with responses evaluated for correctness and validity. The research also conducts various correlation analyses (Difficulty Index, Discriminative Index, Weight Correlation, Entropy Correlation, Time-taken Correlation) between AI and children's performances to understand their reasoning alignment.",
    "Experimental Setup": "The experimental setup centers around the SMART-840 dataset, which consists of 840 problems from the Mathematical Kangaroo (MK) Olympiad spanning grades 1-12 from 2020-2024. The dataset includes human performance statistics such as answer responses and time taken. Seven state-of-the-art LVLMs were benchmarked: GPT-4o, Gemini-Pro, Claude-3 Opus, Claude-3 Sonnet, GPT-4v, Gemini-Flash, XGEN-MM-Phi3-Instruct-v1, LlaVa-Next (34B), InternVL-Chat-V1-2 (40B), and InternLM-XComposer-2.5 (7B). A hand-crafted prompt, 'Solve this question with explanation of the intermediate steps. Your response should end with one of the selected answer options from A1, B2, C3, D4, or E5,' was used for querying closed-source LVLMs via their API interfaces. Each problem was queried at least twice (up to 5 times for GPT-4o) to assess variance. Performance was measured by accuracy (percentage of correctly answered problems). A random baseline (approximately 20%) was established. Manual validation was conducted for GPT-4o responses (GPT-4o (M)) to ensure reasonableness and adherence to problem specifications. The study also analyzed category-level performance (geometry, logic, algebra, numbers) and separated image-text from text-only problem performances.",
    "Limitations": "The SMART-840 dataset is smaller in size compared to other recent and popular datasets used in AI mathematical reasoning benchmarks, which the authors state is 'by necessity' given the goal. The study specifically evaluates the zero-shot performance of LVLMs, meaning models were not trained or fine-tuned on the SMART-840 dataset to maintain fair comparison to human performance (where humans are assumed not to have seen the problems before). This approach means the dataset's primary purpose is to expose failure modes and disparities in machine cognition rather than serving as a training resource. The cost involved limited the number of runs for LVLMs when computing variance.",
    "Future Research Directions": "The paper does not explicitly state future research directions. However, based on the findings, potential areas for extension would include: investigating the underlying reasons for LVLMs' difficulty with foundational, younger-grade mathematical and logical problems; developing novel AI architectures or training methodologies that more closely mimic the cumulative knowledge building and reasoning progression observed in human children; exploring how to imbue AI models with core competencies that allow for generalization beyond mere similarity to training examples; and further analyzing the multimodal integration challenges, especially in geometry and logic problems, where LVLMs significantly falter. Additionally, the study's insights could lead to research on improving the reliability and reducing the variance of LVLM responses, as well as developing better methods to align AI reasoning with human cognitive processes at different developmental stages.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "Main Contributions": "Addresses the problem of high manual effort and computational cost in tuning learning rate (LR) schedules for Deep Neural Networks (DNNs). Proposes AutoLRS, an efficient method for automatic LR optimization by modeling training dynamics. Key contributions include using Bayesian Optimization (BO) on the fly to minimize validation loss, and introducing a lightweight exponential time-series forecasting model to predict validation loss after a stage based on short-term observations, significantly reducing the BO search cost. AutoLRS achieves significant speedups (1.22x for ResNet-50, 1.43x for Transformer, 1.5x for BERT) compared to original hand-tuned LR schedules, and an average speedup of 1.31x over heavily-tuned state-of-the-art schedules (CLR, SGDR), while maintaining or improving test-set performance. It is generalizable, compatible with most optimizers, and computationally efficient with low memory overhead.",
    "Methodology": "AutoLRS divides the training process into multiple stages, each of τ steps, and dynamically optimizes a constant LR for each stage. Bayesian Optimization (BO) is employed to treat the validation loss (objective function) as a black-box function of LR. To overcome the high computational cost of BO (which would typically require τ training steps per LR evaluation), AutoLRS evaluates each candidate LR for only τ' ≪ τ steps. An exponential time-series forecasting model, L(t) = aexp(bt) + c (with b<0), is trained on the validation loss observed during these τ' steps to predict the validation loss after τ steps. This creates a 'mutual-training' process between BO and the forecasting model. The BO uses a Gaussian Process (GP) with a Matern kernel as its surrogate model and Lower Confidence Bound (LCB) as its acquisition function to balance exploration and exploitation. Practical improvements include gradually increasing τ and τ' over training stages (starting with τ=1000, τ'=100 and doubling until τmax) and using training loss as an approximation for validation loss in early stages to save computation. Model parameters and optimizer states are checkpointed and restored for each BO iteration to ensure fair comparisons.",
    "Experimental Setup": "AutoLRS was evaluated on three representative DNNs: ResNet-50 for ImageNet classification (SGD with momentum), Transformer for WMT 2014 English-German NMT (Adam), and BERTBASE for pre-training and fine-tuning on MRPC, MNLI, CoLA, and SQuAD v1.1 NLP tasks (Adam with L2 weight decay). Additional ablation studies and sensitivity tests were performed using VGG-16 and ResNet-50 on CIFAR-10 and CIFAR-100. Experiments were conducted on NVIDIA Tesla V100 GPUs and NVIDIA Titan RTX GPUs with varying batch sizes (e.g., 1024 for ResNet-50/BERT, 128 for VGG-16 on CIFAR). Baselines included original hand-tuned LR schedules, heavily hand-tuned Cyclical Learning Rate (CLR) and SGDR (requiring ≥10 trials per model), Hypergradient Descent (HD), MARTHE, and Hyperband. AutoLRS hyperparameters were set as: k=10 (LRs for BO), τ'=τ/10, κ=1000, initial τ=1000, τ'=100, doubling until τmax (8000 for ResNet-50/Transformer, 32000 for BERT). LR search intervals (ηmin,ηmax) were determined by LR range tests and varied per model (e.g., 10^-3 to 1 for ResNet-50). Validation metrics included top-1 accuracy (ImageNet/CIFAR), BLEU score (Transformer), and accuracy/F1 score (BERT fine-tuning).",
    "Limitations": "AutoLRS does not currently search for LRs during the warmup phase of training, as it lacks an explicit optimization objective for this period. While AutoLRS significantly reduces manual tuning, it still has several hyperparameters (e.g., κ, initial τ, τ', τmax, and the LR search interval) that could potentially be fine-tuned further for optimal performance, though current defaults and sensitivity tests indicate robustness. The lightweight exponential forecasting model may occasionally struggle to fully capture highly non-monotone or unstable loss changes in very early training steps, although an iterative spline smoothing pre-processing step helps to mitigate this. The method incurs a computational overhead for the BO search, costing slightly over twice the training time associated with the identified LR schedule, though this is substantially less than other automated hyperparameter tuning methods.",
    "Future Research Directions": "Future work could involve fully automating the selection of AutoLRS's own hyperparameters (e.g., κ, initial τ, τ', τmax, and the LR search interval) to further reduce human involvement. Enhancing the time-series forecasting model to be more robust to complex or highly non-monotone loss dynamics could improve its predictive accuracy in all scenarios. Expanding AutoLRS to optimize learning rate schedules even during the warmup phase of training, or integrating it with the tuning of other optimizer-specific hyperparameters, presents additional avenues. Furthermore, applying AutoLRS to a wider array of deep learning architectures, emerging tasks, and diverse datasets would demonstrate its broader applicability and potential impact.",
    "Experiment Code": "import torch\nimport os\nimport logging\nimport socket\nimport string\nimport random\nimport time\n\nclass AutoLRS():\n    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):\n        self._net = model\n        self._optimizer = optimizer\n        self._val_fn = val_fn \n        self._lr = 0.000001\n        self._warmup_steps = warmup_steps\n        self._warmup_lr = warmup_lr \n        self._global_step = 0\n        self._socket = socket.socket()\n        self._started = False\n        self._summary_steps = summary_steps\n        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'\n        self._listening_host = listening_host\n        self._listening_port = listening_port\n        self._best_acc = 0\n\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n\n        self.connect_server()\n    \n    def connect_server(self):\n        self._socket.connect((self._listening_host, self._listening_port))\n\n    def _verbose_operation(self, _op):\n        if self._global_step % self._summary_steps == 0:\n            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))\n\n    def save_variables(self):\n        \"\"\"Save model parameters and optimizer states.\"\"\"\n        _start_time = time.time()\n        torch.save({\n            'model_state_dict': self._net.state_dict(),\n            'optimizer_state_dict': self._optimizer.state_dict()\n            }, self._checkpoint_path)\n        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def restore_variables(self):\n        _start_time = time.time()\n        checkpoint = torch.load(self._checkpoint_path)\n        self._net.load_state_dict(checkpoint['model_state_dict'])\n        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))\n\n    def on_train_batch_end(self, loss):\n        if self._global_step < self._warmup_steps:\n            # linear warmup\n            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n            self._global_step += 1\n\n        elif not self._started:\n            self.save_variables()\n            print(\"backup trainable variables to CPU\") \n            self._started = True\n            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))\n            self._verbose_operation(\"Start Bayesian Optimization(BO)\")\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            self._lr = (float(data.split(\",\")[-1]))\n            for param_group in self._optimizer.param_groups:\n            \tparam_group['lr'] = self._lr\n        else:\n            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))\n            data = self._socket.recv(1024).decode(\"utf-8\")\n            self._verbose_operation(\"Received data: \" + data)\n            if data.startswith(\"restore\"):\n                self.restore_variables()\n                self._verbose_operation(\"restore trainable variables\")\n            elif data.startswith(\"ckpt\"):\n                self.save_variables()\n                self._verbose_operation(\"backup trainable variables\")\n            elif data.startswith('evaluate'):\n                val_loss = self._val_fn()\n                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))\n                data = self._socket.recv(1024).decode(\"utf-8\")\n            elif data.startswith('save'):\n                pass\n            else:\n                self._lr = (float(data.split(\",\")[-1]))\n                for param_group in self._optimizer.param_groups:\n                    param_group['lr'] = self._lr\n                self._global_step += 1",
    "Experiment Result": "### AutoLRS Server-side (Controller) Settings:\n- **Initial Stage Parameters:**\n  - `EXPLOITATION_STEP = 1000` (Initial number of steps for actual training with a selected LR, equivalent to `τ`)\n  - `LR_STEPS = 100` (Initial number of steps for evaluating a candidate LR during BO, equivalent to `τ'`)\n  - `RING_BUFFER_LEN = 100` (Length of the ring buffer for storing loss series for exponential forecasting)\n  - `LR_TO_EXPLORE = 10` (Number of LR candidates Bayesian Optimization explores before committing to the best one for an exploitation stage)\n  - `TAU_MAX = 8000` (Maximum value for `τ`, the exploitation step count)\n  - `val_freq = int(self.lr_steps/16)` (Frequency at which validation loss is sampled when in `val_stage`)\n\n- **Bayesian Optimization (BO) Configuration:**\n  - **Search Space:** Learning Rate (`LR`) is searched within a `Real` space, `log-uniform` distribution between `min_lr` and `max_lr` (provided as command-line arguments).\n  - **Surrogate Model:** Gaussian Process (`\"GP\"`).\n  - **Acquisition Function:** Lower Confidence Bound (`'LCB'`) with `kappa=1e6`.\n  - **Initial Points:** `n_initial_points=1`.\n\n- **Progressive Training Stages:**\n  - `self.lr_steps` (τ') and `self.exploitation_step` (τ) are gradually doubled after each exploitation stage, as long as `self.lr_steps < self.tau_max / 10`.\n\n- **Exponential Forecasting Model:**\n  - **Model Form:** `L(t) = aexp(bt) + c` (where `b<0`).\n  - **Noise/Outlier Elimination (`spline_iter` function):**\n    - Uses `UnivariateSpline` with `spline_deg=2`.\n    - For training loss series: `num_of_iter=10`, `filter_ratio=0.03`.\n    - For validation loss series: `num_of_iter=1`, `outliers` limited to the largest one (`[-1:]`).\n    - `bound=0.5` is used for outlier detection.\n\n### AutoLRS Client-side (Callback) Settings:\n- **Initialization Defaults:**\n  - `warmup_steps=0`\n  - `warmup_lr=0`\n  - `summary_steps=1`\n- **Model State Management:** Implements `save_variables()` and `restore_variables()` to checkpoint and restore model parameters and optimizer states for fair LR comparisons.\n\n### CIFAR-10 Example Specific Settings (`cifar10_example.py`):\n- **Dataset:** CIFAR-10.\n- **Model:** `VGG('VGG16')` (other models like ResNet18, MobileNetV2 are commented out but shown as options).\n- **Optimizer:** `optim.SGD` with `momentum=0.9`, `weight_decay=5e-4` (initial `lr=0.1` is specified but overridden by AutoLRS).\n- **Loss Function:** `nn.CrossEntropyLoss()`.\n- **Training Epochs:** Total `350 * 2 = 700` epochs, accounting for AutoLRS search steps.\n- **Batch Sizes:**\n  - `trainloader`: `batch_size=128`.\n  - `testloader`: `batch_size=100`.\n  - `valloader`: `batch_size=128`.\n- **Validation Subset:** `VAL_LEN = 10` (a subset of 10 mini-batches from the test set is used for validation loss calculation during AutoLRS evaluation).\n- **Data Preprocessing:**\n  - **Train Transform:** `RandomCrop(32, padding=4)`, `RandomHorizontalFlip()`, `ToTensor()`, `Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))`.\n  - **Test Transform:** `ToTensor()`, `Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))`."
}{
    "Title": "On Layer Normalization in the Transformer Architecture",
    "Main Contributions": "The paper theoretically and empirically investigates why the learning rate warm-up stage is crucial for training the original Post-Layer Normalization (Post-LN) Transformer architecture. It proves, using mean field theory, that Post-LN Transformers exhibit large expected gradients near the output layer at initialization, leading to training instability without warm-up. Conversely, it demonstrates that Pre-Layer Normalization (Pre-LN) Transformers have well-behaved gradients at initialization, allowing the learning rate warm-up stage to be safely removed. The research further shows that Pre-LN Transformers without warm-up can achieve comparable performance to baselines while significantly reducing training time and hyper-parameter tuning across various applications.",
    "Methodology": "The core methodology involves theoretical analysis using mean field theory to study gradient scales at initialization for both Post-LN and Pre-LN Transformer variants. It analytically derives the expected norms of hidden states and gradients in different layers under specific initialization conditions (Xavier initialization, zero-initialized attention matrices, Gaussian input vectors). This theoretical work is supported by extensive empirical verification of gradient norms and hidden state concentration properties across different model depths and layers. The paper compares the optimization behavior of Post-LN (layer normalization between residual blocks) and Pre-LN (layer normalization inside residual connections and before non-linear transformations, with a final layer normalization) Transformer architectures.",
    "Experimental Setup": "Experiments were conducted on machine translation (IWSLT14 German-to-English, WMT14 English-to-German) and unsupervised pre-training (BERT on English Wikipedia and BookCorpus), with fine-tuning on downstream GLUE tasks (MRPC, RTE). Models included 6-layer encoder/decoder Transformers for translation and a 12-layer BERT base model. Adam and vanilla SGD optimizers were used, with a focus on learning rate schedules. Post-LN models utilized a warm-up stage (e.g., 4000 iterations for translation, 10k steps for BERT) followed by inverse square root decay. Pre-LN models were trained without warm-up, using fixed initial learning rates (e.g., 5e-4, 7e-4, 1.5e-3 for translation; 3e-4 for BERT) and linear or inverse square root decay. Performance was evaluated using validation loss, BLEU score, and accuracy. Experiments were run on NVIDIA Tesla P40 GPUs, using datasets like IWSLT14 (153K sentence pairs), WMT14 (4.5M pairs), and a combined 3.4B-word corpus for BERT.",
    "Limitations": "The theoretical analysis relies on several simplifications, such as focusing on single-head attention, specific initialization of attention parameter matrices to zero (leading to uniform attention), and assuming Gaussian input vectors, which may not fully reflect real-world Transformer complexities. The theory primarily characterizes gradient behavior at initialization, and its applicability throughout the entire training process is not explicitly detailed. Additionally, while using a small, fixed learning rate can mitigate divergence issues for Post-LN Transformers without warm-up, it leads to significantly slower convergence and potentially suboptimal final performance, highlighting an inherent limitation of the Post-LN architecture's optimization properties.",
    "Future Research Directions": "Future research will explore other strategies for positioning layer normalization within the Transformer architecture. There is also a stated goal to deepen the theoretical understanding of Transformer optimization from a broader perspective, beyond the initial gradient analysis presented in this paper.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
    "Main Contributions": "The paper demonstrates that the reliance on the cosine learning rate schedule needlessly complicates large language model (LLM) training and scaling research, as it requires retraining models from scratch for different training durations. It introduces and validates an alternative training recipe: a constant learning rate followed by a cooldown phase. This alternative is shown to reliably match or even outperform the performance of a well-tuned cosine schedule, while offering significant advantages such as enabling continual learning, flexible data mixtures, and drastically reducing compute and GPU hours for scaling law experiments. Additionally, the paper shows that stochastic weight averaging (SWA) consistently improves model performance throughout training without extra computational overhead, providing strong intermediate models.",
    "Methodology": "The core methodology involves comparing the standard cosine learning rate schedule with a constant learning rate followed by a cooldown phase (referred to as constant LR + cooldown). Various functional forms for the cooldown phase were investigated, with a novel (1-sqrt) decay function identified as superior to linear decay. The optimal length of the cooldown phase was also studied, suggesting that short cooldowns (e.g., <20% of total steps, or even 5% for long runs) are sufficient. The paper also analyzed the optimization landscape during cooldown by interpolating weights between checkpoints, revealing a smooth transition to a connected minimum. Additionally, Stochastic Weight Averaging (SWA) was employed to average model weights over fixed windows to improve generalization, and a Schedule-Free Optimizer (SFO) was evaluated as a potential replacement for explicit learning rate decay.",
    "Experimental Setup": "Experiments were conducted using a decoder-only transformer architecture, similar to LLaMa, with the AdamW optimizer. Models ranged from 33M to 8B parameters, with primary experiments on 210M models and extended evaluations on 1B and 8B models. Training was performed on subsets of SlimPajama (6B tokens) and FineWeb (100B and 460B tokens), with additional verification on OpenWebText2. Evaluation metrics included validation loss (perplexity) and, for larger models, common LLM benchmarks such as MMLU, ARC, HellaSwag, and PIQA, using the `lighteval` library. The computational cost (FLOPs and GPU hours) was rigorously tracked, and a detailed FLOPs calculation method, more precise than the common 6ND heuristic, was provided. Training was performed on A100 and H100/GH200 GPUs, utilizing `nanotron` for larger-scale runs.",
    "Limitations": "The experiments were conducted on models up to 8B parameters, and while trends were consistent, the authors acknowledge that training behavior can be more brittle at modern, extremely large scales. The Schedule-Free Optimizer, though promising, was found to be sensitive to the choice of momentum parameters, which may depend on the training length, thus not being fully 'schedule-free'. The common heuristic of decaying cosine to 10% of the maximum LR was shown to be suboptimal, as annealing to lower values improved loss but could hurt downstream metrics due to early saturation, suggesting a need for independent tuning of maximum and final learning rates. Not all downstream benchmarks benefited equally from the cooldown phase, indicating potential areas for further investigation.",
    "Future Research Directions": "Future work could further investigate the impact of the functional form of the cooldown on model performance. Understanding the curriculum aspect of data mixtures during the separate constant and cooldown phases is an open area for research. It remains an interesting question whether a single cooldown schedule is absolutely optimal given a total compute budget for LLM training. Further exploration is also needed to understand why not all downstream benchmarks uniformly benefit from the cooldown. The findings enable more frequent updates of scaling laws, especially given emerging data dependencies, making scaling research more accessible for researchers and practitioners alike.",
    "Experiment Code": "import math\nimport numpy as np\nimport torch\nfrom copy import deepcopy\nfrom pathlib import Path\nimport tempfile\nimport wandb\n\ndef cos_inf_schedule(n_iterations, n_warmup, div_factor, final_div_factor, n_inf):\n    \"\"\"Cosine annealing with warmup and _constant_ final_lr after cycle ended.\n    Args:\n        n_iterations: total number of iterations\n        n_warmup: number of warmup iterations\n        div_factor: initial division factor for warmup\n        final_div_factor: final division factor for final lr\n        n_inf: number of iterations for the final lr (constant lr after cycle ended)\n    Returns:\n        schedule: a function that takes the current iteration and\n        returns the multiplicative factor for the learning rate\n    \"\"\"\n    max_lr = 1.0\n    base_lr = max_lr / div_factor\n    final_lr = base_lr / final_div_factor\n\n    n_anneal_steps = n_iterations - n_inf\n\n    def schedule(step):\n        if step < n_warmup:\n            return (step / n_warmup) + (1 - step / n_warmup) / div_factor\n        elif step < n_anneal_steps:\n            t = (step - n_warmup) / (n_anneal_steps - n_warmup)\n            lr = final_lr + 0.5 * (max_lr - final_lr) * (1 + np.cos(np.pi * t))\n            return lr\n        else:\n            return final_lr\n\n    return schedule\n\ndef wsd_schedule(\n    n_iterations,\n    final_lr_factor=0.0,\n    n_warmup=1000,\n    init_div_factor=100,\n    fract_decay=0.1,\n    decay_type=\"linear\",\n):\n    \"\"\"Warmup, hold, and decay schedule.\n    Args:\n        n_iterations: total number of iterations\n        final_lr_factor: factor by which to reduce max_lr at the end\n        warmup_fract: fraction of iterations used for warmup\n        init_div_factor: initial division factor for warmup\n        fract_decay: fraction of iterations used for decay\n    Returns:\n        schedule: a function that takes the current iteration and\n        returns the multiplicative factor for the learning rate\n    \"\"\"\n    n_anneal_steps = int(fract_decay * n_iterations)\n    n_hold = n_iterations - n_anneal_steps\n\n    def schedule(step):\n        if step < n_warmup:\n            return (step / n_warmup) + (1 - step / n_warmup) / init_div_factor\n        elif step < n_hold:\n            return 1.0\n        elif step < n_iterations:\n            if decay_type == \"linear\":\n                return final_lr_factor + (1 - final_lr_factor) * (\n                    1 - (step - n_hold) / n_anneal_steps\n                )\n            elif decay_type == \"exp\":\n                return final_lr_factor ** ((step - n_hold) / n_anneal_steps)\n            elif decay_type == \"cosine\":\n                return (\n                    final_lr_factor\n                    + (1 - final_lr_factor)\n                    * (1 + math.cos(math.pi * (step - n_hold) / n_anneal_steps))\n                    * 0.5\n                )\n            elif decay_type == \"miror_cosine\":\n                cosine_value = (\n                    final_lr_factor\n                    + (1 - final_lr_factor)\n                    * (1 + math.cos(math.pi * (step - n_hold) / n_anneal_steps))\n                    * 0.5\n                )\n                linear_value = final_lr_factor + (1 - final_lr_factor) * (\n                    1 - (step - n_hold) / n_anneal_steps\n                )\n                return linear_value * 2 - cosine_value\n            elif decay_type == \"square\":\n                return final_lr_factor + (1 - final_lr_factor) * (\n                    1 - ((step - n_hold) / n_anneal_steps) ** 2\n                )\n\n            elif decay_type == \"sqrt\":\n                return final_lr_factor + (1 - final_lr_factor) * (\n                    1 - math.sqrt((step - n_hold) / n_anneal_steps)\n                )\n\n            else:\n                raise ValueError(\n                    f\"decay type {decay_type} is not in ['cosine','miror_cosine','linear','exp']\"\n                )\n\n        else:\n            return final_lr_factor\n\n    return schedule\n\nclass WeightAverager:\n    def __init__(\n        self,\n        model,\n        horizon=100,\n        interval=1,\n        save_dir=None,\n        device=None,\n        dtype=torch.float32,\n        count=0,\n    ):\n        super().__init__()\n        self.device = device  # Where to keep avg model\n        self.dtype = dtype  # Precision for accumulation (>= float32)\n        if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n            model = model.module\n        self.module = deepcopy(model).to(dtype=self.dtype, device=device)\n\n        assert horizon % interval == 0, \"Interval should divide period\"\n        self.interval = interval\n        self.horizon = horizon\n        self.period = horizon // interval\n        if save_dir is None:\n            # Keep in tempdir\n            self._tempdir = tempfile.TemporaryDirectory()\n            self.save_dir = Path(self._tempdir.name)\n        else:\n            self.save_dir = Path(save_dir)\n            self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.count = count\n        # check if there are any checkpoints saved in the directory and set\n        # num_saved to number of checkpoints with name <= count\n        self.num_saved = len(\n            [f for f in self.save_dir.iterdir() if f.is_file() and int(f.stem) <= count]\n        )\n\n    @torch.no_grad()\n    def step(self, model, is_master_rank=True):\n        # Update module with current state\n        if self.count % self.interval == 0:\n            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n                model = model.module\n            for key, avg in self.module.state_dict().items():\n                curr = model.state_dict()[key].to(device=self.device, dtype=avg.dtype)\n                rate = 1 / ((self.count % self.horizon) // self.interval + 1)\n                avg.copy_(torch.lerp(avg, curr, rate))\n\n        self.count += 1\n\n        if self.count % self.horizon == 0 and is_master_rank:\n            torch.save(\n                self.module.to().state_dict(),\n                self.save_dir / f\"{self.count}.pt\",\n            )\n            self.num_saved += 1\n\n    def get_latest_like(self, model):\n        # Return model for latest completed period\n        if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n            model = model.module\n        new_model = deepcopy(model)\n\n        # Assumes that we saved at a specific iteration, will fail otherwise\n        count = self.count - self.count % self.horizon\n        latest_path = self.save_dir / f\"{count}.pt\"\n        map_and_load_state_dict(new_model, torch.load(latest_path))\n\n        return new_model\n\n    def sweep_horizon_like(self, model, max_num=None):\n        if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n            model = model.module\n        new_model = deepcopy(model)\n        avg_state = deepcopy(self.module.state_dict())\n        if max_num is None:\n            max_num = self.num_saved\n        # Assumes all points exist\n        for n in range(min(self.num_saved, max_num)):\n            # Load state from the corresponding checkpoint\n            count = self.count - self.count % self.horizon - n * self.horizon\n            state = torch.load(self.save_dir / f\"{count}.pt\")\n\n            # Update average state\n            for key, avg in avg_state.items():\n                new = state[key].to(dtype=avg.dtype, device=avg.device)\n                rate = 1 / (n + 1)\n                avg.copy_(torch.lerp(avg, new, rate))\n\n            # Set new_model state and yield it\n            map_and_load_state_dict(new_model, avg_state)\n            yield ((n + 1) * self.horizon, new_model)\n\ndef eval_wa(\n    curr_iter,\n    model,\n    weight_averager,\n    val_reader,\n    type_ctx,\n    distributed_backend,\n    cfg,\n    full_eval=False,\n):\n    if not distributed_backend.is_master_process():\n        # Only evaluate and log on master rank\n        return\n\n    if weight_averager.num_saved == 0:\n        return\n    if not cfg.wa_sweep_horizon:\n        val_reader.set_step(0)\n        val_acc, val_loss, val_perplexity = eval(\n            weight_averager.get_latest_like(model).eval(),\n            val_reader,\n            cfg.device,\n            max_num_batches=(\n                val_reader.num_batches()\n                if curr_iter == cfg.iterations or full_eval\n                else cfg.eval_batches\n            ),\n            ctx=type_ctx,\n            cfg=cfg,\n        )\n\n        if cfg.wandb:\n            if curr_iter == cfg.iterations or full_eval:\n                logs = {\n                    \"iter\": curr_iter,\n                    \"final-val/loss_wa\": val_loss,\n                    \"final-val/perplexity_wa\": val_perplexity,\n                    \"final-val/acc_wa\": val_acc,\n                }\n            else:\n                logs = {\n                    \"iter\": curr_iter,\n                    \"val/loss_wa\": val_loss,\n                    \"val/perplexity_wa\": val_perplexity,\n                    \"val/acc_wa\": val_acc,\n                }\n            wandb.log(logs)\n        print(\n            f\">WA Eval: Iter={curr_iter} \"\n            f\"val_loss={val_loss:.3f} \"\n            f\"val_pp={val_perplexity:.3f} \"\n            f\"val_acc={val_acc:3f}\"\n        )\n    else:\n        losses = []\n        for horizon, avg_model in weight_averager.sweep_horizon_like(\n            model, cfg.max_num_wa_sweeps\n        ):\n            avg_model.eval()\n            val_reader.set_step(0)\n            _, val_loss, _ = eval(\n                avg_model,\n                val_reader,\n                cfg.device,\n                max_num_batches=(\n                    val_reader.num_batches()\n                    if curr_iter == cfg.iterations or full_eval\n                    else cfg.eval_batches\n                ),\n                ctx=type_ctx,\n                cfg=cfg,\n            )\n\n            losses.append((val_loss, horizon))\n        if len(losses) == 0:  # in case of none saved yet\n            return\n        best_loss, best_horizon = sorted(losses)[0]\n\n        print(f\"WA Eval: {[(h, f'{l:0.3e}') for (l,h) in losses]}\")\n\n        if cfg.wandb:\n            if curr_iter == cfg.iterations or full_eval:\n                logs = {\n                    \"iter\": curr_iter,\n                    \"final-val/loss_wa\": losses[0][0],\n                    \"final-val/perplexity_wa\": 2.71828 ** losses[0][0],\n                    \"final-val/best_loss_wa\": best_loss,\n                    \"final-val/best_perplexity_wa\": 2.71828**best_loss,\n                }\n            else:\n                logs = {\n                    \"iter\": curr_iter,\n                    \"val/loss_wa\": losses[0][0],\n                    \"val/perplexity_wa\": 2.71828 ** losses[0][0],\n                    \"val/best_loss_wa\": best_loss,\n                    \"val/best_perplexity_wa\": 2.71828**best_loss,\n                    \"wa_best_horizon\": best_horizon,\n                }\n            wandb.log(logs)\n\ndef train(\n    model,\n    opt,\n    datareaders,\n    scheduler,\n    exp_dir,\n    distributed_backend,\n    cfg,\n):\n    not_compiled_model = model\n    # ... other code ...\n    if cfg.weight_average:\n        weight_averager = WeightAverager(\n            not_compiled_model,\n            horizon=cfg.wa_horizon,\n            interval=cfg.wa_interval,\n            save_dir=None if cfg.wa_use_temp_dir else exp_dir / \"avgs\",\n            dtype={\n                \"float32\": torch.float32,\n                \"float64\": torch.float64,\n            }[cfg.wa_dtype],\n            count=curr_iter,\n        )\n    # ... other code ...\n    while curr_iter <= cfg.iterations:\n        # ... other code ...\n        if curr_iter > cfg.wa_interval and cfg.weight_average:\n            eval_wa(\n                curr_iter,\n                not_compiled_model,\n                weight_averager,\n                val_reader,\n                type_ctx,\n                distributed_backend,\n                cfg,\n                full_eval=(curr_iter in cfg.full_eval_at),\n            )\n        # ... other code ...\n        opt.step()\n        scheduler.step()\n        opt.zero_grad(set_to_none=True)\n        if cfg.weight_average:\n            weight_averager.step(not_compiled_model, distributed_backend.is_master_process())\n        # ... other code ...\n\n# --- Schedule-Free Optimizer (SFAdamW) integration from src/main.py ---\nimport schedulefree\n# ... other imports ...\n    if args.opt == \"adamw\":\n        opt = torch.optim.AdamW(\n            group_specs,\n            lr=args.lr,\n            betas=(args.beta1, args.beta2),\n            weight_decay=args.weight_decay,\n        )\n    elif args.opt == \"SFAdamW\":\n        opt = schedulefree.AdamWScheduleFree(\n            group_specs,\n            lr=args.lr,\n            betas=(args.beta1, args.beta2),\n            weight_decay=args.weight_decay,\n            warmup_steps=args.warmup_steps,\n        )",
    "Experiment Result": "Learning Rate Schedule Settings:\n- `scheduler`: default=\"cos\", choices=[\"linear\", \"cos\", \"wsd\", \"none\", \"cos_inf\"]\n- `cos-inf-steps`: default=0 (for \"cos_inf\" scheduler, number of iterations for the constant final LR)\n- `iterations`: default=15000 (total training steps)\n- `warmup-steps`: default=300 (number of warmup iterations)\n- `lr`: default=1e-3 (base learning rate)\n- `wsd-final-lr-scale`: default=0.0 (final LR scale factor for \"wsd\" scheduler cooldown)\n- `wsd-fract-decay`: default=0.1 (fraction of total iterations used for cooldown phase in \"wsd\" scheduler)\n- `decay-type`: default=\"linear\", choices=[\"linear\", \"cosine\", \"exp\", \"miror_cosine\", \"square\", \"sqrt\"] (decay function for \"wsd\" scheduler cooldown)\n\nStochastic Weight Averaging (SWA) Settings:\n- `weight-average`: action=\"store_true\" (flag to enable SWA)\n- `wa-interval`: default=5 (how often to take an average of model weights, every k steps)\n- `wa-horizon`: default=500 (how frequently uniform model averages are saved for evaluation)\n- `wa-dtype`: default=\"float32\", choices=[\"float32\", \"float64\"] (precision for weight accumulation in SWA)\n- `wa-use-temp-dir`: action=\"store_true\" (flag to use a temporary directory for saving WA checkpoints)\n- `wa-sweep-horizon`: action=\"store_true\" (flag to evaluate SWA over multiple horizons)\n- `max-num-wa-sweeps`: default=5 (maximum number of WA sweeps to perform if `wa-sweep-horizon` is enabled)\n\nSchedule-Free Optimizer (SFO) Settings:\n- `opt`: default=\"adamw\", choices=[\"adamw\", \"sgd\", \"SFAdamW\"]\n- `beta1`: default=0.9 (AdamW/SFAdamW hyperparameter)\n- `beta2`: default=0.95 (AdamW/SFAdamW hyperparameter)\n- `weight-decay`: default=1e-1 (AdamW/SFAdamW hyperparameter)"
}{
    "Title": "LEMON: Lossless model expansion",
    "Main Contributions": "The paper introduces LosslEss MOdel ExpansioN (LEMON), a novel approach to address the inefficiency of training large deep neural networks, particularly Transformers, from scratch. LEMON initializes scaled models using the weights of their smaller, pre-trained counterparts. Key contributions include proposing a suite of algorithms for lossless model expansion compatible with various architectures and indivisible width/depth increments, developing an optimized learning rate scheduler with accelerated decay rates for expanded models, and significantly reducing computational costs by up to 56.7% for Vision Transformers and 33.2% for BERT compared to training from scratch.",
    "Methodology": "LEMON operates through three main components: (1) General lossless width expansion with symmetry breaking, which involves replicating neurons in fully-connected layers and entire heads in Multi-Head Attention (MHA), but crucially setting unequal fan-out weights for replicated neurons/heads to prevent symmetry issues during training. (2) Average width expansion for LayerNorm, designed to handle indivisible width increments by padding the output of preceding FC-layers with their average, ensuring lossless expansion for the LayerNorm layer itself. (3) Lossless depth expansion, achieved by either setting the output of added modules (MLP or MHA) to zero (so the residual branch contributes nothing) or by enforcing zero output by setting fan-out weights of replicated neurons to sum to zero. The methodology also includes an optimized training recipe for expanded models, which maintains the default maximum learning rate but employs an accelerated learning rate scheduler with faster decay rates.",
    "Experimental Setup": "The research primarily evaluates LEMON on Vision Transformers (ViT) and BERT models. For ViT, experiments involve expanding ViT(6, 512) to ViT(12, 768) and ViT(6, 384) to ViT(12, 768), using the ImageNet-1k dataset. Training from scratch for ViT employs a maximum learning rate of 1e-3 for 300 epochs with a batch size of 1024 and a cosine learning rate scheduler. For BERT, Pre-LN BERT is trained on the masked language modeling task using the English Wiki corpus, expanding BERT(6, 384) to BERT(12, 768) and BERT(6, 512) to BERT(12, 768). BERT pre-training involves 220k iterations, 5k warmup steps, batch size 256, max LR of 2e-4, and a cosine scheduler. Fine-tuning for BERT is performed on the GLUE dataset. Baselines include training from scratch, bert2BERT-FPI, bert2BERT-AKI, soft KI, and hard KI. All experiments were conducted using NVIDIA-V100 and NVIDIA-A100 GPUs.",
    "Limitations": "The current limitations of LEMON include the necessity for tuning the total number of training epochs, which can be a manual and resource-intensive process. Additionally, the evaluation scale of the method was constrained by the available computational resources, potentially limiting the scope of its demonstrated applicability to extremely large models.",
    "Future Research Directions": "Future research will focus on extending the application of LEMON to even larger models, pushing the boundaries of its current evaluation scale. Another key direction involves developing more sophisticated methodologies for automatically selecting optimal free parameters during the initialization of LEMON to further enhance its efficiency and ease of use. The authors also suggest exploring synergies with neural network pruning, such as iterative adjustments (increases and decreases) in hidden dimensions or layers during training to potentially accelerate the overall training process.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Small-scale proxies for large-scale Transformer training instabilities",
    "Main Contributions": "The paper reproduces and studies Transformer training instabilities at smaller scales, demonstrating that issues like attention logit growth and output logit divergence, previously observed in large models, occur in small models at high learning rates. It validates that mitigations effective at large scales (qk-layernorm, z-loss) are equally effective in this small-scale regime. The research introduces \"learning rate (LR) sensitivity\" as a novel summary statistic for characterizing stability from LR vs. loss curves. It investigates how various interventions (warm-up, independent weight decay, µParam) influence LR sensitivity. Furthermore, the work shows that instabilities can be predicted before they emerge by analyzing the scaling behavior of model activation and gradient norms, identifying that the default AdamW epsilon can lead to issues at scale due to excessively small updates and proposing its reduction.",
    "Methodology": "The methodology involves training small decoder-only Transformer models, similar to GPT-2, and systematically measuring the relationship between learning rate and final validation loss across different model scales. A key component is the introduction and application of \"LR sensitivity,\" a metric quantifying deviation from optimal loss across a three-order-of-magnitude LR sweep. The study reproduces two known instabilities (attention logit growth and output logit divergence) by training small models at high learning rates and evaluates the effectiveness of proposed mitigations. It also analyzes the impact of warm-up length, independent weight decay, µParam, and scaling width versus depth on LR sensitivity. To predict instabilities, the research tracks scaling trends of model characteristics such as activation and gradient norms (e.g., gradient RMS), and validates predictions through additional experiments at larger scales. Additionally, it explores the thresholds for instability by transplanting specific attention logit values into small networks.",
    "Experimental Setup": "Experiments were conducted on small decoder-only Transformer models, ranging from 2.4 million to 4.8 billion parameters, implemented in Flax and JAX, and trained on TPUs using bfloat16 precision with an auto-regressive loss. The C4 dataset was used for training, with sequences of 512 tokens packed to avoid padding. The default optimizer was AdamW with β1=0.9, β2=0.95, and ϵ=1e-8 (with ϵ also explored at 1e-15 and 1e-6), and global norm 1 gradient clipping. The learning rate schedule included a linear warm-up (default 5e3 steps) followed by a cosine-decay to a minimum of 1e-5 over 1e5 total steps. Default regularization consisted of an independent weight decay of 1e-4 and an auxiliary z-loss with coefficient 1e-4. Model scaling involved jointly increasing embedding size, depth, and number of heads. Rotary positional embeddings were used, and the feed-forward components employed GELU activations. The default batch size was 256, with larger sizes (512, 1024) also tested. Metrics included final evaluation loss, LR sensitivity, max attention logit, output logit mean, gradient RMS, and unscaled update RMS.",
    "Limitations": "The study primarily focuses on instabilities that lead to slow divergence, explicitly excluding the investigation of fast loss spikes. The proposed \"LR sensitivity\" metric is limited as it is not meaningful when interventions fundamentally alter the interpretation of the learning rate (e.g., scaling LR by parameter RMS), and its invariance to the absolute scale of the loss necessitates its examination in conjunction with LR vs. loss curves. While acknowledging that optimal LR shifts may require adjusting the sensitivity calculation range, this was not necessary for the main experiments in this paper due to the scales tested. The µParam method, while stabilizing the optimal LR, did not improve overall loss or reduce LR sensitivity in the experiments. The paper also noted that increasing the AdamW epsilon to 1e-6 resulted in loss divergence.",
    "Future Research Directions": "The research highlights the significant opportunity for future work to study training stability using small Transformer models, thereby benefiting large-scale model development without requiring vast computational resources. Specific potential areas include further mechanistic investigation into the connections between parameter norm growth and various forms of instabilities. Additionally, exploring the impact of other advanced model parameterizations beyond µParam, such as those discussed in related work, on learning rate versus loss characteristics is suggested. While not the primary focus, the paper's discussion on the 'edge of stability' and fast loss spikes implicitly points towards continued research to deepen the understanding of these complex Transformer training dynamics.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly",
    "Main Contributions": "Addresses the challenge of automatic learning rate (LR) schedule tuning for deep neural networks (DNNs) without human involvement. Proposes AutoLRS, an efficient method that dynamically optimizes the LR for each training stage by modeling training dynamics. Introduces a novel technique using Bayesian optimization (BO) for black-box LR search, significantly reducing its cost by predicting validation loss after \"τ\" steps from short \"τ'\" (≪ \"τ\") runs using an exponential time-series forecasting model. Achieves notable speedups (1.22x for ResNet-50, 1.43x for Transformer, 1.5x for BERT) compared to original hand-tuned LR schedules, and an average speedup of 1.31x over heavily-tuned state-of-the-art schedules (CLR, SGDR), while maintaining or improving test-set performance. Demonstrates generality and compatibility across diverse DNNs, tasks, and optimizers.",
    "Methodology": "The method involves stage-wise LR optimization, decomposing the overall LR schedule optimization into sequentially solving sub-problems to determine a constant LR for a \"training stage\" of \"τ\" steps, aiming to minimize validation loss. It employs cost-efficient Bayesian Optimization (BO) with a Gaussian Process (GP) surrogate model (Matern kernel, \nu=2.5, l=1) and Lower Confidence Bound (LCB) acquisition function (\n\n\n\nκ=1000) to find optimal LRs. To mitigate BO's computational expense, candidate LRs are evaluated for only \"τ'\" (e.g., \"τ\"/10) steps. An exponential model L(t) = a*exp(b*t) + c is fitted to the validation loss observed during the short \"τ'\" runs to predict the validation loss after \"τ\" steps. BO and the loss forecasting model operate in a mutual training loop. Practical enhancements include gradually increasing \"τ\" and \"τ'\" during training, approximating validation loss with training loss in early stages (and using a small validation subset later), iterative spline smoothing for noise reduction in loss series, and performing LR search in log-scale space. Model parameters and optimizer states are checkpointed and restored for fair LR comparisons within a stage.",
    "Experimental Setup": "The method was evaluated on three widely-used DNNs: ResNet-50 for ImageNet classification, Transformer for WMT 2014 English-German neural machine translation, and BERT for pre-training on English Wikipedia & BooksCorpus, followed by fine-tuning on MRPC, MNLI, CoLA, and SQuAD v1.1 NLP tasks. Additional experiments were conducted with VGG-16 on CIFAR-10 and CIFAR-100. Optimizers used include SGD with momentum (ResNet-50) and Adam (Transformer, BERT). Baselines included original paper's hand-tuned LR schedules, heavily hand-tuned state-of-the-art schedules like Cyclical Learning Rate (CLR) and SGDR (Stochastic Gradient Descent with Warm Restarts), Hypergradient Descent (HD), MARTHE, and Hyperband. Evaluation metrics were Top-1 accuracy, BLEU score, and F1 score. AutoLRS configuration used default k=10, \n\nτ'=\n\nτ/10, initial \n\nτ=1000, \n\nτ'=100 increasing to \n\nτmax (8000/32000 depending on model), and \n\nκ=1000. LR search intervals were defined per model using an LR range test. Experiments were performed on NVIDIA Tesla V100 and Titan RTX GPUs. Ablation studies confirmed the necessity of both the exponential model and Bayesian optimization.",
    "Limitations": "AutoLRS does not incorporate LR search for warmup steps, which are typically used in early training phases to prevent instability. While significantly reducing tuning effort, AutoLRS still requires setting some hyperparameters such as the LR search interval (\n\nηmin, \n\nηmax) and \n\nτmax, although reasonable defaults or easy search methods exist for them. The exponential forecasting model, while generally effective, might not perfectly capture highly non-monotone loss changes in very early training steps, despite iterative spline smoothing. The BO search itself incurs a computational overhead, approximated to be roughly equal to the model update time for the identified LR schedule, leading to approximately 2x total training cost.",
    "Future Research Directions": "Future work could involve extending AutoLRS to automatically manage and optimize warmup phases, as the current method does not search LRs for warmup steps. Another direction is to explore the application of AutoLRS or its principles to automatically tune other crucial hyperparameters beyond the learning rate schedule. Investigating more sophisticated time-series forecasting models for loss prediction in scenarios where the simple exponential model might be insufficient, while balancing complexity with efficiency, is also a potential avenue. Further optimizing the computational overhead of the Bayesian optimization search to reduce the total training cost even more could be considered.",
    "Experiment Code": "import torchimport osimport loggingimport socketimport stringimport randomimport timeclass AutoLRS():    def __init__(self, model, optimizer, val_fn, listening_host='localhost', listening_port=12315, warmup_steps=0, warmup_lr=0, summary_steps=1):        self._net = model        self._optimizer = optimizer        self._val_fn = val_fn         self._lr = 0.000001        self._warmup_steps = warmup_steps        self._warmup_lr = warmup_lr         self._global_step = 0        self._socket = socket.socket()        self._started = False        self._summary_steps = summary_steps        self._checkpoint_path = './checkpoint/autolrs_ckpt_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 7))  + '.pth'        self._listening_host = listening_host        self._listening_port = listening_port        self._best_acc = 0        if not os.path.exists('checkpoint'):            os.makedirs('checkpoint')        self.connect_server()        def connect_server(self):        self._socket.connect((self._listening_host, self._listening_port))    def _verbose_operation(self, _op):        if self._global_step % self._summary_steps == 0:            logging.info(\"[AutoLRS at {}] {}\".format(self._global_step, _op))    def save_variables(self):        \"\"\"Save model parameters and optimizer states.\"\"\"        _start_time = time.time()        torch.save({            'model_state_dict': self._net.state_dict(),            'optimizer_state_dict': self._optimizer.state_dict()            }, self._checkpoint_path)        logging.info(\"[AutoLRS] backup variables, elapsed: {}s\".format(time.time() - _start_time))    def restore_variables(self):        _start_time = time.time()        checkpoint = torch.load(self._checkpoint_path)        self._net.load_state_dict(checkpoint['model_state_dict'])        self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])        logging.info(\"[AutoLRS] restoring variables, elapsed: {}s\".format(time.time() - _start_time))    def on_train_batch_end(self, loss):        if self._global_step < self._warmup_steps:            # linear warmup            self._lr = (self._warmup_lr / self._warmup_steps) * (self._global_step + 1)            for param_group in self._optimizer.param_groups:            \tparam_group['lr'] = self._lr            self._global_step += 1        elif not self._started:            self.save_variables()            print(\"backup trainable variables to CPU\")             self._started = True            self._socket.send(\",\".join(('startBO', str(loss))).encode(\"utf-8\"))            self._verbose_operation(\"Start Bayesian Optimization(BO)\")            data = self._socket.recv(1024).decode(\"utf-8\")            self._verbose_operation(\"Received data: \" + data)            self._lr = (float(data.split(\",\")[-1]))            for param_group in self._optimizer.param_groups:            \tparam_group['lr'] = self._lr        else:            self._socket.send(','.join(('loss', str(loss))).encode('utf-8'))            data = self._socket.recv(1024).decode(\"utf-8\")            self._verbose_operation(\"Received data: \" + data)            if data.startswith(\"restore\"):                self.restore_variables()                self._verbose_operation(\"restore trainable variables\")            elif data.startswith(\"ckpt\"):                self.save_variables()                self._verbose_operation(\"backup trainable variables\")            elif data.startswith('evaluate'):                val_loss = self._val_fn()                self._socket.send(\",\".join((\"val_loss\", str(val_loss))).encode(\"utf-8\"))                data = self._socket.recv(1024).decode(\"utf-8\")            elif data.startswith('save'):                pass            else:                self._lr = (float(data.split(\",\")[-1]))                for param_group in self._optimizer.param_groups:                    param_group['lr'] = self._lr                self._global_step += 1import argparseimport socket                import randomimport numpy as np import threadingimport timeimport mathimport osfrom skopt import Optimizerfrom skopt.space import Realfrom scipy.interpolate import UnivariateSplinefrom scipy import optimize import sysimport logginglogging.basicConfig(level=logging.INFO)def f(b, x, y):    A = np.vstack([np.exp(-np.exp(b) * x), np.ones(len(x))]).T    res = np.linalg.lstsq(A, y, rcond=None)[1]    return resdef spline_iter(xs, ys, is_training, spline_deg=2, filter_ratio=0.03, num_of_iter=10, bound=0.5):    \"\"\" Use iterative spline to eliminate noise and outliers in the loss series.        is_training specifies whether the loss series in use is training loss or validation loss.    \"\"\"    bound = xs[int((len(xs) - 1) * bound)]    if is_training:        num_of_iter = 10    else:        num_of_iter = 1    for _ in range(num_of_iter):        spline_ys = UnivariateSpline(xs, ys, k=spline_deg)(xs)        dys = np.abs(ys - spline_ys)        if is_training:            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[int(round(-len(dys) * filter_ratio)):])        else:            outliers = set(sorted(range(len(dys)), key=lambda i: dys[i])[-1:])        outliers = [i for i in outliers if i < bound]        xs2 = np.zeros(len(xs) - len(outliers))        ys2 = np.zeros(len(xs) - len(outliers))        i1 = 0        for i2 in range(len(xs)):            if i2 not in outliers:                xs2[i1], ys2[i1] = xs[i2], ys[i2]                i1 += 1        xs, ys = xs2, ys2    return xs, ysdef exp_forecast(loss_series, end_step, is_training, spline_order=2):    \"\"\" Do exponential forecasting on a loss series.\"\"\"    xs = np.arange(end_step - len(loss_series), end_step)    xs2, ys2 = spline_iter(xs, loss_series, is_training)    ys = UnivariateSpline(xs2, ys2, k=spline_order)(xs)    logging.debug('ys after spline iter: {}'.format(ys))    b = optimize.fmin(f, 0, args=(xs, ys), xtol=1e-5, ftol=1e-5, disp=False)[0]    b = -np.exp(b)    A = np.vstack([np.exp(b * xs), np.ones(len(xs))]).T    a, c = np.linalg.lstsq(A, ys, rcond=None)[0]    return a, b, cclass RingBuffer:    \"\"\" A class for storing and manipulating loss series and do exponential forecasting. \"\"\"    def __init__(self, size):        self.data = [None for i in range(size)]    def reset(self):        self.data = [None for i in self.data]    def append(self, x):        self.data.pop(0)        self.data.append(x)    def get(self):        return self.data    def average(self):        return sum(self.data)/len(self.data)    def exponential_forcast(self, pred_index, is_training):        loss_series = self.data[:]        end_epoch = len(loss_series)        x = np.arange(end_epoch - len(loss_series), end_epoch)        y = np.array(loss_series)        a3, b3, c3 = exp_forecast(y, len(y), is_training)        forcast_y = a3 * np.exp(b3 * pred_index) + c3        logging.debug(\"Exponential fit: {}, {}, {}\".format(a3, b3, c3))        return forcast_yclass Controller(object):    def __init__(self, host, port, min_lr, max_lr):        # Constants        EXPLOITATION_STEP = 1000        LR_STEPS = 100        RING_BUFFER_LEN = 100        LR_TO_EXPLORE = 10        TAU_MAX = 8000        self.min_lr = float(min_lr)        self.max_lr = float(max_lr)        self.host = host        self.port = port        self.threads = []        self.num_threads = 1        self.event = threading.Event()        self.sock = socket.socket()        self.sock.bind((self.host, self.port))        self.global_step = 0        self.last_total_loss = 0.0        self.average_loss = 0.0        self.loss_vector = []        self.lr = 0        self.lr_steps = LR_STEPS         self.tau_max = TAU_MAX        self.val_freq = int(self.lr_steps/16)        self.lr_counter = 0        self.lr_to_explore = LR_TO_EXPLORE        self.BO_stage = True        self.val_stage = False        self.message = ''        self.loss_after_exploitation = None        self.ring_buffer_len = RING_BUFFER_LEN        if self.val_stage:            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)        else:            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)        self.exploitation_step = EXPLOITATION_STEP        self.exploitation_flag = False         self.exploitation_counter = 0                self.opt = None        self.x_func_dict = dict()        self.x_iters = []        self.func_val_iters = []        self.num_ranks = 0        self.finished_minions = 0        self.lock1 = threading.Lock()        self.lock2 = threading.Lock()    def listen(self):        self.sock.listen(20)        while True:            client, address = self.sock.accept()            logging.info('[Server]: Got connection from {}'.format(address))            self.threads.append(threading.Thread(target = self.run, args = (client, address, self.event)))            if len(self.threads) == self.num_threads:                self.num_ranks = len(self.threads)                self.num_minions = self.num_ranks - 1                logging.info('[Server]: num_ranks: {}'.format(self.num_ranks))                for thread in self.threads:                    thread.start()                logging.info('[Server]: threads started')                for thread in self.threads:                    thread.join()                    self.threads = []                logging.info('[Server]: join finished')                sys.exit()    def run(self, c, address, event):        size = 1024        while True:            data = c.recv(size).decode()            if not data:                sys.exit()            logging.debug(data.split(','))            total_loss = float(data.split(',')[-1])            self.lock2.acquire()            self.loss_vector.append(total_loss)            # compute average loss across ranks            if len(self.loss_vector) == self.num_ranks:                self.average_loss = sum(self.loss_vector) / len(self.loss_vector)                 logging.info('[Server]: average loss = {}, step = {}'.format(self.average_loss, self.global_step))                if self.val_stage:                    if 'val' in data:                        self.ring_loss_buffer.append(self.average_loss)                    else:                        self.global_step += 1                else:                    self.ring_loss_buffer.append(self.average_loss)                    self.global_step += 1                self.loss_vector = []            self.lock2.release()            if 'val' in data:                c.send(str(self.lr).encode('utf-8'))                 continue            if 'minion' in data:                # blocking                event_is_set = event.wait()                c.send(self.message.encode('utf-8'))                 logging.debug('[Server] message: %s', self.message)                self.lock1.acquire()                self.finished_minions += 1                self.lock1.release()                if self.finished_minions == self.num_minions:                    event.clear()                    self.finished_minions = 0                continue            else:                if data.startswith('startBO'):                    self.last_total_loss = self.average_loss                    self.init_loss = self.average_loss                    self.loss_after_exploitation = self.average_loss                # exploitation stage -- actual training stage using the best-found LR                if self.exploitation_flag:                    logging.debug('[Server exploitation]: average loss ' + str(self.average_loss) + ' step=' + str(self.global_step))                    if self.exploitation_counter == self.exploitation_step:                        self.BO_stage = True                        self.exploitation_flag = False                        self.exploitation_counter = 0                        logging.info('[Server]: exploitation stage done')                        logging.info('[Server]: reconfigure...')                        if self.lr_steps < self.tau_max / 10:                            self.lr_steps = self.lr_steps * 2                            self.val_freq = int(self.lr_steps/16)                            self.ring_buffer_len = self.lr_steps                             self.exploitation_step = self.exploitation_step * 2                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)                        else:                            self.val_stage = True                        if self.val_stage:                            self.ring_loss_buffer = RingBuffer(self.ring_buffer_len // self.val_freq)                        self.loss_after_exploitation = self.average_loss                        self.message = 'save'                        c.send(self.message.encode('utf-8'))                        event.set()                        continue                    else:                        self.exploitation_counter += 1                        self.message = str(self.lr)                        c.send(str(self.lr).encode('utf-8'))                         event.set()                        continue                # BO stage -- LR search stage                if self.BO_stage:                    self.opt = Optimizer([Real(self.min_lr, self.max_lr, 'log-uniform')], \"GP\", n_initial_points=1, acq_func='LCB', acq_func_kwargs={'kappa':1e6})                    self.BO_stage = False                    self.lr = self.opt.ask()[0]                    # prevent BO in scikit-optimize from searching for the same LR explored before                    while True:                        if self.lr in self.x_func_dict:                            self.opt.tell([self.lr], self.x_func_dict[self.lr])                            self.lr = self.opt.ask()[0]                        else:                            break                    self.message = str(','.join(('ckpt', str(self.lr))))                    c.send(','.join(('ckpt', str(self.lr))).encode('utf-8'))                    event.set()                    logging.debug('[Server]: checkpoint command sent')                    continue                # ask BO to suggest the next LR                 if self.lr_counter == self.lr_steps:                    logging.debug('ring_buffer: {}'.format(self.ring_loss_buffer.get()))                    if any([math.isnan(x) for x in self.ring_loss_buffer.get()]):                        predicted_loss = \"nan\"                    elif self.val_stage:                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=int(self.exploitation_step/self.val_freq), is_training=False)                        current_loss = sum(self.ring_loss_buffer.get()[-1:])/1.0                    else:                        predicted_loss = self.ring_loss_buffer.exponential_forcast(pred_index=self.exploitation_step, is_training=True)                        current_loss = sum(self.ring_loss_buffer.get()[-10:])/10.0                    logging.info('[Server]: predicted loss: {} due to LR {}'.format(predicted_loss, self.lr))                    # Huge loss jump can make the exponential prediction inaccurate, so set a threshold here.                     #if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) > 10 * self.loss_after_exploitation:                    #    predicted_loss = current_loss                     #    logging.info('New predicted_loss: ' + str(predicted_loss))                    if self.loss_after_exploitation is not None and max(self.ring_loss_buffer.get()) >= 1.0 * self.init_loss and self.val_stage:                        predicted_loss = current_loss                         logging.info('[Server]: New predicted_loss: ' + str(predicted_loss))                    if self.val_stage:                        self.ring_loss_buffer = RingBuffer(int(math.floor(self.ring_buffer_len)/self.val_freq))                    else:                        self.ring_loss_buffer = RingBuffer(self.ring_buffer_len)                    # feed a (LR, predicted loss in tau steps) instance to BO.                    if str(predicted_loss) == 'nan':                        self.opt.tell([float(self.lr)], 1e6)                    else:                        self.opt.tell([float(self.lr)], predicted_loss)                    self.x_iters.append(float(self.lr))                    self.func_val_iters.append(predicted_loss)                    self.x_func_dict[self.lr] = predicted_loss                    self.lr_counter = 1                    if len(self.func_val_iters) == self.lr_to_explore:                        min_index = self.func_val_iters.index(min(self.func_val_iters))                        # log the best lr found for the next stage.                        logging.info('[Server]: best LR: {}, min loss: {}'.format(self.x_iters[min_index], self.func_val_iters[min_index]))                        self.lr = self.x_iters[min_index]                        self.message = str(','.join(('restore', str(self.lr))))                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))                        event.set()                        logging.debug('[Server]: restore command sent')                        self.exploitation_flag = True                        self.func_val_iters = []                        self.x_iters = []                        self.x_func_dict = dict()                    else:                        # Ask BO for the next LR to explore                        self.lr = self.opt.ask()[0]                        while True:                            if self.lr in self.x_func_dict:                                self.opt.tell([self.lr], self.x_func_dict[self.lr])                                self.lr = self.opt.ask()[0]                            else:                                break                        self.message = str(','.join(('restore', str(self.lr))))                        c.send(','.join(('restore', str(self.lr))).encode('utf-8'))                        event.set()                        logging.debug('[Server]: restore command sent')                else:                    self.lr_counter += 1                    if self.val_stage and self.lr_counter % self.val_freq == 0:                        self.message = \"evaluate\"                        c.send(self.message.encode('utf-8'))                     else:                        self.message = str(self.lr)                        c.send(str(self.lr).encode('utf-8'))                     event.set()",
    "Experiment Result": "LR Optimization Strategy: Stage-wise optimization, decomposing the overall LR schedule optimization into sequentially solving sub-problems. Each sub-problem determines a constant LR for a 'training stage' (τ steps) to minimize validation loss.Bayesian Optimization (BO):Cost-efficient BO is used.Surrogate Model: Gaussian Process (GP). The method description specifies a Matern kernel (u=2.5, l=1).Acquisition Function: Lower Confidence Bound (LCB) with kappa=1e6 (1,000,000), as implemented in the code (Note: the method description mentions κ=1000).Search Space: Log-uniform scale for Learning Rates (min_lr to max_lr, passed as command-line arguments to the server).Loss Forecasting Model: An exponential model L(t) = a*exp(b*t) + c is fitted to the validation loss (or training loss in early stages) observed during short runs (τ' steps) to predict the validation loss after τ steps.Stage Lengths (τ, τ'):Initial short run steps (τ'): LR_STEPS = 100.Initial full training stage steps (τ): EXPLOITATION_STEP = 1000.Maximum full training stage steps (τ_max): TAU_MAX = 8000.Both τ and τ' are gradually increased by doubling them after each exploitation stage, until τ reaches TAU_MAX.Loss Series Processing:Loss values are stored in a RingBuffer.Iterative spline smoothing is applied to the loss series to reduce noise: spline_deg=2, filter_ratio=0.03, num_of_iter=10 for training loss, num_of_iter=1 for validation loss, with a bound=0.5.Validation Strategy:In early stages, training loss is used for forecasting (controlled by self.val_stage flag).In later stages, a small validation subset is used. The cifar10_example.py shows this subset consists of VAL_LEN = 10 mini-batches from the test set. Validation loss is calculated every val_freq = LR_STEPS/16 steps during short runs.State Management: Model parameters and optimizer states are checkpointed and restored (save_variables, restore_variables) before and after each LR exploration phase within a stage to ensure fair comparisons.Mutual Training Loop: BO and the loss forecasting model operate in a mutual training loop, with the client (trainer) and server (controller) communicating via sockets.Application Specifics (from cifar10_example.py):Dataset: CIFAR10.Model: VGG16 (selected from a list of available models like ResNet18, MobileNetV2 etc. for CIFAR10).Optimizer: SGD with lr=0.1 (initial, overridden by AutoLRS), momentum=0.9, weight_decay=5e-4.Loss Function: nn.CrossEntropyLoss.Total Training Steps: Approximately 700 epochs (350 * 2) to accommodate the search steps."
}{
    "Title": "MoMo: Momentum Models for Adaptive Learning Rates",
    "Main Contributions": "Addresses the high computational cost of learning-rate tuning in modern machine learning. Introduces MoMo, a novel Polyak-type adaptive learning rate that integrates with any momentum-based method (e.g., SGD-M, Adam), enhancing robustness to hyperparameter choices. MoMo builds a truncated model of the loss function using momentum estimates of past losses and gradients, leveraging known or estimated lower bounds. It achieves an O(1/sqrt(K)) convergence rate for convex problems with interpolation. A key contribution is also an online method for estimating unknown lower bounds of the loss function, which improves performance in tasks where the optimal loss is non-zero. The method also intrinsically performs learning rate warm-up and decay.",
    "Methodology": "MoMo utilizes a model-based stochastic optimization approach, interpreting momentum as a model of the objective function built by exponentially averaging past sampled loss values and their linearized approximations. This model is truncated by a known or estimated lower bound (fk*)—typically zero, but an online estimation method (MoMo*) is provided for unknown, non-zero bounds. The method computes a proximal update, xk+1 = argmin y mk(y) + 1/(2αk) ||y - xk||^2, which yields a closed-form solution for the adaptive learning rate. This framework is extended to MoMo-Adam by incorporating weight decay and Adam's diagonal preconditioner. Exponentially weighted averages are used for coefficients to maintain computational efficiency similar to SGD-M.",
    "Experimental Setup": "Experiments compared MoMo(-Adam) against SGD-M (Adam/AdamW) on various tasks to assess robustness to learning rate selection. Benchmarks included image classification (ResNet110 on CIFAR100, ResNet20/VGG16/ViT on CIFAR10, MLP on MNIST, ResNet18 on Imagenet32/Imagenet-1k, ViT on Imagenet-1k), recommender systems (DLRM on Criteo), neural machine translation (Transformer on IWSLT14), and diffusion models (UNet on Smithsonian Butterflies). Validation involved learning-rate sweeps (constant or scheduled with warm-up/decay) and averaging results over multiple seeds. Performance was evaluated using training loss, validation accuracy, and BLEU scores, with default momentum parameters and varying weight decay. Hardware primarily included NVIDIA A100 GPUs.",
    "Limitations": "The theoretical convergence rate of O(1/sqrt(K)) is established for convex problems with interpolation, which may not fully represent the dynamics in non-convex deep learning. While momentum empirically improves performance, a theoretical justification for this improvement within the established convergence rate is not yet shown. The method relies on loss functions being lower-bounded, typically by zero, though an online estimation method is provided for unknown non-zero bounds. The online lower bound estimation initially depends on the unknown distance to the optimal solution, which is addressed by bootstrapping with a zero initial estimate. MoMo introduces a slight, but generally minimal, increase in per-iteration computational cost and memory footprint compared to baseline momentum methods.",
    "Future Research Directions": "Future work could focus on providing theoretical justifications for the empirical benefits of momentum within the MoMo framework. Extending MoMo to other preconditioned momentum methods beyond SGD-M and Adam (e.g., Adabelief, AdaBound) is a natural next step given the generalized framework. Further research could also explore improvements and broader applications for the proposed online lower bound estimation technique, which is noted to be of independent interest. Additionally, exploring MoMo's performance and theoretical properties under weaker assumptions (e.g., non-interpolation, less strict knowledge of lower bounds) would be valuable.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Stepping on the Edge: Curvature Aware Learning Rate Tuners",
    "Main Contributions": "The paper analyzes the closed-loop feedback effect between learning rate tuning and curvature, particularly the loss Hessian's largest eigenvalue (sharpness). It finds that classical learning rate tuners (like linesearch and quadratically greedy methods) yield greater one-step loss reduction but underperform constant learning rates in the long term in the full batch regime, as they break the stabilization of sharpness. A new method, Curvature Dynamics Aware Tuning (CDAT), is introduced which prioritizes long-term curvature stabilization over instantaneous objective progress. CDAT demonstrates behavior similar to prefixed warm-up schedules on deep learning objectives, outperforming tuned constant learning rates in the full batch regime. The research also shows that stochasticity introduces confounding effects that explain the previous success of some learning rate tuners at appropriate batch sizes. The work highlights the critical role of understanding the joint dynamics of the learning rate and curvature, beyond greedy minimization, to diagnose failures and design effective adaptive learning rate tuners.",
    "Methodology": "The study analyzes classical learning rate tuners, specifically a linesearch method satisfying a decrease criterion (Armijo-Goldstein) and a quadratically greedy method minimizing a quadratic approximation of the objective. For its novel contribution, the paper introduces Curvature Dynamics Aware Tuning (CDAT), which dynamically drives the learning rate towards the edge of stability (EOS) by setting `ηcdat_t = σ * nt / dt`, where `nt = max{-∇f(wt)⊤ut, 0}` and `dt = |u⊤t ∇2f(wt)ut| + ε`. The scaling factor `σ` allows interpolation between greedy (σ=1) and on-edge (σ=2) optimization. Exponential Moving Averages (EMA) are used to smooth estimates of `nt` and `dt` for robustness, especially in stochastic settings or with optimizers like Adam. The theoretical analysis employs a simplified model of the joint dynamics of the largest eigenvalue (sharpness) and the projection of parameters onto its eigendirection, extending previous work by Damian et al. (2022). This model uses normalized variables `y = ηλ - 2` (where y=0 is EOS) and `p = x^2`. Sharpness is estimated using a power iteration method.",
    "Experimental Setup": "Experiments were conducted across various datasets, architectures, optimizers, and losses. Datasets include MNIST (for MLP architecture sensitivity), CIFAR10 (subsets for full batch, full dataset for mini-batch), Tiny Shakespeare (subset for full batch), and Imagenet (Imagenette subset for full batch, full Imagenet-1K for mini-batch). Architectures tested are ResNet34 (without batch norm) and ResNet50 (with layer norm), MLP Mixer Ti/8, Nano Language Model (Transformer), Vision Transformer (ViT Ti/32), and simple MLPs. Base optimizers include GD, GD with Momentum, RMSProp, and Adam, with default hyperparameters except for fine-tuned learning rates on logarithmic grids. Scheduled baselines used linear warm-up followed by constant, linear, or cosine decay. Performance was evaluated using average train loss over the last 5 iterates, and metrics like learning rate, Hessian/preconditioned Hessian sharpness, their product, gradient norm, alignment between update direction and largest eigenvector, and angle between successive updates were analyzed. Mini-batch experiments also reported test error. Weight decay was generally fixed at 10^-5.",
    "Limitations": "The simplified theoretical model is insufficient to fully explain CDAT's benefits, particularly its failure to capture stabilization arising from higher-order terms in the dynamics. The model also inaccurately predicts runaway sharpening for σ < 2 and divergence for σ > 2, contrary to empirical observations of stable settings around σ = 2. CDAT's performance is subdued in the stochastic regime, partly due to the mitigated sharpening effects observed in such settings. The optimal scaling factor for CDAT varies with batch size in mini-batch regimes, with greedy rules sometimes outperforming on-edge rules. The CDAT rule, while showing warm-up like behavior, may not fully capture the benefits of precisely prefixed schedules, suggesting that additional nonlinear effects might be missed. Computationally, calculating the second partial derivative for CDAT can make it twice slower than constant or scheduled learning rates. Furthermore, using exact sharpness (largest eigenvalue of the Hessian) as a base learning rate leads to poor performance at σ=2, suggesting the method misses the crucial alignment of updates with the largest eigenvector.",
    "Future Research Directions": "Future work includes developing improved analytical or numerical models to more accurately capture stabilization due to higher-order terms in curvature dynamics. Integrating powerful tools from control theory, such as state feedback schemes, could aid in algorithm design for the next generation of learning rate tuners. Research is also needed to better understand and estimate the appropriate statistics for the edge of stability in stochastic regimes. Exploring efficient implementations for CDAT, such as using parabolic approximations or computing curvature estimates at intervals, is suggested to mitigate its computational cost. Investigating varying scaling factors or higher-order dynamics could help CDAT fully capture the benefits observed in prefixed warm-up schedules. Additionally, further adaptation of momentum parameters in an 'on-edge' context could be explored.",
    "Experiment Code": "# Copyright 2019 DeepMind Technologies Limited. All Rights Reserved.# Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at#     http://www.apache.org/licenses/LICENSE-2.0# Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================# -*- coding: utf-8 -*-# Functions for computing diagonals of the Hessian wrt to a set of parameters.Computing the Hessian for neural networks is typically intractable due to thequadratic memory requirements. Solving for the diagonal can be done cheaply,with sub-quadratic memory requirements.from typing import Anyimport jaxfrom jax import flatten_utilimport jax.numpy as jnpfrom optax.second_order import _basedef _ravel(p: Any) -> jax.Array:  return flatten_util.ravel_pytree(p)[0]def hvp(    loss: _base.LossFn,    v: jax.Array,    params: Any,    inputs: jax.Array,    targets: jax.Array,) -> jax.Array:  \"\"\"Performs an efficient vector-Hessian (of `loss`) product.  Args:    loss: the loss function.    v: a vector of size `ravel(params)`.    params: model parameters.    inputs: inputs at which `loss` is evaluated.    targets: targets at which `loss` is evaluated.  Returns:    An Array corresponding to the product of `v` and the Hessian of `loss`    evaluated at `(params, inputs, targets)`.  \"\"\"  _, unravel_fn = flatten_util.ravel_pytree(params)  loss_fn = lambda p: loss(p, inputs, targets)  return jax.jvp(jax.grad(loss_fn), [params], [unravel_fn(v)])[1]def hessian_diag(    loss: _base.LossFn,    params: Any,    inputs: jax.Array,    targets: jax.Array,) -> jax.Array:  \"\"\"Computes the diagonal hessian of `loss` at (`inputs`, `targets`).  Args:    loss: the loss function.    params: model parameters.    inputs: inputs at which `loss` is evaluated.    targets: targets at which `loss` is evaluated.  Returns:    A DeviceArray corresponding to the product to the Hessian of `loss`    evaluated at `(params, inputs, targets)`.  \"\"\"  vs = jnp.eye(_ravel(params).size)  comp = lambda v: jnp.vdot(v, _ravel(hvp(loss, v, params, inputs, targets)))  return jax.vmap(comp)(vs)",
    "Experiment Result": "The method analyzes classical learning rate tuners such as the linesearch method satisfying the Armijo-Goldstein decrease criterion (implemented in `optax/_src/linesearch.py` as `scale_by_backtracking_linesearch`) and a quadratically greedy method minimizing a quadratic approximation of the objective (implemented in `optax/_src/linesearch.py` as `scale_by_zoom_linesearch`).For its novel contribution, Curvature Dynamics Aware Tuning (CDAT) is introduced, dynamically driving the learning rate towards the edge of stability (EOS). The learning rate `ηcdat_t` is set by the formula `σ * nt / dt`, where `nt = max{-∇f(wt)⊤ut, 0}` and `dt = |u⊤t ∇2f(wt)ut| + ε`. The scaling factor `σ` is set to 1 for greedy optimization and 2 for on-edge optimization.Exponential Moving Averages (EMA) (implemented in `optax/transforms/_accumulation.py` as `ema` and `optax/tree_utils/_tree_math.py` as `tree_update_moment`) are employed for robust estimation of `nt` and `dt`, especially in stochastic environments or with optimizers like Adam.Sharpness estimation (the largest eigenvalue of the Hessian) is performed using a power iteration method (implemented in `optax/_src/linear_algebra.py` as `power_iteration`).The theoretical analysis extends previous work by Damian et al. (2022) using normalized variables `y = ηλ - 2` (where `y=0` indicates EOS) and `p = x^2`. The Hessian-vector product `∇2f(wt)ut` required for `dt` is provided by the `hvp` function in `optax/second_order/_hessian.py`."
}{
    "Title": "Mechanic: A Learning Rate Tuner",
    "Main Contributions": "The paper introduces 'mechanic', an automatic learning rate tuner that acts as a generic wrapper around any base optimization algorithm and schedule. It provides a practical implementation of recent theoretical reductions in online convex optimization, eliminating the need for manual tuning of the scalar learning rate scale factor. The method is shown to match or improve upon manually tuned learning rates across various large-scale deep learning tasks.",
    "Methodology": "Mechanic is built upon the Online Convex Optimization (OCO) framework, specifically by leveraging a theorem from [16] that allows a 'meta' OCO algorithm (tuner) to automatically tune the scale factor of a base OCO algorithm. The core idea is to set the wrapped algorithm's iterate as xmechanic_t = xbase_1 + st * (xbase_t - xbase_1), where st is the scale factor learned by the tuner. The tuner receives a 'gradient' ht, which is an approximation of the gradient over the base optimizer's course, initially defined as <gt, xbase_t - xbase_1>. The tuner itself is a variant of a parameter-free OCO algorithm (Algorithm 2, simplified to Algorithm 1 for practical implementation), employing exponentially weighted moving averages with multiple beta (β) values (n=6) to introduce recency bias and obviate tuning. A peculiar weight-decay-esque term (λ=0.01) is added to ht: ht = <gt + λ * (sum(st,i)) * ||gt|| * xt / ||xt||, ∆t>, which empirically improved performance.",
    "Experimental Setup": "Mechanic was rigorously evaluated on a range of deep learning tasks. For Natural Language Processing, BERT models (BERT-B 110M, BERT-L 340M) were pre-trained on the Wikibooks dataset using Masked Language Modeling (MLM), and then finetuned on the 5 largest datasets from the GLUE suite (MNLI, QNLI, SST-2, QQP). Experiments varied batch sizes, model sizes, and base optimizers (AdamW, Lion). For Image Classification, CNNs (ResNet-18, WRN-40-10) were trained from scratch on CIFAR-10 and CIFAR-100. Transfer learning experiments involved pre-training ViT models (ViT-B/16, ViT-L/16) on JFT-300M and finetuning on ImageNet, CIFAR-10, and CIFAR-100 datasets. Comparisons were made against D-adaptation on tasks like CIFAR-10 (WRN-16-8), CIFAR-100 (DenseNet), IWSLT14 (LSTM), BookWiki (GPT Transformer), and a suite of 12 logistic regression problems. Results were typically reported as peak validation scores or test accuracy/loss, averaged over 3 independent runs.",
    "Limitations": "Mechanic does not perform well in conjunction with dropout, suggesting a more fundamental incompatibility beyond just noise. The method optimizes for train loss because it re-purposes gradients from the training set, which contrasts with manual learning rate tuning based on validation set performance. The 'peculiar weight-decay-esque term' used in the tuner currently lacks a formal analytical or theoretical explanation. Furthermore, the practical implementation of the tuner (Algorithm 1) simplifies the theoretically tractable version (Algorithm 2), and while it performs well, it does not come with the same rigorous theoretical guarantees as the more complex version.",
    "Future Research Directions": "Key future research directions include investigating a theoretical motivation for the empirically successful weight-decay-esque term. Another promising area is exploring whether similar techniques can be extended to learn per-layer scale factors, which could significantly enhance performance without a substantial increase in computational cost, given the infeasibility of manual per-layer tuning. Additionally, developing a principled approach to address the discrepancy between optimizing for train loss (mechanic's current behavior) and tuning based on validation set performance (common manual practice) is an interesting avenue.",
    "Experiment Code": null,
    "Experiment Result": null
}{
    "Title": "Prodigy: An Expeditiously Adaptive Parameter-Free Learner",
    "Main Contributions": "Prodigy is a novel algorithm designed to expeditiously estimate the optimal learning rate in adaptive optimization methods like AdaGrad and Adam. It provides a provable estimation of the distance to the solution (D), which is crucial for optimal learning rate setting. Prodigy improves upon the non-asymptotic convergence rate of the D-Adaptation method by a factor of O(p log(D/d0)) and establishes a new state-of-the-art for learning rate adaptation, consistently outperforming D-Adaptation. The paper also develops a lower complexity bound for methods that grow the learning rate at most exponentially fast and demonstrates Prodigy's practical applicability in fine-tuning various deep learning models.",
    "Methodology": "Prodigy is a modification of the D-Adaptation method for learning-rate-free learning. Its core improvement lies in handling the error term using modified AdaGrad-like step sizes. Instead of the standard sum of squared gradient norms in the denominator, Prodigy utilizes `d_i^2 ||g_i||^2` where `d_i` is the estimated distance to solution at iteration `i`. The step size formula for the Gradient Descent version is `ηk = d_k^2 λk / q(d_k^2 G^2 + Pk i=0 d_i^2 λ_i^2 ||g_i||^2)`, where `λk` is an extra weighting factor (e.g., `λk = 1` or `λk = k^p`). An extra `G^2` term is introduced in the denominator to account for G-Lipschitz objective functions. For Adam-like methods, Prodigy derives step sizes by approximating Adam's exponential moving average of squared gradients, effectively computing an exponential moving average of `d_k g_k` rather than `g_k` itself, often using `λk = β_2^(-k/2)` and coordinate-wise `ℓ1` norms for the denominator. The algorithm ensures `d_k` is non-decreasing.",
    "Experimental Setup": "Prodigy was evaluated across a diverse range of problems and datasets. For convex optimization, it was tested on 12 common logistic-regression benchmark datasets using multi-margin loss and full-batch optimization over 1,000 steps with 10 random seeds. For deep learning, experiments included: VGG11 and ResNet-50 training on CIFAR10 (batch size 256, cosine annealing, no weight decay, 8 seeds); a 6-layer nanoGPT transformer on the Shakespeare dataset (batch size 256, gradient clipping, float16, AdamW with specific hyperparameters, 8 seeds); LSTM training on IWSLT14; RoBERTa and GPT transformer training on BookWiki; DLRM on the Criteo Kaggle Display Advertising dataset; VarNet on the Knee MRI dataset (fastMRI); and Vision Transformer (ViT) training on ImageNet (ILSVRC 2012) with a tuned weight decay of 0.05. Comparisons were made against D-Adaptation (SGD/DA variants), DoG, L-DoG, and hand-tuned Adam/AdamW.",
    "Limitations": "The theoretical analysis for the Dual Averaging version (Algorithm 2) is limited; it lacks a convergence result for weights other than `λk = d_k^2` because its analysis requires monotonically decreasing step sizes. While the Gradient Descent version's theoretical rate includes a `log(n+1)` term, which is absent in Dual Averaging's tighter guarantee, Gradient Descent generally performs better in practice. The developed lower complexity bounds for exponentially bounded algorithms are primarily applicable for `n` small relative to `D/d0`. Using methods that significantly overshoot the standard `D/G` step size, though not problematic for Lipschitz functions, risks training divergence with functions whose gradients are only locally bounded. Experimentally, training CIFAR10 models without weight decay led to slight overfitting, and a heuristic Adam-like modification for DoG diverged. ViT training exhibited high run-to-run variance, making precise comparisons challenging, and Prodigy sometimes slightly overfitted on LSTM training compared to baselines.",
    "Future Research Directions": "An open problem remains to establish a more general lower complexity bound for exponentially bounded algorithms that covers larger values of `n` beyond the current `n < log(D/d0)` scale. Future work could also focus on extending the convergence results for the Dual Averaging variant of Prodigy (Algorithm 2) to incorporate a wider range of weighting factors (`λk`) other than `d_k^2`. Furthermore, exploring adaptive proximal algorithms for non-smooth differentiable functions, building upon previous work on locally smooth functions, is another potential area for extending this research.",
    "Experiment Code": "import mathfrom typing import TYPE_CHECKING, Any, Callable, Optionalimport torchimport torch.optimimport loggingimport osimport torch.distributed as distif TYPE_CHECKING: from torch.optim.optimizer import _params_telse: _params_t = Anyclass Prodigy(torch.optim.Optimizer): r\"\"\"Implements Adam with Prodigy step-sizes. Leave LR set to 1 unless you encounter instability. Arguments: params (iterable): Iterable of parameters to optimize or dicts defining parameter groups. lr (float): Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate. betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) beta3 (float): coefficients for computing the Prodidy stepsize using running averages. If set to None, uses the value of square root of beta2 (default: None). eps (float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8). weight_decay (float): Weight decay, i.e. a L2 penalty (default: 0). decouple (boolean): Use AdamW style decoupled weight decay use_bias_correction (boolean): Turn on Adam's bias correction. Off by default. safeguard_warmup (boolean): Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default. d0 (float): Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing. d_coef (float): Coefficient in the expression for the estimate of d (default 1.0). Values such as 0.5 and 2.0 typically work as well. Changing this parameter is the preferred way to tune the method. growth_rate (float): prevent the D estimate from growing faster than this multiplicative rate. Default is inf, for unrestricted. Values like 1.02 give a kind of learning rate warmup effect. fsdp_in_use (bool): If you're using sharded parameters, this should be set to True. The optimizer will attempt to auto-detect this, but if you're using an implementation other than PyTorch's builtin version, the auto-detection won't work. slice_p (int): Reduce memory usage by calculating LR adaptation statistics on only every pth entry of each tensor. For values greater than 1 this is an approximation to standard Prodigy. Values ~11 are reasonable (default 1).\"\"\" def __init__(self, params, lr=1.0, betas=(0.9, 0.999), beta3=None, eps=1e-8, weight_decay=0, decouple=True, use_bias_correction=False, safeguard_warmup=False, d0=1e-6, d_coef=1.0, growth_rate=float('inf'), fsdp_in_use=False, slice_p=1): if not 0.0 < d0: raise ValueError(\"Invalid d0 value: {}\".format(d0)) if not 0.0 < lr: raise ValueError(\"Invalid learning rate: {}\".format(lr)) if not 0.0 < eps: raise ValueError(\"Invalid epsilon value: {}\".format(eps)) if not 0.0 <= betas[0] < 1.0: raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0])) if not 0.0 <= betas[1] < 1.0: raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1])) if decouple and weight_decay > 0: print(f\"Using decoupled weight decay\") defaults = dict(lr=lr, betas=betas, beta3=beta3, eps=eps, weight_decay=weight_decay, d=d0, d0=d0, d_max=d0, d_numerator=0.0, d_coef=d_coef, k=0, growth_rate=growth_rate, use_bias_correction=use_bias_correction, decouple=decouple, safeguard_warmup=safeguard_warmup, fsdp_in_use=fsdp_in_use, slice_p=slice_p) self.d0 = d0 super().__init__(params, defaults) @property def supports_memory_efficient_fp16(self): return False @property def supports_flat_params(self): return True def step(self, closure=None): \"\"\"Performs a single optimization step. Arguments: closure (callable, optional): A closure that reevaluates the model and returns the loss.\"\"\" loss = None if closure is not None: loss = closure() d_denom = 0.0 group = self.param_groups[0] use_bias_correction = group['use_bias_correction'] beta1, beta2 = group['betas'] beta3 = group['beta3'] if beta3 is None: beta3 = math.sqrt(beta2) k = group['k'] d = group['d'] d_max = group['d_max'] d_coef = group['d_coef'] lr = max(group['lr'] for group in self.param_groups) if use_bias_correction: bias_correction = ((1 - beta2**(k+1))**0.5) / (1 - beta1**(k+1)) else: bias_correction = 1 dlr = d*lr*bias_correction growth_rate = group['growth_rate'] decouple = group['decouple'] fsdp_in_use = group['fsdp_in_use'] d_numerator = group['d_numerator'] d_numerator *= beta3 delta_numerator = 0.0 for group in self.param_groups: decay = group['weight_decay'] k = group['k'] eps = group['eps'] group_lr = group['lr'] d0 = group['d0'] safeguard_warmup = group['safeguard_warmup'] slice_p = group['slice_p'] if group_lr not in [lr, 0.0]: raise RuntimeError(f\"Setting different lr values in different parameter groups is only supported for values of 0\") for p in group['params']: if p.grad is None: continue if hasattr(p, \"_fsdp_flattened\"): fsdp_in_use = True grad = p.grad.data # Apply weight decay (coupled variant) if decay != 0 and not decouple: grad.add_(p.data, alpha=decay) state = self.state[p] # State initialization if 'step' not in state: state['step'] = 0 state['s'] = torch.zeros_like(p.data.flatten()[::slice_p]).detach() if p.any(): state['p0'] = p.flatten()[::slice_p].detach().clone() else: # All values are zero, so save VRAM with a zero-tensor state['p0'] = torch.tensor(0, device=p.device, dtype=p.dtype) # Exponential moving average of gradient values if beta1 > 0: state['exp_avg'] = torch.zeros_like(p.data).detach() # Exponential moving average of squared gradient values state['exp_avg_sq'] = torch.zeros_like(p.data).detach() exp_avg_sq = state['exp_avg_sq'] s = state['s'] p0 = state['p0'] if group_lr > 0.0: # we use d / d0 instead of just d to avoid getting values that are too small sliced_grad = grad.flatten()[::slice_p] delta_numerator += (d / d0) * dlr * torch.dot(sliced_grad, p0.data - p.data.flatten()[::slice_p]).item() # Adam EMA updates if beta1 > 0: exp_avg = state['exp_avg'] exp_avg.mul_(beta1).add_(grad, alpha=d * (1-beta1)) exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=d * d * (1-beta2)) if safeguard_warmup: s.mul_(beta3).add_(sliced_grad, alpha=((d / d0) * d)) else: s.mul_(beta3).add_(sliced_grad, alpha=((d / d0) * dlr)) d_denom += s.abs().sum().item() ###### d_hat = d # if we have not done any progres, return # if we have any gradients available, will have d_denom > 0 (unless \\|g\\|=0) if d_denom == 0 and not fsdp_in_use: return loss if lr > 0.0: if fsdp_in_use: dist_tensor = torch.zeros(2).cuda() dist_tensor[0] = delta_numerator dist_tensor[1] = d_denom dist.all_reduce(dist_tensor, op=dist.ReduceOp.SUM) global_d_numerator = d_numerator + dist_tensor[0] global_d_denom = dist_tensor[1] else: global_d_numerator = d_numerator + delta_numerator global_d_denom = d_denom d_hat = d_coef * global_d_numerator / global_d_denom if d == group['d0']: d = max(d, d_hat) d_max = max(d_max, d_hat) d = min(d_max, d * growth_rate) for group in self.param_groups: group['d_numerator'] = global_d_numerator group['d_denom'] = global_d_denom group['d'] = d group['d_max'] = d_max group['d_hat'] = d_hat decay = group['weight_decay'] k = group['k'] eps = group['eps'] for p in group['params']: if p.grad is None: continue grad = p.grad.data state = self.state[p] exp_avg_sq = state['exp_avg_sq'] state['step'] += 1 denom = exp_avg_sq.sqrt().add_(d * eps) # Apply weight decay (decoupled variant) if decay != 0 and decouple: p.data.add_(p.data, alpha=-decay * dlr) ### Take step if beta1 > 0: exp_avg = state['exp_avg'] p.data.addcdiv_(exp_avg, denom, value=-dlr) else: p.data.addcdiv_(grad, denom, value=-dlr * d) group['k'] = k + 1 return loss",
    "Experiment Result": "Prodigy implements Adam with Prodigy step-sizes. Leave LR set to 1 unless you encounter instability. Arguments: lr (float): Learning rate adjustment parameter. Increases or decreases the Prodigy learning rate. (default: 1.0). betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)). beta3 (float): coefficients for computing the Prodigy stepsize using running averages. If set to None, uses the value of square root of beta2 (default: None). eps (float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-8). weight_decay (float): Weight decay, i.e. a L2 penalty (default: 0). decouple (boolean): Use AdamW style decoupled weight decay (default: True). use_bias_correction (boolean): Turn on Adam's bias correction. Off by default. (default: False). safeguard_warmup (boolean): Remove lr from the denominator of D estimate to avoid issues during warm-up stage. Off by default. (default: False). d0 (float): Initial D estimate for D-adaptation (default 1e-6). Rarely needs changing. d_coef (float): Coefficient in the expression for the estimate of d (default 1.0). Values such as 0.5 and 2.0 typically work as well. Changing this parameter is the preferred way to tune the method. growth_rate (float): prevent the D estimate from growing faster than this multiplicative rate. Default is inf, for unrestricted. Values like 1.02 give a kind of learning rate warmup effect. fsdp_in_use (bool): If you're using sharded parameters, this should be set to True. The optimizer will attempt to auto-detect this, but if you're using an implementation other than PyTorch's builtin version, the auto-detection won't work. (default: False). slice_p (int): Reduce memory usage by calculating LR adaptation statistics on only every pth entry of each tensor. For values greater than 1 this is an approximation to standard Prodigy. Values ~11 are reasonable (default: 1)."
}

# Instructions
Following the instructions below, please provide an evaluation of the new hypothesis.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new hypothesis has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new hypothesis on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new hypothesis is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new hypothesis on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'REACTOR combines several ideas that are not present together in prior tuners: 1) it frames learning-rate selection itself as an online reinforcement-learning problem whose reward is exact-match gain per millisecond of wall-clock compute, 2) it jointly trains a global LR sampler and a token-wise multiplier head with policy-gradient updates, 3) it incorporates curvature (max Hessian), gradient noise, and self-consistency reward into the policy state, and 4) it delivers this in a lightweight, pure-PyTorch implementation without external solvers.   Existing works cover only parts of this space: AutoLRS and Mechanic optimise LR but via Bayesian optimisation or Online Convex Optimisation, not RL nor reward-per-compute; BATS gives static token-wise multipliers derived from sharpness but does not learn them online; CDAT tracks sharpness to stay on the edge of stability but ignores downstream reward and compute cost; no cited work links LR decisions to self-consistency EM or trains a two-head policy during fine-tuning. Hence the hypothesis introduces a qualitatively new control-loop design for LR scheduling in LLM PEFT.', 'novelty_score': 8, 'significance_reason': 'If REACTOR’s claimed 2–3 pp EM gain over the best curvature-aware baseline within a fixed 45-minute budget holds, it would materially lower the cost of achieving state-of-the-art accuracy on GSM8K with small LLMs and provide a general recipe for compute-budgeted fine-tuning. The method is task- and model-agnostic, adds <4 % overhead, removes dependencies on CVXPY/QP solvers, and could be adopted easily in industry workflows constrained by GPU time. Academically, it bridges RL for optimiser control with curvature statistics, opening a new line of research on reward-aware optimisation hyper-policies. The societal impact is indirect but positive—more efficient fine-tuning means lower energy use and wider accessibility. The improvement margin and domain (math reasoning) are meaningful but not transformational, so significance is high but not maximal.', 'significance_score': 7}
